# Ковариация и корреляция {#cov_cor}

Возьмем новый набор данных, на этот раз про американских студентов, вес их рюкзаков и проблемы со спиной. Этот набор данных хранится в пакете `Stat2Data` --- пакет с большим количеством разнообразных данных.

```{r, eval = FALSE}
install.packages("Stat2Data")
```

С помощью функции `data()` загрузим набор данных `Backpack`:

```{r}
library(tidyverse)
library(Stat2Data)
data(Backpack)
```

Давайте посмотрим, что внутри этой переменной:

```{r}
skimr::skim(Backpack)
```

С помощью `?Backpack` можно получить подробное описание колонок этого датасета.

Например, можно заметить, что масса как самих студентов, так и их рюкзаков выражена в фунтах. Давайте создадим новые переменные `backpack_kg` и `body_kg`, в которых будет записан вес (рюкзаков и самих студентов соответственно) в понятным для нас килограммах. Новый набор данных сохраним под названием `back`.

```{r}
back <- Backpack %>%
  mutate(backpack_kg = 0.45359237 * BackpackWeight,
         body_kg = 0.45359237 * BodyWeight)
```

До этого мы говорили о *различиях* между выборками. Теперь мы будем говорить о *связи* между переменными.

## Ковариация {#cov} 

Самая простая мера связи между двумя переменными --- это ковариация. Если **ковариация положительная**, то чем *больше* одна переменная, тем *больше* другая переменная. При **отрицательной ковариации** все наоборот: чем *больше* одна переменная, тем *меньше* другая.

- Формула ковариации: 

$$\sigma_{xy} = cov(x, y) = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n}$$

- Оценка ковариации по выборке:

$$\hat{\sigma}_{xy} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n-1}$$

Ковариация переменной самой с собой --- дисперсия.

В R есть функция `cov()` для подсчета ковариации. На самом деле, функция `var()` делает то же самое. Обе эти функции считают сразу **матрицу ковариаций** для всех сочетаний колонок на входе:

```{r}
back %>%
  select(body_kg, backpack_kg) %>%
  cov()

back %>%
  select(body_kg, backpack_kg) %>%
  var()
```

Ну а по углам этой матрицы --- дисперсии!

## Корреляция {#cor}

**Корреляцией** обычно называют любую связь между двумя переменными, это просто синоним слова "ассоциация". Если вдруг слово "корреляция" вам еще непривычно, то попробуйте мысленно заменять "корреляцию" на "ассоциацию", а "коррелирует" на "связано".
**Коэффициент корреляции** --- это уже конкретная математическая формула, которая позволяет посчитать эту связь и принимает значения от -1 до 1.[^coef] 

- Если коэффициент корреляции **положительный**, то чем **больше** значения в одной переменной, тем **больше** значения в другой переменной.

- Если коэффициент корреляции **отрицательный**, то чем **больше** значения в одной переменной, тем **меньше** значения в другой переменной.

- Если коэффициент корреляции равен 0, то изменения одной переменной не связано с изменениями в другой переменной.

[^coef]: Впрочем, это не так уж и важно: на практике часто опускают слово "коэффициент" и называют корреляцией как и просто связь, так и ее способ измерения.

### Коэффициент корреляции Пирсона {#pearson_cor}

Самый известный коэффициент корреляции - коэффициент корреляции Пирсона:

$$\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x \sigma_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n}\sum_{i = 1}^n z_{x,i} z_{y, i}$$

Оценка коэффициента корреляции Пирсона по выборке:
$$r_{xy} = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n - 1}\sum_{i = 1}^n z_{x,i} z_{y, i}$$

Коэффициент корреляции Пирсона можно понимать по-разному. С одной стороны, это просто ковариация, нормированная на стандартное отклонение обоих переменных. С другой стороны, можно понимать это как среднее произведение z-оценок. 

Корреляцию в R можно посчитать с помощью функции `cor()`:

```{r}
back %>%
  select(body_kg, backpack_kg) %>%
  cor()
```

Для тестирования уровня значимости нулевой гипотезы для корреляции есть функция `cor.test()`. В случае с коэффициентами корреляции, нулевая гипотеза формулируется как отсутствие корреляции (т.е. она равна нулю) в генеральной совокупности.

```{r}
cor.test(back$backpack_kg, back$body_kg)
```

Результат выполнения этой функции очень похож на то, что мы получали при проведении t-теста.

### Непараметрические коэффициенты корреляции {#nonparam_cor}

У коэффициента корреляции Пирсона, как и у t-теста, есть свои непараметрические братья: коэффициент корреляции Спирмена и коэффициент корреляции Кэнделла. Из них чаще используется коэффициент корреляции Спирмена. Посчитать его можно с помощью той же функции `cor.test()`, задав соответствующее значение параметра `method =`:

```{r}
cor.test(back$backpack_kg, back$body_kg, method = "spearman")
cor.test(back$backpack_kg, back$body_kg, method = "kendall")
```

> Заметьте, в данном случае два метода хотя и привели к схожим размерам корреляции, но в одном случае *p-value* оказался больше 0.05, а в другом случае - меньше 0.05. Выбирать тест *a posteriori* на основе того, какие результаты вам нравятся больше, --- плохая практика (\@ref(bad_practice)). Не надо так делать.

## Корреляционная матрица {#cor_mat}

Возможно, вы нашли что-то более интересное для проверки гипотезы о корреляции. Например, вы еще хотите проверить гипотезу о связи количества учебных кредитов и массе рюкзака: логично предположить, что чем больше студент набрал себе курсов, тем тяжелее его рюкзак (из-за большего количества учебников). Или что студенты к старшим курсам худеют и становятся меньше. Или что те, кто набрал себе много курсов, меньше питаются и от того меньше весят. В общем, хотелось бы прокоррелировать все интересующие нас переменные со всеми. Это можно сделать с помощью функции `cor()`:

```{r}
back %>%
  select(body_kg, backpack_kg, Units, Year) %>%
  cor()
```

Но функция `cor()`не позволяет посчитать *p-value* для этих корреляций! Функция `cor.test()` позволяет получить *p-value*, но только для одной пары переменных.

На помощь приходит пакет `psych` с функцией `corr.test()`:

```{r}
back %>%
  select(body_kg, backpack_kg, Units, Year) %>%
  psych::corr.test()
```

Тем не менее, если у вас много гипотез для тестирования, то у вас появляется проблема: вероятность выпадения статистически значимых результатов сильно повышается. Даже если эти переменные никак не связаны друг с другом. 

Эта проблема называется **проблемой множественных сравнений (multiple comparisons problem)** [^mult]. Если мы проверяем сразу несколько гипотез, то у нас возрастает **групповая вероятность ошибки первого рода (Family-wise error rate)** --- вероятность ошибки первого рода для хотя бы одной из множества гипотез.

Например, если вы коррелируете 10 переменных друг с другом, то вы проверяете 45 гипотез о связи. Пять процентов из этих гипотез, т.е. в среднем 2-3 гипотезы у вас будут статистически значимыми даже если никаких эффектов на самом деле нет!

Поэтому если вы проверяете сразу много гипотез, то необходимо применять **поправки на множественные сравнения (multiple testing correction)**. Эти поправки позволяют контролировать групповую вероятность ошибки первого рода на желаемом уровне. Самая простая и популярная поправка на множественные сравнения --- **поправка Бонферрони (Bonferroni correction)**. Она считается очень просто: мы просто умножаем p-value на количество проверяемых гипотез! 

```{r}
back %>%
  select(body_kg, backpack_kg, Units, Year) %>%
  psych::corr.test(adjust = "bonferroni")
```

Это очень "дубовая" и излишне консервативная поправка. Да, она гарантирует контроль групповую вероятности ошибки первого рода, но при этом сильно повышает вероятность ошибки второго рода --- вероятность пропустить эффект, если он на самом деле существует. Поэтому по умолчанию в R используется более либеральная поправка на множественные сравнения под названием **поправка Холма** или **поправка Холма-Бонферрони (Holm-Bonferroni correction)**, которая, тем не менее, тоже гарантирует контроль групповой вероятности ошибки первого рода. 

Альтернативный подход к решению проблемы множественных сравнений --- это контроль **средней доли ложных отклонений (False Discovery Rate; FDR)** на на уровне не выше уровня $\alpha$. Это более либеральный подход: в данном случае мы контролируем, что ложно-положительных результатов у нас не больше, например, 5%. Такой подход применяется в областях, где происходит масштабное множественное тестирование. Попытка контролировать *групповую вероятность ошибки первого уровня* не выше уровня $\alpha$ привела бы к чрезвычайно низкой вероятности обнаружить хоть какие-нибудь эффекты (т.е. к низкой статистической мощности). 

Самая известная поправка для контроля *средней доли ложных отклонений* --- это **поправка Бенджамини — Хохберга (Benjamini-Hochberg correction).**

```{r}
back %>%
  select(body_kg, backpack_kg, Units, Year) %>%
  psych::corr.test(adjust = "BH")
```

Все перечсиленные поправки (и еще несколько других) доступны не только в функции `corr.test()`, но и в базовом R с помощью функции `p.adjust()`. Эта функция принимает вектор из p-values и возвращает результат применения поправок.

```{r}
p_vec <- seq(0.0001, 0.06, length.out = 10)
p_vec
p.adjust(p_vec) #по умолчанию используется поправка Холма-Бонферрони
p.adjust(p_vec, method = "bonferroni")
p.adjust(p_vec, method = "BH")
```

[^mult]: "Проблема множественных сравнений" - это устоявшийся термин, который используется и в случае множественных корреляций, и в случае множественных сравнений средних и в любых других случаях с тестированием нескольких гипотез одновременно.

## Хитмэп корреляций {#hitmap_cor}

