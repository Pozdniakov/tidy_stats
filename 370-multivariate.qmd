# Многомерные методы анализа данных {#sec-multivar}

**Многомерные методы анализа данных** -- методы работы с данными, в которых много колонок. Мы уже сталкивались с некоторыми многомерными методами, например, с множественной линейной регрессией (@sec-lm_mult). Поэтому вы знаете, что многомерность создает новые проблемы. Например, при множественных корреляциях или попарных сравнениях возникает проблема множественных сравнений, а при использовании множественной регрессии лишние предикторы могут ловить только шум и приводить к переобучению (если говорить в терминах машинного обучения). Короче говоря, больше - не значит лучше. Нужно четко понимать, зачем мы используем данные и что пытаемся измерить.

Однако в некоторых случаях мы в принципе не можем ничего интересного сделать с маленьким набором переменных. Много ли мы можем измерить личностным тестом с одним единственным вопросом? Можем ли мы точно оценить уровень интеллекта по успешности выполнения одного единственного задания? Очевидно, что нет. Более того, даже концепция интеллекта в современном его представлении появилась во многом благодаря разработке многомерных методов анализа! Ну или наоборот: исследования интеллекта подстегнули развитие многомерных методов.

## Уменьшение размерности {#sec-dim_red}

Представьте многомерное пространство, где каждая колонка --- это отдельная ось, а каждая строка задает координаты одной точки в этом пространстве. Мы получим многомерную диаграмму рассеяния.

Многомерную диаграмму рассеяния, к сожалению, нельзя нарисовать, поэтому нарисуем несколько двухмерных диаграмм рассеяния для отображения сочетания всех колонок со всеми набора данных `penguins`.

<!--# Написать про датасет пингвины -->

```{r}
library(tidyverse)
library(palmerpenguins)

penguins <- penguins %>%
  drop_na(bill_length_mm:body_mass_g)

penguins %>%
  select(bill_length_mm:body_mass_g) %>%
  plot(col = penguins$species)
```

Даже представить как устроены многомерные данные очень трудно, а ведь мы отобразили только четыре числовых колонки! Понять связи между отдельными переменными мы можем, если посмотрим на каждую диаграмму рассеяния по отдельности, но и то если их не слишком много. Но связи в многомерных данных могут быть гораздо более сложными и на простых диаграммах рассеяния их иногда нельзя заметить. Это как смотреть на тени, поворачивая какой-нибудь предмет различными сторонами.

Уменьшение размерности позволяет вытащить из данных самую мякотку, уместив исходные данные в небольшое количество переменных. Остальное из данных удаляется.

::: callout-important
## *Осторожно:* уменьшение размерности приводит к потере данных

Уменьшение размерности -- не "бесплатная" операция. Уменьшение размерности всегда приводит к потере части данных и может значительно их изменить.
:::

Данные сниженной размерности гораздо проще визуализировать. Например, снизив размерность данных до двух шкал, можно просто нарисовать диаграмму рассеяния. Анализируя связь полученных шкал с изначальными шкалами и взаимное расположение точек в новых шкалах, можно многое понять об исследуемых данных.

Однако снижение размерности используется не только для эксплораторного анализа, но и для снижения количества фичей в машинном обучения, для удаления шума из данных, для более экономного хранения данных и многих других задач.

## Анализ главных компонент *(Principal component analysis)* {#pca}

**Анализ главных компонент** ***(АГК; Principal component analysis)*** -- это, пожалуй, самый известный метод **уменьшения размерности*.*** АГК просто поворачивает систему координат многомерного пространства, которое задано имеющимися числовыми колонками. Координатная система поворачивается таким образом, чтобы первые оси объясняли как можно больший разброс данных, а последние - как можно меньший. Тогда мы могли бы отбросить последние оси и не очень-то многое потерять в данных. Для двух осей это выглядит вот так:

```{r, echo = FALSE, results = 'asis'}
if (knitr:::is_latex_output()) {
  knitr::include_graphics("images/q7hip.png")
  cat("\n\nК сожалению, в PDF нельзя вставить .gif анимацию, посмотреть
      ее можно по [ссылке](https://github.com/Pozdniakov/tidy_stats/blob/master/images/q7hip.gif)")
} else {
  knitr::include_graphics("images/q7hip.gif")
}
```

Первая ось должна минимизировать красные расстояния. Вторая ось будет просто перпендикулярна первой оси.

::: callout-caution
## *Для продвинутых:* какая математика стоит за АГК?

Математически, АГК - это нахождение собственных векторов и собственных значений матрицы корреляций или ковариаций. Собственные вектора - это такие особенные вектора матрицы, умножив которые на данную матрицу, можно получить тот же самый вектор (т.е. того же направления), но другой длины. А вот коэффициент множителя длины нового вектора - это собственное значение. В контексте АГК, собственные вектора - это новые оси (т.е. те самые новые компоненты), а собственные значения - это размер объясняемой дисперсии с помощью новых осей. Собственные вектора, ранжированные по их собственным значениям от большего к меньшему, --- это и есть главные компоненты в искомом порядке.
:::

Итак, для начала нам нужно центрировать и нормировать данные - вычесть среднее и поделить на стандартное отклонение, т.е. посчитать $z$-оценки ( \@ref(z_scores)). Это нужно для того, чтобы сделать все шкалы равноценными. Это особенно важно делать когда разные шкалы используют несопоставимые единицы измерения. Скажем, одна колонка - это масса человека в килограммах, а другая - рост в метрах. Если применять АГК на этих данных, то ничего хорошего не выйдет: вклад роста будет слишком маленьким. А вот если мы сделаем z-преобразование, то приведем и вес, и рост к "общему знаменателю".

В базовом R уже есть инструменты для АГК `princomp()` и `prcomp()`, считают они немного по-разному. Возьмем более рекомендуемый вариант, `prcomp()`. Эта функция умеет самостоятельно поводить z-преобразования, для чего нужно поставить `center = TRUE` и `scale. = TRUE`.

```{r}
iris_pr <- iris %>%
  select(!Species) %>%
  prcomp(center = TRUE, scale. = TRUE)
```

Уже много раз встречавшаяся нам функция `summary()`, примененная на результат проведения АГК, выдаст информацию о полученных компонентах. Наибольший интерес представляют строчки "Proportion of Variance" и "Cumulative Proportion", которые показывают долю дисперсию, объясненную компонентной, и кумулятивную долю объясненной дисперсии.

```{r}
summary(iris_pr)
```

Функция `plot()` повзоляет визуализировать соотношение разных компонент.

```{r}
plot(iris_pr)
```

Как мы видим, первый компонент объясняет большую часть дисперсии, второй компонент заметно меньше, остальные два практически не имеют влияния, то есть, скорее всего, они репрезентируют некоторый шум в данных.

Теперь мы можем визуализировать первые два компонента. Это можно сделать с помощью базовых инструментов R.

```{r}
plot(iris_pr$x[,1:2], col=iris$Species)
```

Однако пакет `{factoextra}` представляет гораздо более широкие возможности для визуализации.

```{r, eval = FALSE}
install.packages("factoextra")
```

```{r}
library(factoextra)
fviz_pca_ind(iris_pr,
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
fviz_pca_var(iris_pr,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
fviz_pca_biplot(iris_pr, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```

```{r}
library(ggfortify)
autoplot(iris_pr, data = iris, colour = "Species", loadings = TRUE, loadings.label = TRUE) +
  theme_bw()
```

## tSNE

```{r}

```

## Эксплораторный факторный анализ

## Конфирматорный факторный анализ

## Кластерный анализ

```{r}
iris_3means <- kmeans(iris %>% select(!Species), centers = 3)
table(iris$Species, iris_3means$cluster)
plot(iris %>% select(!Species), col = iris$Species, pch = iris_3means$cluster)
```

## Многомерное шкалирование

## Сетевой анализ

## Другие методы многомерного анализа данных
