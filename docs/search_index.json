[["index.html", "Анализ данных и статистика в R 1 О курсе", " Анализ данных и статистика в R Поздняков Иван 2021-04-01 1 О курсе Здесь будут материалы для курса “Анализ данных и статистика в R.” Эта онлайн-книжка будет постоянно пополняться, поэтому следите за обновлениями! По всем вопросам пишите на мне на почту ivanspozdniakov@gmail.com, VK или в Telegram:@pozdniakovivan. "],["intro.html", "2 Введение в R 2.1 Установка R и Rstudio 2.2 Знакомство с RStudio 2.3 R как калькулятор 2.4 Функции 2.5 В любой непонятной ситуации — гуглите 2.6 Переменные 2.7 Логические операторы", " 2 Введение в R 2.1 Установка R и Rstudio Для работы с R необходимо его сначала скачать и установить. R на Windows, найдите большую кнопку Download R (номер версии) for Windows. на Mac, если маку меньше, чем 5 лет, то смело ставьте *.pkg файл с последней версией. Если старше, то поищите на той же странице версию для вашей системы. на Linux, также можно добавить зеркало и установить из командной строки: sudo apt-get install r-cran-base В данной книге используется следующая версия R: sessionInfo()$R.version$version.string ## [1] &quot;R version 4.0.2 (2020-06-22)&quot; После установки R необходимо скачать и установить RStudio: RStudio Если вдруг что-то установить не получается (или же вы просто не хотите устанавливать на компьютер лишние программы), то можно работать в облаке, делая все то же самое в веб-браузере: RStudio cloud Первый и вполне закономерный вопрос: зачем мы ставили R и отдельно еще какой-то RStudio? Если опустить незначительные детали, то R — это сам язык программирования, а RStudio — это среда (IDE), которая позволяет в этом языке очень удобно работать. RStudio — это не единственная среда для R, но, определенно, самая удобная на сегодняшний день. Почти все пользуются именно ею и не стоит тратить время на поиск чего-то более удобного и лучшего. Если же вы привыкли работать с Jupyter Notebook, то в R обычно вместо него используется великолепный RMarkdown — с помощью которого и написан этот онлайн-учебник, кстати говоря. И с RMarkdown мы тоже будем разбираться! 2.2 Знакомство с RStudio Так, давайте взглянем на то, что нам тут открылось: В первую очередь нас интересуют два окна: 1 - Code Editor (окно для написания скриптов)1 и 2 - R Console (консоль). Здесь можно писать команды и запускать их. При этом работа в консоли и работа со скриптом немного различается. В 2 - R Console вы пишите команду и запускаете ее нажиманием Enter. Иногда после запуска команды появляется какой-то результат. Если нажимать стрелку вверх на клавиатуре, то можно выводить в консоль предыдущие команды. Это очень удобно для запуска предыдущих команд с небольшими изменениями. В 1 - Code Editor для запуска команды вы должны выделить ее и нажать Ctrl + Enter (Cmd + Enter на macOS). Если не нажать эту комбинацию клавиш, то команда не запустится. Можно выделить и запустить сразу несколько команд или даже все команды скрипта. Все команды скрипта можно выделить с помощью сочетания клавиш Ctrl + A на Windows и Linux, Cmd + A на macOS.2 Как только вы запустите команду (или несколько команд), соответствующие строчки кода появятся в 2 - R Console, как будто бы вы запускали их прямо там. Обычно в консоли удобно что-то писать, чтобы быстро что-то посчитать. Скрипты удобнее при работе с длинными командами и как способ сохранения написанного кода для дальнейшей работы. Для сохранения скрипта нажмите File - Save As.... R скрипты сохраняются с разрешением .R, но по своей сути это просто текстовые файлы, которые можно открыть и модифицировать в любом текстовом редакторе а-ля “Блокнот.” 3 - Workspace and History — здесь можно увидеть переменные. Это поле будет автоматически обновляться по мере того, как Вы будете запускать строчки кода и создавать новые переменные. Еще там есть вкладка с историей всех команд, которые были запущены. 4 - Plots and files. Здесь есть очень много всего. Во-первых, небольшой файловый менеджер, во-вторых, там будут появляться графики, когда вы будете их рисовать. Там же есть вкладка с вашими пакетами (Packages) и Help по функциям. Но об этом потом. 2.3 R как калькулятор R — полноценный язык программирования, который позволяет решать широкий спектр задач. Но в первую очередь R используется для анализа данных и статистических вычислений. Тем не менее, многими R до сих пор воспринимается как просто продвинутый калькулятор. Ну что ж, калькулятор, так калькулятор. Давайте начнем с самого простого и попробуем использовать R как калькулятор с помощью арифметических операторов +, -, *, /, ^ (степень), () и т.д. Просто запускайте в консоли пока не надоест: 40+2 ## [1] 42 3-2 ## [1] 1 5*6 ## [1] 30 99/9 #деление ## [1] 11 2^3 #степень ## [1] 8 13 %/% 3 #целочисленное деление ## [1] 4 13 %% 3 #остаток от деления ## [1] 1 Попробуйте самостоятельно посчитать что-нибудь с разными числами. Ничего сложного, верно? Вводим выражение и получаем результат. Вы могли заметить, что некоторые команды у меня заканчиваются знаком решетки (#). Все, что написано в строчке после # игнорируется R при выполнении команды. Написанные команды в скрипте рекомендуется сопровождать комментариями, которые будут объяснять вам же в будущем (или кому-то еще), что конкретно происходит в соответствующем куске кода.3 Кроме того, комментарии можно использовать в тех случаях, когда вы хотите написать кусок кода по-другому, не стирая полностью предыдущий код: достаточно “закомментить” нужные строчки - поставить # в начало каждой строки, которую вы хотите переписать. Для этого есть специальное сочетание горячих клавиш: Ctrl + Shift + C (Cmd + Shift + C на macOS) — во всех выделенных строчках будет написан # в начале. Согласно данным навязчивых рекламных баннеров в интернете, только 14% россиян могут справиться с этим примером: 2 + 2 * 2 ## [1] 6 На самом деле, разные языки программирования ведут себя по-разному в таких ситуациях, поэтому ответ 6 (сначала умножаем, потом складываем) не так очевиден. Порядок выполнения арифметических операций (т.е. приоритет операторов, operator precedence) в R как в математике, так что не забывайте про скобочки. (2+2)*2 ## [1] 8 Если Вы не уверены в том, какие операторы имеют приоритет, то используйте скобочки, чтобы точно обозначить, в каком порядке нужно производить операции. Или же смотрите на таблицу приоритета операторов с помощью команды ?Syntax. 2.4 Функции Давайте теперь извлечем корень из какого-нибудь числа. В принципе, тем, кто помнит школьный курс математики, возведения в степень вполне достаточно: 16 ^ 0.5 ## [1] 4 Ну а если нет, то можете воспользоваться специальной функцией: это обычно какие-то буквенные символы с круглыми скобками сразу после названия функции. Мы подаем на вход (внутрь скобочек) какие-то данные, внутри этих функций происходят какие-то вычисления, которые выдает в ответ какие-то другие данные (или же функция записывает файл, рисует график и т.д.). Вот, например, функция для корня: sqrt(16) ## [1] 4 R — case-sensitive язык, т.е. регистр важен. SQRT(16) не будет работать. А вот так выглядит функция логарифма: log(8) ## [1] 2.079442 Так, вроде бы все нормально, но… Если Вы еще что-то помните из школьной математики, то должны понимать, что что-то здесь не так. Здесь не хватает основания логарифма! Логарифм — показатель степени, в которую надо возвести число, называемое основанием, чтобы получить данное число. То есть у логарифма 8 по основанию 2 будет значение 3: \\(\\log_2 8 = 3\\) То есть если возвести 2 в степень 3 у нас будет 8: \\(2^3 = 8\\) Только наша функция считает все как-то не так. Чтобы понять, что происходит, нам нужно залезть в хэлп этой функции: ?log Справа внизу в RStudio появится вот такое окно: Действительно, у этой функции есть еще аргумент base =. По умолчанию он равен числу Эйлера (2.7182818…), т.е. функция считает натуральный логарифм. В большинстве функций R есть какой-то основной инпут — данные в том или ином формате, а есть и дополнительные параметры, которые можно прописывать вручную, если параметры по умолчанию вас не устраивают. log(x = 8, base = 2) ## [1] 3 …или просто (если Вы уверены в порядке переменных): log(8,2) ## [1] 3 Более того, Вы можете использовать результат выполнения одних функций в качестве аргумента для других: log(8, sqrt(4)) ## [1] 3 Если эксплицитно писать имена аргументов, то их порядок в функции не важен: log(base = 2, x = 8) ## [1] 3 А еще можно писать имена аргументов не полностью, если они не совпадают с другими: log(b = 2, x = 8) ## [1] 3 Мы еще много раз будем возвращаться к функциям. Вообще, функции — это одна из важнейших штук в R (примерно так же как и в Python). Мы будем создавать свои функции, использовать функции как инпут для функций и многое-многое другое. В R очень крутые возможности работы с функциями. Поэтому подружитесь с функциями, они клевые. Арифметические знаки, которые мы использовали: +,-,/,^ и т.д. называются операторами и на самом деле тоже являются функциями: &#39;+&#39;(3,4) ## [1] 7 2.5 В любой непонятной ситуации — гуглите Если вдруг вы не знаете, что искать в хэлпе, или хэлпа попросту недостаточно, то… гуглите! Нет ничего постыдного в том, чтобы гуглить решения проблем. Это абсолютно нормально. Используйте силу интернета во благо и да помогут вам Stackoverflow4 и бесчисленные R-туториалы! Computer Programming To Be Officially Renamed “Googling Stack Overflow”Source: http://t.co/xu7acfXvFF pic.twitter.com/iJ9k7aAVhd — Stack Exchange ((StackExchange?)) July 20, 2015 Главное, помните: загуглить работающий ответ всегда недостаточно. Надо понять, как и почему решение работает. Иначе что-то обязательно пойдет не так. Кроме того, правильно загуглить проблему — не так уж и просто. Does anyone ever get good at R or do they just get good at googling how to do things in R — 🔬🖤Lauren M. Seyler, Ph.D.❤️ ((mousquemere?)) May 6, 2019 Короче говоря: гуглить — хорошо, бездумно копировать чужие решения — плохо. 2.6 Переменные Важная штука в программировании на практически любом языке — возможность сохранять значения в переменных. В R это обычно делается с помощью вот этих символов: &lt;- (но можно использовать и обычное =, хотя это не очень принято). Для этого есть удобное сочетание клавиш: нажмите одновременно Alt + - (или option + - на macOS). Заметьте, при присвоении результат вычисления не выводится в консоль! Если опустить детали, то обычно результат выполнения комманды либо выводится в консоль, либо записывается в переменную. a &lt;- 2 a ## [1] 2 Справа от &lt;- находится значение, которое вы хотите сохранить, или же какое-то выражение, результат которого вы хотите сохранить в эту переменную5: Слева от &lt;- находится название будущей переменной. Название переменных может быть самым разным. Есть несколько ограничений для синтаксически валидных имен переменных: они должны включать в себя буквы, цифры, . или _, начинаться на букву (или точку, за которой не будет следовать цифра), не должны совпадать с коротким списком зарезервированных слов. Короче говоря, название не должно включать в себя пробелы и большинство других знаков. Нельзя: - new variable - _new_variable - .1var - v-r Можно: - new_variable - .new.variable - var_2 Обязательно делайте названия переменных осмысленными! Старайтесь делать при этом их понятными и короткими, это сохранит вам очень много времени, когда вы (или кто-то еще) будете пытаться разобраться в написанном ранее коде. Если название все-таки получается длинным и состоящим из нескольких слов, то лучше всего использовать нижнее подчеркивание в качестве разделителя: some_variable6. После присвоения переменная появляется во вкладке Environment в RStudio: Можно использовать переменные в функциях и просто вычислениях: b &lt;- a ^ a + a * a b ## [1] 8 log(b, a) ## [1] 3 2.7 Логические операторы Вы можете сравнивать разные переменные: a == b ## [1] FALSE Заметьте, что сравнивая две переменные мы используем два знака равно ==, а не один =. Иначе это будет означать присвоение. a = b a ## [1] 8 Теперь Вы сможете понять комикс про восстание роботов на следующей странице (пусть он и совсем про другой язык программирования) Этот комикс объясняет, как важно не путать присваивание и сравнение (хотя я иногда путаю до сих пор =( ). Иногда нам нужно проверить на неравенство: a &lt;- 2 b &lt;- 3 a == b ## [1] FALSE a != b ## [1] TRUE Восклицательный язык в программировании вообще и в R в частности стандартно означает отрицание. Еще мы можем сравнивать на больше/меньше: a &gt; b ## [1] FALSE a &lt; b ## [1] TRUE a &gt;= b ## [1] FALSE a &lt;= b ## [1] TRUE Этим мы будем пользоваться в дальнейшем регулярно! Именно на таких простых логических операциях построено большинство операций с данными. ##Типы данных {#data_types} До этого момента мы работали только с числами (numeric): class(a) ## [1] &quot;numeric&quot; На самом деле, в R три типа numeric: integer (целые), double (дробные), complex (комплексные числа)7. R сам будет конвертировать числа в нужный тип numeric при необходимости, поэтому этим можно не заморачиваться. Если же все-таки нужно задать конкретный тип числа эксплицитно, то можно воспользоваться функциями as.integer(), as.double() и as.complex(). Кроме того, при создании числа можно поставить в конце L, чтобы обозначить число как integer: is.integer(5) ## [1] FALSE is.integer(5L) ## [1] TRUE Про double есть еще один маленький секрет. Дело в том, что дробные числа хранятся в R как числа с плавающей запятой двойной точности. Дробные числа в компьютере могут быть записаны только с определенной степенью точности, поэтому иногда встречаются вот такие вот ситуации: sqrt(2)^2 == 2 ## [1] FALSE Это довольно стандартная ситуация, характерная не только для R. Чтобы ее избежать, можно воспользоваться функцией all.equal(): all.equal(sqrt(2)^2, 2) ## [1] TRUE Теперь нам нужно ознакомиться с двумя другими важными типами данных в R: Строковые (character) данные: набор букв, цифр и символов, которые должны выделяться кавычками. s &lt;- &quot;Всем привет!&quot; s ## [1] &quot;Всем привет!&quot; class(s) ## [1] &quot;character&quot; Можно использовать как \", так и ' (что удобно, когда строчка внутри уже содержит какие-то кавычки). &quot;Ph&#39;nglui mglw&#39;nafh Cthulhu R&#39;lyeh wgah&#39;nagl fhtagn&quot; ## [1] &quot;Ph&#39;nglui mglw&#39;nafh Cthulhu R&#39;lyeh wgah&#39;nagl fhtagn&quot; Логические (logical) данные: просто TRUE или FALSE. t1 &lt;- TRUE f1 &lt;- FALSE t1 ## [1] TRUE f1 ## [1] FALSE Вообще, можно еще писать T и F (но не True и False!) t2 &lt;- T f2 &lt;- F Это плохая практика, так как R защищает от перезаписи переменные TRUE и FALSE, но не защищает от этого T и F. TRUE &lt;- FALSE ## Error in TRUE &lt;- FALSE: неправильная (do_set) левая сторона в присвоении TRUE ## [1] TRUE T &lt;- FALSE T ## [1] FALSE Функция rm() позволяет удалить ненужную переменную: rm(T) Мы уже встречались с логическими значениями при сравнении двух числовых переменных. Теперь вы можете догадаться, что результаты сравнения, например, числовых или строковых переменных, можно тоже сохранять в переменные! comparison &lt;- a == b comparison ## [1] FALSE Это нам очень понадобится, когда мы будем работать с реальными данными: нам нужно будет постоянно вытаскивать какие-то данные из датасета, что как раз и построено на игре со сравнением переменных. Чтобы этим хорошо уметь пользоваться, нам нужно еще освоить как работать с логическими операторами. Про один мы немного уже говорили — это логическое НЕ (!). ! превращает TRUE в FALSE, а FALSE в TRUE: t1 ## [1] TRUE !t1 ## [1] FALSE !!t1 #Двойное отрицание! ## [1] TRUE Еще есть логическое И (выдаст TRUE только в том случае если обе переменные TRUE): t1 &amp; t2 ## [1] TRUE t1 &amp; f1 ## [1] FALSE А еще логическое ИЛИ (выдаст TRUE в случае если хотя бы одна из переменных TRUE): t1 | f1 ## [1] TRUE f1 | f2 ## [1] FALSE Если кому-то вдруг понадобится другое ИЛИ (строгое ЛИБО) — есть функция xor(), принимающая два аргумента и возвращая TRUE только в том случае, если ровно один из двух аргументов равен TRUE. Итак, мы только что разобрались с самой занудной (хотя и важной) частью - с основными типа данных в R и как с ними работать8. Пора переходить к чему-то более интересному и специфическому для R. Вперед к ВЕКТОРАМ! При первом запуске RStudio вы не увидите это окно. Для того, чтобы оно появилось, нужно нажать File - New File - R Script.↩︎ В RStudio есть много удобных сочетаний горячих клавиш. Чтобы посмотреть их все, нажмите Help - Keyboard Shortcuts Help.↩︎ Во время написания кода вам может казаться понятным то, что вы написали, но при возвращении к коду через некоторое время вы уже не будете этого помнить. Старайтесь писать комментарии как можно чаще!↩︎ Stackoverflow — это сайт с вопросами и ответами. Эдакий аналог Quora, The Question, ну или Ответы Mail.ru в мире программирования.↩︎ Есть еще оператор -&gt;, который позволяет присваивать значения слева направо, но так делать не рекомендуется, хотя это бывает довольно удобным.↩︎ Еще иногда используются большие буквы SomeVariable, но это плохо читается, а иногда — точка, но это тоже не рекомендуется.↩︎ Комплексные числа в R пишутся так: complexnumber &lt;- 2+2i. i здесь - это та самая мнимая единица, которая является квадратным корнем из -1.↩︎ Кроме описанных пяти типов данных (integer, double, complex, character и logical) есть еще и шестой — это raw, сырая последовательность байтов, но нам она не понадобится.↩︎ "],["vector.html", "3 Вектор 3.1 Понятие atomic вектора в R 3.2 Приведение типов 3.3 Векторизация 3.4 Ресайклинг 3.5 Индексирование векторов 3.6 Работа с логическими векторами 3.7 NA - пропущенные значения 3.8 Заключение", " 3 Вектор 3.1 Понятие atomic вектора в R Если у вас не было линейной алгебры (или у вас с ней было все плохо), то просто запомните, что вектор (atomic vector или просто atomic) — это набор (столбик) чисел в определенном порядке. Если вы привыкли из школьного курса физики считать вектора стрелочками, то не спешите возмущаться и паниковать. Представьте стрелочки как точки из нуля координат {0,0} до какой-то точки на координатной плоскости, например, {2,3}: Вот последние два числа и будем считать вектором. Попытайтесь теперь мысленно стереть координатную плоскость и выбросить стрелочки из головы, оставив только последовательность чисел {2,3}: На самом деле, мы уже работали с векторами в R, но, возможно, вы об этом даже не догадывались. Дело в том, что в R нет как таковых скалярных (т.е. одиночных) значений, есть вектора длиной 1. Такие дела! Чтобы создать вектор из нескольких значений, нужно воспользоваться функцией c(): c(4, 8, 15, 16, 23, 42) ## [1] 4 8 15 16 23 42 c(&quot;Хэй&quot;, &quot;Хэй&quot;, &quot;Ха&quot;) ## [1] &quot;Хэй&quot; &quot;Хэй&quot; &quot;Ха&quot; c(TRUE, FALSE) ## [1] TRUE FALSE Одна из самых мерзких и раздражающих причин ошибок в коде — это использование с из кириллицы вместо c из латиницы. Видите разницу? И я не вижу. А R видит. И об этом сообщает: с(3, 4, 5) ## Error in с(3, 4, 5): не могу найти функцию &quot;с&quot; Для создания числовых векторов есть удобный оператор :. 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 5:-3 ## [1] 5 4 3 2 1 0 -1 -2 -3 Этот оператор создает вектор от первого числа до второго с шагом 1. Вы не представляете, как часто эта штука нам пригодится… Если же нужно сделать вектор с другим шагом, то есть функция seq(): seq(10, 100, by = 10) ## [1] 10 20 30 40 50 60 70 80 90 100 Кроме того, можно задавать не шаг, а длину вектора. Тогда функция seq() сама посчитает шаг: seq(1, 13, length.out = 4) ## [1] 1 5 9 13 Другая функция — rep() — позволяет создавать вектора с повторяющимися значениями. Первый аргумент — значение, которое нужно повторять, а второй аргумент — сколько раз повторять. rep(1, 5) ## [1] 1 1 1 1 1 И первый, и второй аргумент могут быть векторами! rep(1:3, 3) ## [1] 1 2 3 1 2 3 1 2 3 rep(1:3, 1:3) ## [1] 1 2 2 3 3 3 Еще можно объединять вектора (что мы, по сути, и делали, просто с векторами длиной 1): v1 &lt;- c(&quot;Hey&quot;, &quot;Ho&quot;) v2 &lt;- c(&quot;Let&#39;s&quot;, &quot;Go!&quot;) c(v1, v2) ## [1] &quot;Hey&quot; &quot;Ho&quot; &quot;Let&#39;s&quot; &quot;Go!&quot; Очень многие функции в R работают именно с векторами. Например, функции sum() (считает сумму значений вектора) и mean() (считает среднее арифметическое всех значений в векторе): sum(1:10) ## [1] 55 mean(1:10) ## [1] 5.5 3.2 Приведение типов Что будет, если вы объедините два вектора с значениями разных типов? Ошибка? Мы уже обсуждали, что в обычных векторах (atomic векторах) может быть только один тип данных. В некоторых языках программирования при операции с данными разных типов мы бы получили ошибку. А вот в R при несовпадении типов произойдет попытка привести типы к “общему знаменателю,” то есть конвертировать данные в более “широкий” тип (а иногда — более “узкий” тип, если того требует функция). Например: c(FALSE, 2) ## [1] 0 2 FALSE превратился в 0 (а TRUE превратился бы в 1), чтобы оба значения можно было объединить в вектор. То же самое произошло бы в случае операций с векторами: 2 + TRUE ## [1] 3 Это называется неявным приведением типов (implicit coercion). Вот более сложный пример: c(TRUE, 3, &quot;Привет&quot;) ## [1] &quot;TRUE&quot; &quot;3&quot; &quot;Привет&quot; Здесь все значения были приведены сразу к строковому типу данных. У R есть иерархия приведения типов: NULL &lt; raw &lt; logical &lt; integer &lt; double &lt; complex &lt; character &lt; list &lt; expression. Мы из этого списка еще многого не знаем, сейчас важно запомнить, что логические данные — TRUE и FALSE — превращаются в 0 и 1 соответственно, а 0 и 1 в строчки \"0\" и \"1\". Если Вы боитесь полагаться на приведение типов, то можете воспользоваться функциями as.нужныйтипданных для явного приведения типов (explicit coercion): as.numeric(c(TRUE, FALSE, FALSE)) ## [1] 1 0 0 as.character(as.numeric(c(TRUE, FALSE, FALSE))) ## [1] &quot;1&quot; &quot;0&quot; &quot;0&quot; Можно превращать и обратно, например, строковые значения в числовые. Если среди числа встретится буква или другой неподходящий знак, то мы получим предупреждение NA — пропущенное значение (мы очень скоро научимся с ними работать). as.numeric(c(&quot;1&quot;, &quot;2&quot;, &quot;три&quot;)) ## Warning: в результате преобразования созданы NA ## [1] 1 2 NA Один из распространенных примеров использования неявного приведения типов — использования функций sum() и mean() для подсчета в логическом векторе количества и доли TRUE соответсвенно. Мы будем много раз пользоваться этим приемом в дальнейшем! 3.3 Векторизация Все те арифметические операторы, что мы использовали ранее, можно использовать с векторами одинаковой длины: n &lt;- 1:4 m &lt;- 4:1 n + m ## [1] 5 5 5 5 n - m ## [1] -3 -1 1 3 n * m ## [1] 4 6 6 4 n / m ## [1] 0.2500000 0.6666667 1.5000000 4.0000000 n ^ m + m * (n - m) ## [1] -11 5 11 7 Если применить операторы на двух векторах одинаковой длины, то мы получим результат поэлементного применения оператора к двум векторам. Это называется векторизацией (vectorization). Если после какого-нибудь MATLAB Вы привыкли, что по умолчанию операторы работают по правилам линейной алгебры и m * n будет давать скалярное произведение (dot product), то снова нет. Для скалярного произведения нужно использовать операторы с % по краям: n %*% m ## [,1] ## [1,] 20 Абсолютно так же и с операциями с матрицами в R, хотя про матрицы будет немного позже. В принципе, большинство функций в R, которые работают с отдельными значениями, так же хорошо работают и с целыми векторами. Скажем, если вы хотите извлечь корень из нескольких чисел, то для этого не нужны никакие циклы (как это обычно делается во многих других языках программирования). Можно просто “скормить” вектор функции и получить результат применения функции к каждому элементу вектора: sqrt(1:10) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Таких векторизованных функций в R очень много. Многие из них написаны на более низкоуровневых языках программирования (C, C++, FORTRAN), за счет чего использование таких функций приводит не только к более элегантному, лаконичному, но и к более быстрому коду. Векторизация в R — это очень важная фишка, которая отличает этот язык программирования от многих других. Если вы уже имеете опыт программирования на другом языке, то вам во многих задачах захочется использовать циклы типа for и while 7.2. Не спешите этого делать! В очень многих случаях циклы можно заменить векторизацией. Тем не менее, векторизация — это не единственный способ избавить от циклов типа for и while 8.5.1. 3.4 Ресайклинг Допустим мы хотим совершить какую-нибудь операцию с двумя векторами. Как мы убедились, с этим обычно нет никаких проблем, если они совпадают по длине. А что если вектора не совпадают по длине? Ничего страшного! Здесь будет работать правило ресайклинга (правило переписывания, recycling rule). Это означает, что если мы делаем операцию на двух векторах разной длины, то если короткий вектор кратен по длине длинному, короткий вектор будет повторяться необходимое количество раз: n &lt;- 1:4 m &lt;- 1:2 n * m ## [1] 1 4 3 8 А что будет, если совершать операции с вектором и отдельным значением? Можно считать это частным случаем ресайклинга: короткий вектор длиной 1 будет повторятся столько раз, сколько нужно, чтобы он совпадал по длине с длинным: n * 2 ## [1] 2 4 6 8 Если же меньший вектор не кратен большему (например, один из них длиной 3, а другой длиной 4), то R посчитает результат, но выдаст предупреждение. n + c(3,4,5) ## Warning in n + c(3, 4, 5): длина большего объекта не является произведением ## длины меньшего объекта ## [1] 4 6 8 7 Проблема в том, что эти предупреждения могут в неожиданный момент стать причиной ошибок. Поэтому не стоит полагаться на ресайклинг некратных по длине векторов. А вот ресайклинг кратных по длине векторов — это очень удобная штука, которая используется очень часто. 3.5 Индексирование векторов Итак, мы подошли к одному из самых сложных моментов. И одному из основных. От того, как хорошо вы научись с этим работать, зависит весь ваш дальнейший успех на R-поприще! Речь пойдет об индексировании векторов. Задача, которую Вам придется решать каждые пять минут работы в R — как выбрать из вектора (или же списка, матрицы и датафрейма) какую-то его часть. Для этого используются квадратные скобочки [] (не круглые — они для функций!). Самое простое — индексировать по номеру индекса, т.е. порядку значения в векторе. n &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34) n[1] ## [1] 0 n[10] ## [1] 34 Если вы знакомы с другими языками программирования (не MATLAB, там все так же) и уже научились думать, что индексация с 0 — это очень удобно и очень правильно (ну или просто свыклись с этим), то в R вам придется переучиться обратно. Здесь первый индекс — это 1, а последний равен длине вектора — ее можно узнать с помощью функции length(). С обоих сторон индексы берутся включительно. С помощью индексирования можно не только вытаскивать имеющиеся значения в векторе, но и присваивать им новые: n[3] &lt;- 20 n ## [1] 0 1 20 2 3 5 8 13 21 34 Конечно, можно использовать целые векторы для индексирования: n[4:7] ## [1] 2 3 5 8 n[10:1] ## [1] 34 21 13 8 5 3 2 20 1 0 n[4:6] &lt;- 0 n ## [1] 0 1 20 0 0 0 8 13 21 34 Индексирование с минусом выдаст вам все значения вектора кроме выбранных: n[-1] ## [1] 1 20 0 0 0 8 13 21 34 n[c(-4, -5)] ## [1] 0 1 20 0 8 13 21 34 Минус здесь “выключает” выбранные значения из вектора, а не означает отсчет с конца как в Python. Более того, можно использовать логический вектор для индексирования. В этом случае нужен логический вектор такой же длины: n[c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE)] ## [1] 0 20 0 8 21 Логический вектор работает здесь как фильтр: пропускает только те значения, где на соответствующей позиции в логическом векторе для индексирования содержится TRUE, и не пропускает те значения, где на соответствующей позиции в логическом векторе для индексирования содержится FALSE. Ну а если эти два вектора (исходный вектор и логический вектор индексов) не равны по длине, то тут будет снова работать правило ресайклинга! n[c(TRUE, FALSE)] #то же самое - recycling rule! ## [1] 0 20 0 8 21 Есть еще один способ индексирования векторов, но он несколько более редкий: индексирование по имени. Дело в том, что для значений векторов можно (но не обязательно) присваивать имена: my_named_vector &lt;- c(first = 1, second = 2, third = 3) my_named_vector[&#39;first&#39;] ## first ## 1 А еще можно “вытаскивать” имена из вектора с помощью функции names() и присваивать таким образом новые имена. d &lt;- 1:4 names(d) &lt;- letters[1:4] names(d) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; d[&quot;a&quot;] ## a ## 1 letters — это “зашитая” в R константа — вектор букв от a до z. Иногда это очень удобно! Кроме того, есть константа LETTERS — то же самое, но заглавными буквами. А еще в R есть названия месяцев на английском и числовая константа pi. Вернемся к нашему вектору n и посчитаем его среднее с помощью функции mean(): mean(n) ## [1] 9.7 А как вытащить все значения, которые больше среднего? Сначала получим логический вектор — какие значения больше среднего: larger &lt;- n &gt; mean(n) larger ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE А теперь используем его для индексирования вектора n: n[larger] ## [1] 20 13 21 34 Можно все это сделать в одну строчку: n[n &gt; mean(n)] ## [1] 20 13 21 34 Предыдущая строчка отражает то, что мы будем постоянно делать в R: вычленять (subset) из данных отдельные куски на основании разных условий. 3.6 Работа с логическими векторами На работе с логическими векторами построено очень много удобных фишек, связанных со сравнением условий. eyes &lt;- c(&quot;green&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;brown&quot;, &quot;green&quot;, &quot;blue&quot;) 3.6.1 mean() и sum() для подсчета пропорций и количества TRUE Уже знакомая нам функция sum() позволяет посчитать количество TRUE в логическом векторе. Например, можно удобно посчитать сколько раз значение \"blue\" встречается в векторе eyes: eyes == &quot;blue&quot; ## [1] FALSE TRUE TRUE FALSE FALSE TRUE sum(eyes == &quot;blue&quot;) ## [1] 3 Функцию mean() можно использовать для подсчета пропорций TRUE в логическом векторе. eyes == &quot;blue&quot; ## [1] FALSE TRUE TRUE FALSE FALSE TRUE mean(eyes == &quot;blue&quot;) ## [1] 0.5 Умножив на 100, мы получим долю выраженную в процентах: mean(eyes == &quot;blue&quot;) * 100 ## [1] 50 3.6.2 all() и any() Функция all() выдает TRUE только когда все значения логического вектора на входе равны TRUE: all(eyes == &quot;blue&quot;) ## [1] FALSE Функция any() выдает TRUE когда есть хотя бы одно значение TRUE: any(eyes == &quot;blue&quot;) ## [1] TRUE Вместе с оператором ! можно получить много дополнительных вариантов. Например, есть ли хотя бы один FALSE в векторе? any(!eyes == &quot;blue&quot;) ## [1] TRUE !all(eyes == &quot;blue&quot;) ## [1] TRUE Все ли значения в векторе равны FALSE? all(!eyes == &quot;blue&quot;) ## [1] FALSE !any(eyes == &quot;blue&quot;) ## [1] FALSE 3.6.3 Превращение логических значений в индексы: which() Как вы уже знаете, и логические векторы, и числовые вектора с индексами могут использоваться для индексирования векторов. Иногда может понадобиться превратить логический вектор в вектор индексов. Для этого есть функция which() which(eyes == &quot;blue&quot;) ## [1] 2 3 6 3.6.4 оператор %in% и match() Часто возникает такая задача: нужно проверить вектор на равенство с хотя бы одним значением из другого вектора. Например, мы хотим вычленить всех зеленоглазых и голубоглазых. Может возникнуть идея сделать так: eyes[eyes == c(&quot;green&quot;, &quot;blue&quot;)] ## [1] &quot;green&quot; &quot;blue&quot; &quot;green&quot; &quot;blue&quot; Перед нами самый страшный случай: результат похож на правильный, но не правильный! Попытайтесь самостоятельно понять почему этот ответ неверный и что произошло на самом деле. А на самом деле мы просто сравнили два вектора, один из которых короче другого, следовательно, у нас сработало правило ресайклинга. Как мы видим, это совсем не то, что нам нужно! В данной ситуации нам подойдет сравнение с двумя значениями вместе с логическим ИЛИ. eyes[eyes == &quot;green&quot; | eyes == &quot;blue&quot;] ## [1] &quot;green&quot; &quot;blue&quot; &quot;blue&quot; &quot;green&quot; &quot;blue&quot; Однако это не очень удобно, особенно если значений больше 2. Тогда на помощь приходит оператор %in%, который выполняет именно то, что нам изначально нужно: выдает для каждого значения в векторе слева, есть ли это значение среди значений вектора справа. eyes[eyes %in% c(&quot;green&quot;, &quot;blue&quot;)] ## [1] &quot;green&quot; &quot;blue&quot; &quot;blue&quot; &quot;green&quot; &quot;blue&quot; Основное преимущество оператора %in% в его простоте и понятности. У оператора %in% есть старший брат, более сложный и более мощный. Функция match() работает похожим образом на %in%, но при совпадении значения в левом векторе с одним из значений в правом выдает индекс соответствующего значения вместо TRUE. Если же совпадений нет, то вместо FALSE функция match() выдает NA (что можно поменять параметром nomatch =). match(eyes, c(&quot;green&quot;, &quot;blue&quot;)) ## [1] 1 2 2 NA 1 2 Зачем это может понадобиться? Во-первых, это способ соединить два набора данных (хотя для этого есть и более подходящие инструменты), во-вторых, так можно заменить все значения кроме выбранных заменить на NA. c(&quot;green&quot;, &quot;blue&quot;)[match(eyes, c(&quot;green&quot;, &quot;blue&quot;))] ## [1] &quot;green&quot; &quot;blue&quot; &quot;blue&quot; NA &quot;green&quot; &quot;blue&quot; 3.7 NA - пропущенные значения В реальных данных у нас часто чего-то не хватает. Например, из-за технической ошибки или невнимательности не получилось записать какое-то измерение. Для обозначения пропущенных значений в R есть специальное значение NA (расшифровывается как Not Available - недоступное значение). NA — это не строка \"NA\", не 0, не пустая строка \"\" и не FALSE. NA — это NA. Большинство операций с векторами, содержащими NA будут выдавать NA: missed &lt;- NA missed == &quot;NA&quot; ## [1] NA missed == &quot;&quot; ## [1] NA missed == NA ## [1] NA Заметьте, даже сравнение NA c NA выдает NA. Это может прозвучать абсурдно: ну как же так, и то NA, и другое NA — это же одно и то же, они должны быть равны! Не совсем: NA — это отсутствие информации об объекте, неопределенность, неизвестная нам величина. Если мы не знаем двух значений (т.е. имеем два NA), то это еще не значит, что они равны. Иногда наличие NA в данных очень бесит: n[5] &lt;- NA n ## [1] 0 1 20 0 NA 0 8 13 21 34 mean(n) ## [1] NA Получается, что наличие NA “заражает” неопределенностью все последующие действия. Что же делать? Наверное, надо сравнить вектор с NA и исключить этих пакостников. Давайте попробуем: n == NA ## [1] NA NA NA NA NA NA NA NA NA NA Ах да, мы ведь только что узнали, что даже сравнение NA c NA приводит к NA! Сначала это может показаться нелогичным: ведь с обоих сторон NA, почему же тогда результат их сравнения — это тоже NA, а не TRUE? Дело в том, что сравнивая две неопределенности, вы не можете установить между ними знак равенства. Представим себе двух супергероев: Бэтмена и Спайдермена. Допустим, мы не знаем их рост: Batman &lt;- NA Spiderman &lt;- NA Одинаковый ли у них рост? Batman == Spiderman ## [1] NA Мы не знаем! Возможно, да, возможно, и нет. Поэтому у нас здесь остается неопределенность. Так как же избавиться от NA в данных? Самый простой способ — это функция is.na(): is.na(n) ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE Результат выполнения is.na(n) выдает FALSE на тех позициях, где у нас числа (или другие значения), и TRUE там, где у нас NA. Чтобы вычленить из вектора n все значения кроме NA нам нужно, чтобы было наоборот: TRUE, если это не NA, FALSE, если это NA. Здесь нам понадобится логический оператор НЕ ! (мы его уже встречали — см. @ref(data_types)), который инвертирует логические значения: n[!is.na(n)] ## [1] 0 1 20 0 0 8 13 21 34 Ура, мы можем считать среднее без NA! mean(n[!is.na(n)]) ## [1] 10.77778 Теперь Вы понимаете, зачем нужно отрицание (!) Вообще, есть еще один из способов посчитать среднее, если есть NA. Для этого надо залезть в хэлп по функции mean(): ?mean() В хэлпе мы найдем параметр na.rm =, который по умолчанию FALSE. Вы знаете, что нужно делать! mean(n, na.rm = T) ## [1] 10.77778 NA может появляться в векторах разных типов. На самом деле, NA - это специальное значение в логических векторах, тогда как в векторах других типов NA появляется как NA_integer_, NA_real_, NA_complex_ или NA_character_, но R обычно сам все переводит в нужный формат и показывает как просто NA. Таким образом, NA в векторах разных типов — это разные NA, хотя на практике эта деталь обычно несущественна. Кроме NA есть еще NaN — это разные вещи. NaN расшифровывается как Not a Number и получается в результате таких операций как 0 / 0. Тем не менее, функция is.na() выдает TRUE на NaN, а вот функция is.nan() выдает TRUE на NaN и FALSE на NA: is.na(NA) ## [1] TRUE is.na(NaN) ## [1] TRUE is.nan(NA) ## [1] FALSE is.nan(NaN) ## [1] TRUE 3.8 Заключение Итак, с векторами мы более-менее разобрались. Помните, что вектора — это один из краеугольных камней вашей работы в R. Если вы хорошо с ними разобрались, то дальше все будет довольно несложно. Тем не менее, вектора — это не все. Есть еще два важных типа данных: списки (list) и матрицы (matrix). Их можно рассматривать как своеобразное “расширение” векторов, каждый в свою сторону. Ну а списки и матрицы нужны чтобы понять основной тип данных в R — data.frame. "],["complex-structures.html", "4 Сложные структуры данных в R 4.1 Матрица 4.2 Массив 4.3 Список 4.4 Датафрейм", " 4 Сложные структуры данных в R 4.1 Матрица Если вдруг вас пугает это слово, то совершенно зря. Матрица (matrix) — это всего лишь “двумерный” вектор: вектор, у которого есть не только длина, но и ширина. Создать матрицу можно с помощью функции matrix() из вектора, указав при этом количество строк и столбцов. A &lt;- matrix(1:20, nrow=5,ncol=4) A ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 Заметьте, значения вектора заполняются следующим образом: сначала заполняется первый столбик сверху вниз, потом второй сверху вниз и так до конца, т.е. заполнение значений матрицы идет в первую очередь по вертикали. Это довольно стандартный способ создания матриц, характерный не только для R. Если мы знаем сколько значений в матрице и сколько мы хотим строк, то количество столбцов указывать необязательно: A &lt;- matrix(1:20, nrow=5) A ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 Все остальное так же как и с векторами: внутри находится данные только одного типа. Поскольку матрица — это уже двумерный массив, то у него имеется два индекса. Эти два индекса разделяются запятыми. A[2,3] ## [1] 12 A[2:4, 1:3] ## [,1] [,2] [,3] ## [1,] 2 7 12 ## [2,] 3 8 13 ## [3,] 4 9 14 Первый индекс — выбор строк, второй индекс — выбор колонок. Если же мы оставляем пустое поле вместо числа, то мы выбираем все строки/колонки в зависимости от того, оставили мы поле пустым до или после запятой: A[, 1:3] ## [,1] [,2] [,3] ## [1,] 1 6 11 ## [2,] 2 7 12 ## [3,] 3 8 13 ## [4,] 4 9 14 ## [5,] 5 10 15 A[2:4, ] ## [,1] [,2] [,3] [,4] ## [1,] 2 7 12 17 ## [2,] 3 8 13 18 ## [3,] 4 9 14 19 A[, ] ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 Так же как и в случае с обычными векторами, часть матрицы можно переписать: A[2:4, 2:4] &lt;- 100 A ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 100 100 100 ## [3,] 3 100 100 100 ## [4,] 4 100 100 100 ## [5,] 5 10 15 20 В принципе, это все, что нам нужно знать о матрицах. Матрицы используются в R довольно редко, особенно по сравнению, например, с MATLAB. Но вот индексировать матрицы хорошо бы уметь: это понадобится в работе с датафреймами. То, что матрица — это просто двумерный вектор, не является метафорой: в R матрица — это по сути своей вектор с дополнительными атрибутами dim и (опционально) dimnames. Атрибуты — это свойства объектов, своего рода “метаданные.” Для всех объектов есть обязательные атрибуты типа и длины и могут быть любые необязательные атрибуты. Можно задавать свои атрибуты или удалять уже присвоенные: удаление атрибута dim у матрицы превратит ее в обычный вектор. Про атрибуты подробнее можно почитать здесь или на стр. 99-101 книги “R in a Nutshell” (Adler 2010). 4.2 Массив Два измерения — это не предел! Структура с одним типом данных внутри, но с тремя измерениями или больше, называется массивом (array). Создание массива очень похоже на создание матрицы: задаем вектор, из которого будет собран массив, и размерность массива. array_3d &lt;- array(1:12, c(3, 2, 2)) array_3d ## , , 1 ## ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 ## ## , , 2 ## ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 ## [3,] 9 12 4.3 Список Теперь представим себе вектор без ограничения на одинаковые данные внутри. И получим список (list)! simple_list &lt;- list(42, &quot;Пам пам&quot;, TRUE) simple_list ## [[1]] ## [1] 42 ## ## [[2]] ## [1] &quot;Пам пам&quot; ## ## [[3]] ## [1] TRUE А это значит, что там могут содержаться самые разные данные, в том числе и другие списки и векторы! complex_list &lt;- list(c(&quot;Wow&quot;, &quot;this&quot;, &quot;list&quot;, &quot;is&quot;, &quot;so&quot;, &quot;big&quot;), &quot;16&quot;, simple_list) complex_list ## [[1]] ## [1] &quot;Wow&quot; &quot;this&quot; &quot;list&quot; &quot;is&quot; &quot;so&quot; &quot;big&quot; ## ## [[2]] ## [1] &quot;16&quot; ## ## [[3]] ## [[3]][[1]] ## [1] 42 ## ## [[3]][[2]] ## [1] &quot;Пам пам&quot; ## ## [[3]][[3]] ## [1] TRUE Если у нас сложный список, то есть очень классная функция, чтобы посмотреть, как он устроен, под названием str(): str(complex_list) ## List of 3 ## $ : chr [1:6] &quot;Wow&quot; &quot;this&quot; &quot;list&quot; &quot;is&quot; ... ## $ : chr &quot;16&quot; ## $ :List of 3 ## ..$ : num 42 ## ..$ : chr &quot;Пам пам&quot; ## ..$ : logi TRUE Представьте, что список - это такое дерево с ветвистой структурой. А на конце этих ветвей - листья-векторы. Как и в случае с векторами мы можем давать имена элементам списка: named_list &lt;- list(age = 24, PhDstudent = T, language = &quot;Russian&quot;) named_list ## $age ## [1] 24 ## ## $PhDstudent ## [1] TRUE ## ## $language ## [1] &quot;Russian&quot; К списку можно обращаться как с помощью индексов, так и по именам. Начнем с последнего: named_list$age ## [1] 24 А вот с индексами сложнее, и в этом очень легко запутаться. Давайте попробуем сделать так, как мы делали это раньше: named_list[1] ## $age ## [1] 24 Мы, по сути, получили элемент списка — просто как часть списка, т.е. как список длиной один: class(named_list) ## [1] &quot;list&quot; class(named_list[1]) ## [1] &quot;list&quot; А вот чтобы добраться до самого элемента списка (и сделать с ним что-то хорошее), нам нужна не одна, а две квадратных скобочки: named_list[[1]] ## [1] 24 class(named_list[[1]]) ## [1] &quot;numeric&quot; Indexing lists in #rstats. Inspired by the Residence Inn pic.twitter.com/YQ6axb2w7t — Hadley Wickham ((hadleywickham?)) September 14, 2015 Как и в случае с вектором, к элементу списка можно обращаться по имени. named_list[[&#39;age&#39;]] ## [1] 24 Хотя последнее — практически то же самое, что и использование знака $. Списки довольно часто используются в R, но реже, чем в Python. Со многими объектами в R, такими как результаты статистических тестов, удобно работать именно как со списками — к ним все вышеописанное применимо. Кроме того, некоторые данные мы изначально получаем в виде древообразной структуры — хочешь не хочешь, а придется работать с этим как со списком. Но обычно после этого стоит как можно скорее превратить список в датафрейм. 4.4 Датафрейм Итак, мы перешли к самому главному. Самому-самому. Датафреймы (data.frames). Более того, сейчас станет понятно, зачем нам нужно было разбираться со всеми предыдущими темами. Без векторов мы не смогли бы разобраться с матрицами и списками. А без последних мы не сможем понять, что такое датафрейм. name &lt;- c(&quot;Petr&quot;, &quot;Eugeny&quot;, &quot;Lena&quot;, &quot;Misha&quot;, &quot;Sasha&quot;) age &lt;- c(26, 34, 23, 27, 26) student &lt;- c(F, F, T, T, T) df &lt;- data.frame(name, age, student) df str(df) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ name : chr &quot;Petr&quot; &quot;Eugeny&quot; &quot;Lena&quot; &quot;Misha&quot; ... ## $ age : num 26 34 23 27 26 ## $ student: logi FALSE FALSE TRUE TRUE TRUE Вообще, очень похоже на список, не правда ли? Так и есть, датафрейм — это что-то вроде проименованного списка, каждый элемент которого является atomic вектором фиксированной длины. Скорее всего, вы представляли список “горизонтально.” Если это так, то теперь “переверните” список у себя в голове на 90 градусов. Так, чтобы названия векторов оказались сверху, а элементы списка стали столбцами. Поскольку длина всех этих векторов одинаковая (обязательное условие!), то данные представляют собой табличку, похожую на матрицу. Но в отличие от матрицы, разные столбцы могут иметь разные типы данных. В нашем случае первая колонка — character, вторая колонка — numeric, третья колонка — logical. Тем не менее, обращаться с датафреймом можно и как с проименованным списком, и как с матрицей: df$age[2:3] ## [1] 34 23 Здесь мы сначала ивлекли колонку age с помощью оператора $. Результатом этой операции является числовой вектор, из которого мы вытащили кусок, выбрав индексы 2 и 3. Используя оператор $ и присваивание можно создавать новые колонки датафрейма: df$lovesR &lt;- TRUE #правило recycling - узнали? df Ну а можно просто обращаться с помощью двух индексов через запятую, как мы это делали с матрицей: df[3:5, 2:3] Как и с матрицами, первый индекс означает строчки, а второй — столбцы. А еще можно использовать названия колонок внутри квадратных скобок: df[1:2, &quot;age&quot;] ## [1] 26 34 И здесь перед нами открываются невообразимые возможности! Узнаем, любят ли R те, кто моложе среднего возраста в группе: df[df$age &lt; mean(df$age), 4] ## [1] TRUE TRUE TRUE TRUE Эту же задачу можно выполнить другими способами: df$lovesR[df$age &lt; mean(df$age)] ## [1] TRUE TRUE TRUE TRUE df[df$age &lt; mean(df$age), &#39;lovesR&#39;] ## [1] TRUE TRUE TRUE TRUE В большинстве случаев подходят сразу несколько способов — тем не менее, стоит овладеть ими всеми. Датафреймы удобно просматривать в RStudio. Для это нужно написать команду View(df) или же просто нажать на названии нужной переменной из списка вверху справа (там где Environment). Тогда увидите табличку, очень похожую на Excel и тому подобные программы для работы с таблицами. Там же есть и всякие возможности для фильтрации, сортировки и поиска.9 Но, конечно, интереснее все эти вещи делать руками, т.е. с помощью написания кода. Все, что вы нажмете в этом окошке, никак не повлияет на исходную переменную. Так что можете смело использовать эти функции для исследования содержимого датафрейма.↩︎ "],["r-packages.html", "5 Пакеты в R 5.1 Дополнительные пакеты 5.2 Встроенные пакеты R 5.3 Установка пакетов с CRAN 5.4 Загрузка установленного пакета 5.5 Вызов функции из пакета с помощью :: 5.6 Установка пакетов c Bioconductor 5.7 Установка пакетов с Github 5.8 Где искать нужные пакеты", " 5 Пакеты в R 5.1 Дополнительные пакеты R — очень богатый язык с широкими возможностями. Однако очень скоро мы поймем, что этих возможностей нам не хватает. Эти возможности нам могут предоставить дополнительные пакеты (packages). В большинстве случаев основным содержанием пакетов является набор дополнительных функций. Кроме функций, пакеты могут содержать наборы данных и новые структуры данных. Обычно пакеты посвящены решению какого-то класса задач в определенной области. Например, есть множество пакетов для создания какого-то одного типа визуализации. Еще один пример — пакет beepr, который содержит всего две функции: beep() и beep_on_error() для воспроизведения звукового сигнала. Это может быть удобно, если ваш скрипт работает долго, но вы хотите получить уведомление, когда его выполнение завершится. Более крупные пакеты посвящены целому классу задач. Например, пакеты stringi и stringr посвящены работе со строками, значительно расширяя и делая более удобной работу со строковыми данными в R. Еще один пример: пакет igraph для работы с графами (сетями). Этот пакет предоставляет дополнительный класс данных igraph для хранения и работы с сетями. Есть и совсем крупные пакеты, которые значительно расширяют базовый функционал R, изменяя основные принципы работы в нем. Это пакеты data.table и tidyverse. Это настолько крупные пакеты, что их даже называют отдельными диалектами R, потому что код, написанный с использованием этих пакетов, довольно сильно отличается от базового R. Кроме того, tidyverse - это не просто пакет, а целая экосистема пакетов, который взаимодополняют друг друга, но для удобства их можно устанавливать и загружать как один пакет tidyverse. Еще один пример крупной экосистемы из пакетов — это пакет mlr3 для машинного обучения, который представляет собой большой расширяемый “пакет пакетов,” где отдельные пакеты посвящены отдельным этапам и задачам машинного обучения. 5.2 Встроенные пакеты R Вообще, даже сам R является набором из нескольких пакетов: основного base и нескольких других, таких как stats, utils, graphics. Вот их полный список: rownames(installed.packages(priority = &quot;base&quot;)) ## [1] &quot;base&quot; &quot;compiler&quot; &quot;datasets&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;grid&quot; ## [7] &quot;methods&quot; &quot;parallel&quot; &quot;splines&quot; &quot;stats&quot; &quot;stats4&quot; &quot;tcltk&quot; ## [13] &quot;tools&quot; &quot;utils&quot; Чтобы пользоваться этими пакетами ничего дополнительно делать не нужно. 5.3 Установка пакетов с CRAN Функция install.packages() позволяет скачивать пакеты с Comprehensive R Archive Network (CRAN). На репозитории CRAN собрано более 16000 пакетов. Каждый из этих пакетов проходит проверку перед попаданием в CRAN: он должен быть хорошо задокументирован, стабильно работать и решать какую-то задачу. Для примера установим пакет remotes. Это пакет для удобной установки пакетов не с CRAN и скоро нам понадобится. install.packages(&quot;remotes&quot;) При установке вы увидите много непонятных надписей красным шрифтом. Не пугайтесь, это нормально, происходит скачивание и установка пакетов. В конце вы увидите что-то вроде этого: !()images/install_success.png Иногда установка бывает очень долгой, потому что большие пакеты склонны иметь много зависимостей: для работы какого-то пакета может понадобиться другие пакеты, а для тех пакетов - еще какие-то пакеты. Таким образом, устанавливая какой-нибудь современный пакет, вы, возможно, установите десятки других пакетов! Зато если они понадобятся сами по себе, то их уже не нужно будет устанавливать. 5.4 Загрузка установленного пакета Установить пакет с помощью install.packages() недостаточно, пакет нужно еще загрузить. Для этого есть функция library(). library(&quot;remotes&quot;) В отличие от install.packages(), функция library() принимает название пакета и как строчку в кавычках, и как название без кавычек. library(remotes) Теперь функции, данные и классы из пакета доступны для работы. 5.5 Вызов функции из пакета с помощью :: Если пакетом нужно воспользоваться всего один-два раза, то имеет смысл не подключать весь пакет, а загрузить отдельную функцию из него. Для этого есть специальный оператор ::, который использует функцию (указанную справа от ::) из выбранного пакета (указанного слева от ::), не загружая пакет полностью. Для примера воспользуемся функцией package_deps() из только что установленного пакета remotes, которая возвращает все зависимости пакета: remotes::package_deps(&quot;tidyverse&quot;) В дальнейшем использование оператора :: будет иногда использоваться, чтобы указать, из какого пакета взята функция. Оператор :: полезен еще и в тех случаях, когда в разных пакетах присутствуют функции с одинаковым названием. Например, у основного пакета tidyverse, dplyr, есть функция filter(). Функция с точно таким же названием есть в базовом R в пакете stats, в котором та выполняет совершенно другую задачу. Если у вас уже загружен dplyr, то использование :: укажет на то, что вы хотите воспользоваться именно функцией filter() из пакета stats: stats::filter(1:20, rep(1,3)) ## Time Series: ## Start = 1 ## End = 20 ## Frequency = 1 ## [1] NA 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 NA Подобные путаницы могут возникнуть, если у вас загружено много пакетов, поэтому старайтесь не загружать слишком много пакетов, а если есть функции с одинаковым названием, то обязательно используйте оператор ::. Иначе слишком велик риск загрузить пакеты не в том порядке и получить из-за этого ошибку или некорректный результат. Выгрузить ненужный пакет можно с помощью функции detach(). detach(package:remotes) 5.6 Установка пакетов c Bioconductor У биологов есть свой большой репозиторий, который является альтернативой CRAN, — Bioconductor. С него можно скачать множество специализированных пакетов для работы с биологическими данными. Для установки пакетов с Bioconductor сначала нужно скачать пакет BiocManager с CRAN. install.packages(&quot;BiocManager&quot;) Теперь можно воспользоваться функцией install() из пакета BiocManager для установки пакета flowCore — пакета для анализа данных проточной цитометрии. BiocManager::install(&quot;flowCore&quot;) 5.7 Установка пакетов с Github Некоторых пакетов нет ни на CRAN, ни на Bioconductor. Обычно это касается пакетов, разработчики которых по каким-либо причинам решили не проходить проверки или не прошли проверки на строгие требования CRAN. Иногда бывает, что пакет был удален с CRAN (например, автор давно не занимается им) или же версия пакета на CRAN отстает от последней, а именно в ней реализованы так нужные вам функции. В некоторых случаях пакета может не быть на CRAN, потому что его разработчики активно занимаются его развитием и постоянно переделывают уже имеющийся функционал, добавляя новые возможности и удаляя старые. Это нужно делать с осторожностью, когда пакет уже выложен на CRAN, потому что если функции новой версии пакета будут работать по-другому, то это может вызвать массу проблем. Во всех этих случаях пакет обычно можно скачать с репозитория Github. Для этого нам понадобится уже установленный (с CRAN, разумеется) пакет remotes10. remotes::install_github(&quot;dracor-org/rdracor&quot;) Теперь установленный пакет осталось загрузить, после чего им можно пользоваться. library(rdracor) godunov &lt;- play_igraph(corpus = &quot;rus&quot;, play = &quot;pushkin-boris-godunov&quot;) plot(godunov) Пакет remotes можно так же использовать для загрузки пакетов из Bioconductor: remotes::install_bioc(&quot;flowCore&quot;) 5.8 Где искать нужные пакеты Мы разобрались с тем, как устанавливать пакеты. А где же их находить? Это вопрос гораздо более сложный чем может показаться. Например, можно работать в R и не знать, что существует пакет, который решает нужную для вас задачу. Или же найти такой пакет и не знать, что есть более современный пакет, который делает это еще лучше! Здесь нет каких-то готовых решений. CRAN пытается создавать и поддерживать тематические списки (Task View) пакетов с описанием задач, которые они решают: https://cran.r-project.org/web/views/ Безусловно, если вы глубоко занимаетесь какой-либо темой из списка, то стоит изучить соотвестствующий Task View, но начинать знакомство с помощью Task View достаточно тяжело. Другой вариант — просто погуглить, найти релевантные статьи или книги. Внимательно смотрите на дату публикации: R — очень быстро развивающийся язык, поэтому с большой вероятностью то, что было написано пять лет назад уже потеряло актуальность. Нет, работать это будет, но, скорее всего, появился более удобный и продвинутый инструмент. пакет remotes “откололся” от более старого пакета devtools, а многие функции из remote просто скопированы из devtools. Разработчики devtools/remotes рекомендуют использовать для установки пакетов именно более легковесный remotes, но во многих случаях вы увидите код с devtools::install_github(). Оба варианта будут работать.↩︎ "],["real-data.html", "6 Импорт и экспорт данных 6.1 Рабочая папка и проекты RStudio 6.2 Организация проектов 6.3 Проверка импортированных данных 6.4 Экспорт данных 6.5 Импорт таблиц в бинарном формате: таблицы Excel, SPSS 6.6 Быстрый импорт данных", " 6 Импорт и экспорт данных Итак, пришло время перейти к реальным данным. Мы начнем с использования датасета (так мы будем называть любой набор данных) по супергероям. Этот датасет представляет собой табличку, каждая строка которой - отдельный супергерой, а столбик — какая-либо информация о нем. Например, цвет глаз, цвет волос, вселенная супергероя11, рост, вес, пол и так далее. Несложно заметить, что этот датасет идеально подходит под структуру датафрейма: прямоугольная табличка, внутри которой есть разные колонки, каждая из которой имеет свой тип (числовой или строковый). 6.1 Рабочая папка и проекты RStudio Для начала скачайте файл по ссылке Он, скорее всего, появился у Вас в папке “Загрузки.” Если мы будем просто пытаться прочитать этот файл (например, с помощью read.csv() — мы к этой функцией очень скоро перейдем), указав его имя и разрешение, то наткнемся на такую ошибку: read.csv(&quot;heroes_information.csv&quot;) ## Warning in file(file, &quot;rt&quot;): не могу открыть файл &#39;heroes_information.csv&#39;: No ## such file or directory ## Error in file(file, &quot;rt&quot;): не могу открыть соединение Это означает, что R не может найти нужный файл. Вообще-то мы даже не сказали, где искать. Нам нужно как-то совместить место, где R ищет загружаемые файлы и сами файлы. Для этого есть несколько способов. Магомет идет к горе: перемещение файлов в рабочую папку. Для этого нужно узнать, какая папка является рабочей с помощью функции getwd() (без аргументов), найти эту папку в проводнике и переместить туда файл. После этого можно использовать просто название файла с разрешением: heroes &lt;- read.csv(&quot;heroes_information.csv&quot;) Кроме того, путь к рабочей папке можно увидеть в RStudio во вкладке с консолью, в самой верхней части (прямо под надписью “Console”): Гора идет к Магомету: изменение рабочей папки. Можно просто сменить рабочую папку с помощью setwd() на ту, где сейчас лежит файл, прописав путь до этой папки. Теперь файл находится в рабочей папке: heroes &lt;- read.csv(&quot;heroes_information.csv&quot;) Этот вариант использовать не рекомендуется! Как минимум, это сразу делает невозможным запустить скрипт на другом компьютере. Ну а если все-таки вдруг повезет и получится, то ваш коллега будет очень недоволен, что ваш скрипт изменяет рабочую директорию. Гора находит Магомета по месту прописки: указание полного пути файла. heroes &lt;- read.csv(&quot;/Users/Username/Some_Folder/heroes_information.csv&quot;) Этот вариант страдает теми же проблемами, что и предыдущий, поэтому тоже не рекомендуется! Для пользователей Windows есть дополнительная сложность: знак / является особым знаком для R, поэтому вместо него нужно использовать двойной //. Магомет использует кнопочный интерфейс: Import Dataset. Во вкладке Environment справа в окне RStudio есть кнопка “Import Dataset.” Возможно, у Вас возникло непреодолимое желание отдохнуть от написания кода и понажимать кнопочки — сопротивляйтесь этому всеми силами, но не вините себя, если не сдержитесь. Гора находит Магомета в интернете. Многие функции в R, предназначенные для чтения файлов, могут прочитать файл не только на Вашем компьютере, но и сразу из интернета. Для этого просто используйте ссылку вместо пути: heroes &lt;- read.csv(&quot;https://raw.githubusercontent.com/agricolamz/2020-2021-ds4dh/master/data/heroes_information.csv&quot;) Каждый Магомет получает по своей горе: использование проектов в RStudio. На первый взгляд это кажется чем-то очень сложным, но это не так. Это очень просто и ОЧЕНЬ удобно. При создании проекта создается отдельная папка, где у вас лежат данные, хранятся скрипты, вспомогательные файлы и отчеты. Кроме папки создается файл формата .Rproj, в котором хранятся настройки проекта. Если нужно вернуться к другому проекту — просто открываете другой проект, с другими файлами и скриптами. Можно даже иметь открытыми несколько окон RStudio таким образом. Это еще помогает не пересекаться переменным из разных проектов — а то, знаете, использование двух переменных data в разных скриптах чревато ошибками. Поэтому очень удобным решением будет выделение отдельного проекта под этот курс. При закрытии проекта все переменные по умолчанию тоже будут сохраняться, а при открытии — восстанавливаться (а вот пакеты все равно придется подгружать заново). Это очень удобно, хотя некоторые рекомендуют от этого отказаться. Это можно сделать во вкладке Tool - Global Options... 6.2 Организация проектов Даже если не пользоваться проектами RStudio (но я настоятельно рекомендую, это очень удобно), то все равно имеет смысл разделять различные свои проекты по отдельным папкам. Для небольших проектов этого уже может быть достаточно, но я рекомендую делать немного более сложную структуру папок внутри проекта. Например, такую: . └── my_project ├── R ├── data │ ├── raw │ ├── temp │ └── processed ├── figures ├── main_script.R ├── my_project.Rproj ├── output └── README.txt В основной папке содержится автоматически созданный RStudio файл .Rproj, основной скрипт с формат .R (или же это может быть .Rmd файл — см. 12). Вспомогательные скрипты (например, с функциями) могут храниться в папке R. Если скриптов несколько, то их порядок стоит обозначить числами: . ├── 01_first_script_preposcessing.R ├── 02_second_script_statistics.R └── 03_third_script_figures.R Данные стоит держать в отдельной папке, причем в некоторых ситуациях вы захотите создать отдельные подпапки, например, отдельные подпапки для данных на входе, временных файлов и данных на выходе. Результаты работы, например, отчеты, сгенерированные с помощью R Markdown (см. 12). Туда же можно поместить папку с графиками или же можно поместить эту папку в корневую директорию. Это лишь пример структуры организации проектов, детали могут различаться, но такая структура позволит не заблудиться в собственных файлах, если тех накопилось достаточно много. Кроме того, другому человеку в такой структуре проекта будет разобраться значительно проще При создании папок внутри основного проекта важно помнить о том, что теперь ваши файлы больше нельзя найти в вашей корневой директории: нужно искать их в соответствующих папках. Это значит, что путь до файла теперь будет не \"heroes_information.csv\", а \"data/heroes_information.csv\" или даже \"data/raw/heroes_information.csv\". Пакет {here} позволяет удобно работать с путями на любых операционных системах, создавая путь в зависимости от вашей корневой директории проекта. here::here(&quot;data&quot;, &quot;heroes_information.csv&quot;) ## [1] &quot;/Users/ivan/R/tidy_stats/data/heroes_information.csv&quot; Созданный путь можно использовать для чтения файлов: heroes &lt;- read.csv(here::here(&quot;data&quot;, &quot;heroes_information.csv&quot;)) Сами скрипты тоже лучше разделять на смысловые части. Для этого есть горячие клавиши Cmd + Shift + R. Это сочетание клавиш выведет окно, в котором вам нужно вписать название, после чего появится вот такой аккуратный комментарий: # Meaningful part of the script ------------------------------------------- Разделенный на такие части скрипт (да еще и с подробными комментариями) гораздо удобнее читать! 6.2.1 Табличные данные: текстовые и бинарные данные Как Вы уже поняли, импортирование данных - одна из самых муторных и неприятных вещей в R. Если у Вас получится с этим справится, то все остальное - ерунда. Мы уже разобрались с первой частью этого процесса - нахождением файла с данными, осталось научиться их читать. Здесь стоит сделать небольшую ремарку. Довольно часто данные представляют собой табличку. Или же их можно свести к табличке. Такая табличка, как мы уже выяснили, удобно репрезентируется в виде датафрейма. Но как эти данные хранятся на компьютере? Есть два варианта: в бинарном и в текстовом файле. Текстовый файл означает, что такой файл можно открыть в программе “Блокнот” или аналоге (например, TextEdit на macOS) и увидеть напечатанный текст: скрипт, роман или упорядоченный набор цифр и букв. Нас сейчас интересует именно последний случай. Таблица может быть представлена как текст: отдельные строчки в файле будут разделять разные строчки таблицы, а какой-нибудь знак-разделитель отделять колонки друг от друга. Для чтения данных из текстового файла есть довольно удобная функция read.table(). Почитайте хэлп по ней и ужаснитесь: столько разных параметров на входе! Но там же вы увидете функции read.csv(), read.csv2() и некоторые другие — по сути, это тот же read.table(), но с другими параметрами по умолчанию, соответствующие формату файла, который мы загружаем. В данном случае используется формат .csv, что означает “Comma Separated Values” (Значения, Разделенные Запятыми). Формат .csv — это самый известный способ хранения табличных данных в файде на сегодняшний день. Файлы с расширением .csv можно легко открыть в любой программе, работающей с таблицами, в том числе Microsoft Excel и его аналогах. Файл с расширением .csv — это просто текстовый файл, в котором “закодирована” таблица: разные строчки разделяют разные строчки таблицы, а столбцы отделяются запятыми (отсюда и название). Вы можете вручную создать такие файлы в Блокноте и сохранять их с форматом .csv - и такая табличка будет нормально открываться в Microsoft Excel и других программах для работы с таблицами. Можете попробовать это сделать самостоятельно! Как говорилось ранее, в качестве разделителя ячеек по горизонтали — то есть разделителя между столбцами — используется запятая. С этим связана одна проблема: в некоторых странах (в т.ч. и России) принято использовать запятую для разделения дробной части числа, а не точку, как это делается в большинстве стран мира. Поэтому есть альтернативный вариант формата .csv, где значения разделены точкой с запятой (;), а дробные значения - запятой (,). В этом и различие функций read.csv() и read.csv2() — первая функция предназначена для “международного” формата, вторая - для (условно) “Российского.” Оба варианта формата имеют расширение .csv, поэтому заранее понять какой именно будет вариант довольно сложно, приходится либо пробовать оба, либо заранее открывать файл в текстовом редакторе. В первой строчке обычно содержатся названия столбцов - и это чертовски удобно, функции read.csv() и read.csv2() по умолчанию считают первую строчку именно как название для колонок. Кроме .csv формата есть и другие варианты хранения таблиц в виде текста. Например, .tsv — тоже самое, что и .csv, но разделитель - знак табуляции. Для чтения таких файлов есть функция read.delim() и read.delim2(). Впрочем, даже если бы ее и не было, можно было бы просто подобрать нужные параметры для функции read.table(). Есть даже функции, которые пытаются сами “угадать” нужные параметры для чтения — часто они справляются с этим довольно удачно. Но не всегда. Поэтому стоит научиться справляться с любого рода данными на входе. Итак, прочитаем наш файл. Для этого используем только параметр file =, который идет первым, и для параметра stringsAsFactors = поставим значение FALSE: heroes &lt;- read.csv(&quot;data/heroes_information.csv&quot;, stringsAsFactors = FALSE) Параметр stringsAsFactors = задает то, как будут прочитаны строковые значения - как уже знакомые нам строки или как факторы. По сути, факторы - это примерно то же самое, что и character, но закодированные числами. Когда-то это было придумано для экономии используемых времени и памяти, сейчас же обычно становится просто лишней морокой. Но некоторые функции требуют именно character, некоторые factor, в большинстве случаев это без разницы. Но иногда непонимание может привести к дурацким ошибкам. В данном случае мы просто пока обойдемся без факторов. Если у вас версия R выше 4.0.0, то stringsAsFactors = будет FALSE по умолчанию. Можете проверить с помощью View(heroes): все работает! Если же вылезает какая-то странная ерунда или же просто ошибка - попробуйте другие функции (read.table(), read.delim()) и покопаться с параметрами. Для этого читайте Help. 6.3 Проверка импортированных данных При импорте данных обратите внимания на предупреждения (если таковые появляются), в большинстве случаев они указывают на то, что данные импортированы некорректно. Проверим, что все прочиталось нормально с помощью уже известной нам функции str(): str(heroes) ## &#39;data.frame&#39;: 734 obs. of 11 variables: ## $ X : int 0 1 2 3 4 5 6 7 8 9 ... ## $ name : chr &quot;A-Bomb&quot; &quot;Abe Sapien&quot; &quot;Abin Sur&quot; &quot;Abomination&quot; ... ## $ Gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Eye.color : chr &quot;yellow&quot; &quot;blue&quot; &quot;blue&quot; &quot;green&quot; ... ## $ Race : chr &quot;Human&quot; &quot;Icthyo Sapien&quot; &quot;Ungaran&quot; &quot;Human / Radiation&quot; ... ## $ Hair.color: chr &quot;No Hair&quot; &quot;No Hair&quot; &quot;No Hair&quot; &quot;No Hair&quot; ... ## $ Height : num 203 191 185 203 -99 193 -99 185 173 178 ... ## $ Publisher : chr &quot;Marvel Comics&quot; &quot;Dark Horse Comics&quot; &quot;DC Comics&quot; &quot;Marvel Comics&quot; ... ## $ Skin.color: chr &quot;-&quot; &quot;blue&quot; &quot;red&quot; &quot;-&quot; ... ## $ Alignment : chr &quot;good&quot; &quot;good&quot; &quot;good&quot; &quot;bad&quot; ... ## $ Weight : int 441 65 90 441 -99 122 -99 88 61 81 ... Всегда проверяйте данные на входе и никогда не верьте на слово, если вам говорят, что данные вычищенные и не содержат никаких ошибок. На что нужно обращать внимание? Прочитаны ли пропущенные значения как NA. По умолчанию пропущенные значения обозначаются пропущенной строчкой или “NA,” но встречаются самые разнообразные варианты. Возможные варианты кодирования пропущенных значений можно задать в параметре na.strings = функции read.table() и ее вариантов. В нашем датасете как раз такая ситуация, где нужно самостоятельно задавать, какие значения будут прочитаны как NA. Попытайтесь самостоятельно догадаться, как именно. Прочитаны ли те столбики, которые должны быть числовыми, как int или num. Если в колонке содержатся числа, а написано chr (= \"character\") или Factor (в случае если stringsAsFactors = TRUE), то, скорее всего, одна из строчек содержит в себе нечисловые знаки, которые не были прочитаны как NA. Странные названия колонок. Это может случиться по самым разным причинам, но в таких случаях стоит открывать файл в другой программе и смотреть первые строчки. Например, может оказаться, что первые несколько строчек — пустые или что первая строчка не содержит название столбцов (тогда для параметра header = нужно поставить FALSE) Вместо строковых данных у вас кракозябры. Это означает проблемы с кодировкой. В первую очередь попробуйте выставить значение \"UTF-8\" для параметра encoding = в функции для чтения файла: heroes &lt;- read.csv(&quot;data/heroes_information.csv&quot;, stringsAsFactors = FALSE, encoding = &quot;UTF-8&quot;) В случае если это не помогает, попробуйте разобрать, что это за кодировка. Все прочиталось как одна колонка. В этом случае, скорее всего, неправильно подобран разделить колонок — параметр sep =. Откройте файл в текстовом редакторе, чтобы понять какой нужно использовать. В отдельных строчках все прочиталось как одна колонка, а в остальных нормально. Скорее всего, в файле есть значения типа \\ или \", которые в функциях read.csv(), read.delim(), read.csv2(), read.delim2() читаются как символы для закавычивания значений. Это может понадобиться, если у вас в таблице есть строковые значения со знаками , или ;, которые могут восприниматься как разделитель столбцов. Появились какие-то новые числовые колонки. Возможно неправильно поставлен разделитель дробной части. Обычно это либо . (read.table(), read.csv(), read.delim()), либо , (read.csv2(), read.delim2()). Конкретно в нашем случае все прочиталось хорошо с помощью функции read.csv(), но в строковых переменных есть много прочерков, которые обозначают отсутствие информации по данному параметру супергероя, т.е. пропущенное значение. А вот с числовыми значениями все не так просто: для всех супергероев прописано какое-то число, но во многих случаях это -99. Очевидно, отрицательного роста и массы не бывает, это просто обозначение пропущенных значений (такое часто используется). Таким образом, чтобы адекватно прочитать файл, нам нужно поменять параметр na.strings = функции read.csv(): heroes &lt;- read.csv(&quot;data/heroes_information.csv&quot;, stringsAsFactors = FALSE, na.strings = c(&quot;-&quot;, &quot;-99&quot;)) 6.4 Экспорт данных Представим, что вы хотите сохранить табличку с данными про супергероев из вселенной DC в виде отдельного файла .csv. dc &lt;- heroes[heroes$Publisher == &quot;DC Comics&quot;,] Функция write.csv() позволит записать датафрейм в файл формата .csv: write.csv(dc, &quot;data/dc_heroes_information.csv&quot;) Обычно названия строк не используются, и их лучше не записывать, поставив для row.names = значение FALSE: write.csv(dc, &quot;data/dc_heroes_information.csv&quot;, row.names = FALSE) По аналогии с read.csv2(), write.csv2() позволит записать файлы формата .csv с разделителем ;. write.csv2(dc, &quot;data/dc_heroes_information.csv&quot;, row.names = FALSE) 6.5 Импорт таблиц в бинарном формате: таблицы Excel, SPSS Тем не менее, далеко не всегда таблицы представлены в виде текстового файла. Самый распространенный пример таблицы в бинарном виде — родные форматы Microsoft Excel. Если Вы попробуете открыть .xlsx файл в Блокноте, то увидите кракозябры. Это делает работу с этим файлами гораздо менее удобной, поэтому стоит избегать экселевских форматов и стараться все сохранять в .csv. Такие файлы не получится прочитать при помощи базового инструментария R. Тем не менее, для чтения таких файлов есть много дополнительных пакетов: файлы Microsoft Excel: лучше всего справляется пакет readxl (является частью расширенного tidyverse), у него есть много альтернатив (xlsx, openxlsx). файлы SPSS, SAS, Stata: существуют два основных пакета — haven (часть расширенного tidyverse) и foreign. Что такое пакеты и как их устанавливать мы изучим очень скоро. 6.6 Быстрый импорт данных Чтение табличных данных обычно происходит очень быстро. По крайней мере, до тех пор пока ваши данные не содержат очень много значений. Если вы попробуете прочитать с помощью read.csv() таблицу с миллионами строчками, то заметите, что это происходит довольно медленно. Впрочем, эта проблема эффективно решается дополнительными пакетами. Пакет readr (часть базового tidyverse) предлагает функции, очень похожие на стандартные read.csv(), read.csv2() и тому подобные, только в названиях используется нижнее подчеркивание: read_csv() и read_csv2(). Они быстрее и немного удобнее, особенно если вы работаете в tidyverse. readr::read_csv(&quot;data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## name = col_character(), ## Gender = col_character(), ## `Eye color` = col_character(), ## Race = col_character(), ## `Hair color` = col_character(), ## Height = col_double(), ## Publisher = col_character(), ## `Skin color` = col_character(), ## Alignment = col_character(), ## Weight = col_double() ## ) Пакет vroom - это часть расширенного tidyverse. Это такая альтернатива readr из того же tidyverse, но еще быстрее (отсюда и название). vroom::vroom(&quot;data/heroes_information.csv&quot;) ## New names: ## * `` -&gt; ...1 ## Rows: 734 ## Columns: 11 ## Delimiter: &quot;,&quot; ## chr [8]: name, Gender, Eye color, Race, Hair color, Publisher, Skin color, Alignment ## dbl [3]: ...1, Height, Weight ## ## Use `spec()` to retrieve the guessed column specification ## Pass a specification to the `col_types` argument to quiet this message Пакет data.table - это не просто пакет, а целый фреймворк для работы с R, основной конкурент tidyverse. Одна из основных фишек data.table - быстрота работы. Это касается не только процессинга данных, но и их загрузки и записи. Поэтому некоторые используют функции data.table для чтения и записи данных в отдельности от всего остального пакета - они даже и называются соответствующе: fread() и fwrite(), где f означет fast12. data.table::fread(&quot;data/heroes_information.csv&quot;) Чем же пользоваться среди всего этого многообразия? Бенчмарки13 показывают, что быстрее всех vroom и data.table. Если же у вас нет задачи ускорить работу кода на несколько миллисекунд или прочитать датасет на много миллионов строк, то стандартного read.csv() (если вы работаете в базовом R) и readr::read_csv() (если вы работаете в tidyverse) должно быть достаточно. Все перечисленные пакеты повзоляют не только быстро импортировать данные, но и быстро (и удобно!) экспортировать их: readr::write_csv(dc, &quot;data/dc_heroes_information.csv&quot;) readr::write_excel_csv(dc, &quot;data/dc_heroes_information.csv&quot;) #Если в Excel возникают проблемы с кодировками при открытии созданного .csv файла, то эта функция решает эти проблемы vroom::vroom_write(dc, &quot;data/dc_heroes_information.csv&quot;, delim = &quot;,&quot;) data.table::fwrite(dc, &quot;data/dc_heroes_information.csv&quot;) В плане скорости записи файлов соотношение сил примерно такое же, как и для чтения: vroom и data.table обгоняют всех, затем идет readr, и только после него - базовые функции R. супергерои в комиксах, фильмах и телесериалах часто взаимодействуют друг с другом, однако обычно это взаимодействие происходит между супергероями одного издателя. Два крупнейших издателя комиксов — DC и Marvel, поэтому принято говорить о вселенной DC и Marvel.↩︎ А еще friendly: fread() обычно самостоятельно хорошо угадывает формат таблицы на входе. vroom тоже так умеет.↩︎ бенчмаркинг — это тест производительности, в данном случае — сравнение скорости работы конкурирующих пакетов.↩︎ "],["loops-conditions.html", "7 Условные конструкции и циклы 7.1 Выражения if, else, else if 7.2 Циклы for 7.3 Векторизованные условные конструкции: функции ifelse() и dplyr::case_when()", " 7 Условные конструкции и циклы 7.1 Выражения if, else, else if Стандратная часть практически любого языка программирования — условные конструкции. R не исключение. Однако и здесь есть свои особенности. Начнем с самого простого варианта с одним условием. Выглядеть условная конcтрукция будет вот так: if (условие) выражение Вот так это будет работать на практике: number &lt;- 1 if (number &gt; 0) &quot;Положительное число&quot; ## [1] &quot;Положительное число&quot; Если выражение (expression) содержит больше одной строчки, то они объединяются фигурными скобками. Впрочем, использовать их можно, даже если строчка всего в выражении всего одна. number &lt;- 1 if (number &gt; 0) { &quot;Положительное число&quot; } ## [1] &quot;Положительное число&quot; В рассмотренной нами конструкции происходит проверка на условие. Если условие верно14, то происходит то, что записано в последующем выражении. Если же условие неверно15, то ничего не происходит. Оператор else позволяет задавать действие на все остальные случаи: if (условие) выражение else выражение Работает это так: number &lt;- -3 if (number &gt; 0) { &quot;Положительное число&quot; } else { &quot;Отрицательное число или ноль&quot; } ## [1] &quot;Отрицательное число или ноль&quot; Иногда нам нужна последовательная проверка на несколько условий. Для этого есть оператор else if. Вот как выглядит ее применение: number &lt;- 0 if (number &gt; 0) { &quot;Положительное число&quot; } else if (number &lt; 0){ &quot;Отрицательное число&quot; } else { &quot;Ноль&quot; } ## [1] &quot;Ноль&quot; Как мы помним, R — язык, в котором векторизация играет большое значение. Но вот незадача — условные конструкции не векторизованы в R! Давайте попробуем применить эти конструкции для вектора значений и посмотрим, что получится. number &lt;- -2:2 if (number &gt; 0) { &quot;Положительное число&quot; } else if (number &lt; 0){ &quot;Отрицательное число&quot; } else { &quot;Ноль&quot; } ## Warning in if (number &gt; 0) {: длина условия &gt; 1, будет использован только первый ## элемент ## Warning in if (number &lt; 0) {: длина условия &gt; 1, будет использован только первый ## элемент ## [1] &quot;Отрицательное число&quot; R выдает сообщение, что используется только первое значение логического вектора внутри условия. Остальные просто игнорируются. Как же посчитать для всего вектора сразу? 7.2 Циклы for Во-первых, можно использовать for. Синтаксис у for похож на синтаксис условных конструкций. for(переменная in последовательность) выражение Теперь мы можем объединить условные конструкции и for. Немножко монструозно, но это работает: for (i in number) { if (i &gt; 0) { print(&quot;Положительное число&quot;) } else if (i &lt; 0) { print(&quot;Отрицательное число&quot;) } else { print(&quot;Ноль&quot;) } } ## [1] &quot;Отрицательное число&quot; ## [1] &quot;Отрицательное число&quot; ## [1] &quot;Ноль&quot; ## [1] &quot;Положительное число&quot; ## [1] &quot;Положительное число&quot; Чтобы выводить в консоль результат вычислений внутри for, нужно использовать print(). Здесь стоит отметить, что for используется в R относительно редко. В подавляющем числе ситуаций использование for можно избежать. Обычно мы работаем в R с векторами или датафреймами, которые представляют собой множество относительно независимых наблюдений. Если мы хотим провести какие-нибудь операции с этими наблюдениями, то они обычно могут быть выполнены параллельно. Скажем, вы хотите для каждого испытуемого пересчитать его массу из фунтов в килограммы. Этот пересчет осуществляется по одинаковой формуле для каждого испытуемого. Эта формула не изменится из-за того, что какой-то испытуемый слишком большой или слишком маленький - для следующего испытуемого формула будет прежняя. Если Вы встречаете подобную задачу (где функцию можно применить независимо для всех значений), то без цикла for вполне можно обойтись. Даже во многих случаях, где расчеты для одной строчки зависят от расчетов предыдущих строчек, можно обойтись без for векторизованными функциями, например, cumsum() для подсчета кумулятивной суммы. cumsum(1:10) ## [1] 1 3 6 10 15 21 28 36 45 55 Если же нет подходящей векторизованной функции, то можно воспользоваться семейством функций apply() (см. @ref(apply_f) ). После этих объяснений кому-то может показаться странным, что я вообще упоминаю про эти циклы. Но для кого-то циклы for настолько привычны, что их полное отсутствие в курсе может показаться еще более странным. Поэтому лучше от меня, чем на улице. Зачем вообще избегать конструкций for? Некоторые говорят, что они слишком медленные, и частично это верно, если мы сравниваем с векторизованными функциями, которые написаны на более низкоуровневых языках. Но в большинстве случаев низкая скорость for связана с неправильным использованием этой конструкции. Например, стоит избегать ситуации, когда на каждой итерации for какой-то объект (вектор, список, что угодно) изменяется в размере. Лучше будет создать заранее объект нужного размера, который затем будет наполняться значениями: number_descriptions &lt;- character(length(number)) #создаем строковый вектор с такой же длиной, как и исходный вектор for (i in 1:length(number)) { if (number[i] &gt; 0) { number_descriptions[i] &lt;- &quot;Положительное число&quot; } else if (number[i] &lt; 0) { number_descriptions[i] &lt;- &quot;Отрицательное число&quot; } else { number_descriptions[i] &lt;- &quot;Ноль&quot; } } number_descriptions ## [1] &quot;Отрицательное число&quot; &quot;Отрицательное число&quot; &quot;Ноль&quot; ## [4] &quot;Положительное число&quot; &quot;Положительное число&quot; В общем, при правильном обращении с for особых проблем со скоростью не будет. Но все равно это будет громоздкая конструкция, в которой легко ошибиться, и которую, скорее всего, можно заменить одной короткой строчкой. Кроме того, без конструкции for код обычно легко превратить в набор функций, последовательно применяющихся к данным, что мы будем по максимуму использовать, работая в tidyverse и применяя пайпы (см. [pipe]). 7.3 Векторизованные условные конструкции: функции ifelse() и dplyr::case_when() Альтернатива сочетанию условных конструкций и циклов for является использование встроенной функции ifelse(). Функция ifelse() принимает три аргумента - 1) условие (т.е. просто логический вектор, состоящий из TRUE и FALSE), 2) что выдавать в случае TRUE, 3) что выдавать в случае FALSE. На выходе получается вектор такой же длины, как и изначальный логический вектор (условие). ifelse(number &gt; 0, &quot;Положительное число&quot;, &quot;Отрицательное число или ноль&quot;) ## [1] &quot;Отрицательное число или ноль&quot; &quot;Отрицательное число или ноль&quot; ## [3] &quot;Отрицательное число или ноль&quot; &quot;Положительное число&quot; ## [5] &quot;Положительное число&quot; Периодически я встречаю у студентов строчку вроде такой: ifelse(условие, TRUE, FALSE). Эта конструкция избыточна, т.к. получается, что логический вектор из TRUE и FALSE превращается в абсолютно такой же вектор из TRUE и FALSE на тех же самых местах. Выходит, что ничего не меняется! Пакеты {dplyr} и {data.table} предоставляют более быстрые и более строгие альтернативы для базовой функции ifelse() с аналогичным синтаксисом: dplyr::if_else(number &gt; 0, &quot;Положительное число&quot;, &quot;Отрицательное число или ноль&quot;) ## [1] &quot;Отрицательное число или ноль&quot; &quot;Отрицательное число или ноль&quot; ## [3] &quot;Отрицательное число или ноль&quot; &quot;Положительное число&quot; ## [5] &quot;Положительное число&quot; data.table::fifelse(number &gt; 0, &quot;Положительное число&quot;, &quot;Отрицательное число или ноль&quot;) ## [1] &quot;Отрицательное число или ноль&quot; &quot;Отрицательное число или ноль&quot; ## [3] &quot;Отрицательное число или ноль&quot; &quot;Положительное число&quot; ## [5] &quot;Положительное число&quot; Если вы пользуетесь одним из этих пакетов (о них пойдет речь далее — см. @ref(tidy_intro)), то я советую пользоваться соотвествующей функцией вместо базового ifelse(). Обе функции будут избегать скрытого приведения типов (см. 3.2) и намеренно выдавать ошибку при использовании разных типов данных в параметрах yes = и no =. Помните, что NA по умолчанию — это логический тип данных, поэтому в этих функциях нужно использовать NA соответствующего типа NA_character_, NA_integer_, NA_real_, NA_complex_ (см. 3.7). У ifelse() тоже есть недостаток: он не может включать в себя дополнительных условий по типу else if. В простых ситуациях можно вставлять ifelse() внутри ifelse(): ifelse(number &gt; 0, &quot;Положительное число&quot;, ifelse(number &lt; 0, &quot;Отрицательное число&quot;, &quot;Ноль&quot;)) ## [1] &quot;Отрицательное число&quot; &quot;Отрицательное число&quot; &quot;Ноль&quot; ## [4] &quot;Положительное число&quot; &quot;Положительное число&quot; Достаточно симпатичное решение есть в пакете dplyr — функция case_when(), которая работает с использованием формулы: dplyr::case_when( number &gt; 0 ~ &quot;Положительное число&quot;, number &lt; 0 ~ &quot;Отрицательное число&quot;, number == 0 ~ &quot;Ноль&quot;) ## [1] &quot;Отрицательное число&quot; &quot;Отрицательное число&quot; &quot;Ноль&quot; ## [4] &quot;Положительное число&quot; &quot;Положительное число&quot; В data.table тоже есть свой (более быстрый) аналог case_when() — функция fcase(). Синтаксис отличается только тем, что вместо формул используются простые запятые: data.table::fcase( number &gt; 0, &quot;Положительное число&quot;, number &lt; 0, &quot;Отрицательное число&quot;, number == 0, &quot;Ноль&quot;) ## [1] &quot;Отрицательное число&quot; &quot;Отрицательное число&quot; &quot;Ноль&quot; ## [4] &quot;Положительное число&quot; &quot;Положительное число&quot; Задача создания вектора или колонки по множественным условиям из другой колонки плавно перетекает в задачу объединения двух датафреймов по единому ключу, и такое решение может оказаться наиболее быстрым (см. @ref(tidy_join)). В принципе, необязательно внутри должна быть проверка условий, достаточно просто значения TRUE.↩︎ Аналогично, достаточно просто значения FALSE.↩︎ "],["functional.html", "8 Функциональное программирование в R 8.1 Создание функций 8.2 Проверка на адекватность 8.3 Когда и зачем создавать функции? 8.4 Функции как объекты первого порядка 8.5 Семейство функций apply()", " 8 Функциональное программирование в R 8.1 Создание функций Поздравляю, сейчас мы выйдем на качественно новый уровень владения R. Вместо того, чтобы пользоваться теми функциями, которые уже написали за нас, мы можем сами создавать свои функции! В этом нет ничего сложного. Синтаксис создания функции внешне похож на создание циклов или условных конструкций. Мы пишем ключевое слово function, в круглых скобках обозначаем переменные, с которыми собираемся что-то делать. Внутри фигурных скобок пишем выражения, которые будут выполняться при запуске функции. У функции есть свое собственное окружение — место, где хранятся переменные. Именно те объекты, которые мы передаем в скобочках, и будут в окружении, так же как и “обычные” переменные для нас в глобальном окружении. Это означает, что функция будет искать переменные в первую очередь среди объектов, которые переданы в круглых скобочках. С ними функция и будет работать. На выходе функция выдаст то, что вычисляется внутри функции return(). Если return() появляется в теле функции несколько раз, то до результат будет возвращаться из той функции return(), до которой выполнение дошло первым. pow &lt;- function(x, p) { power &lt;- x ^ p return(power) } pow(3, 2) ## [1] 9 Если функция проработала до конца, а функция return() так и не встретилась, то возвращается последнее посчитанное значение. pow &lt;- function(x, p) { x ^ p } pow(3, 2) ## [1] 9 Если в последней строчке будет присвоение, то функция ничего не вернет обратно. Это очень распространенная ошибка: функция вроде бы работает правильно, но ничего не возвращает. Нужно писать так, как будто бы в последней строчке результат выполнения выводится в консоль. pow &lt;- function(x, p) { power &lt;- x ^ p #Функция ничего не вернет, потому что в последней строчке присвоение! } pow(3, 2) #ничего не возвращается из функции Если функция небольшая, то ее можно записать в одну строчку без фигурных скобок. pow &lt;- function(x, p) x ^ p pow(3, 2) ## [1] 9 Вообще, фигурные скобки используются для того, чтобы выполнить серию выражений, но вернуть только результат выполнения последнего выражения. Это можно использовать, чтобы не создавать лишних временных переменных в глобальном окружении. Мы можем оставить в функции параметры по умолчанию. pow &lt;- function(x, p = 2) x ^ p pow(3) ## [1] 9 pow(3, 3) ## [1] 27 В R работают ленивые вычисления (lazy evaluations). Это означает, что параметры функций будут только когда они понадобятся, а не заранее. R будет как самый ленивый прокрастинатор откладывать чтение данных, пока они не понадобятся в вычислениях. Это приводит к тому, что если параметр никак не задан, то обнаружится это только при его непосредственном использовании. Например, эта функция не будет выдавать ошибку, если мы не зададим параметр we_will_not_use_this_parameter =, потому что он нигде не используется в расчетах. pow &lt;- function(x, p = 2, we_will_not_use_this_parameter) x ^ p pow(x = 3) ## [1] 9 8.2 Проверка на адекватность Лучший способ не бояться ошибок и предупреждений — научиться прописывать их самостоятельно в собственных функциях. Это позволит понять, что за текстом предупреждений и ошибок, которые у вас возникают, стоит забота разработчиков о пользователях, которые хотят максимально обезопасить нас от наших непродуманных действий. Хорошо написанные функции не только выдают правильный результат на все возможные адекватные данные на входе, но и не дают получить правдоподобные результаты при неадекватных входных данных. Как вы уже знаете, если на входе у вас имеются пропущенные значения, то многие функции будут в ответ тоже выдавать пропущенные значения. И это вполне осознанное решение, которое позволяет избегать ситуаций вроде той, когда около одной пятой научных статей по генетике содержало ошибки в приложенных данных и замечать пропущенные значения на ранней стадии. Кроме того, можно проводить проверки на адекватность входящих данных (sanity check). Разберем это на примере самодельной функции imt(), которая выдает индекс массы тела, если на входе задать вес (аргумент weight =) в килограммах и рост (аргумент height =) в метрах. imt &lt;- function(weight, height) weight / height ^ 2 Проверим, что функция работает верно: w &lt;- c(60, 80, 120) h &lt;- c(1.6, 1.7, 1.8) imt(weight = w, height = h) ## [1] 23.43750 27.68166 37.03704 Очень легко перепутать и написать рост в сантиметрах. Было бы здорово предупредить об этом пользователя, показав ему предупреждающее сообщение, если рост больше, чем, например, 3. Это можно сделать с помощью функции warning() imt &lt;- function(weight, height) { if (any(height &gt; 3)) warning(&quot;Рост в аргументе height больше 3: возможно, указан рост в сантиметрах, а не в метрах\\n&quot;) weight / height ^ 2 } imt(78, 167) ## Warning in imt(78, 167): Рост в аргументе height больше 3: возможно, указан рост в сантиметрах, а не в метрах ## [1] 0.002796802 В некоторых случаях ответ будет совершенно точно некорректным, хотя функция все посчитает и выдаст ответ, как будто так и надо. Например, если какой-то из аргументов функции imt() будет меньше или равен 0. В этом случае нужно прописать проверку на это условие, и если это действительно так, то выдать пользователю ошибку. imt &lt;- function(weight, height) { if (any(weight &lt;= 0 | height &lt;= 0)) stop(&quot;Индекс массы тела не может быть посчитан для отрицательных значений&quot;) if (any(height &gt; 3)) warning(&quot;Рост в аргументе height больше 3: возможно, указан рост в сантиметрах, а не в метрах\\n&quot;) weight / height ^ 2 } imt(-78, 167) ## Error in imt(-78, 167): Индекс массы тела не может быть посчитан для отрицательных значений Когда вы попробуете самостоятельно прописывать предупреждения и ошибки в функциях, то быстро поймете, что ошибки - это вовсе не обязательно результат того, что где-то что-то сломалось и нужно паниковать. Совсем даже наоборот, прописанная ошибка - чья-то забота о пользователях, которых пытаются максимально проинформировать о том, что и почему пошло не так. Это естественно в начале работы с R (и вообще с программированием) избегать ошибок, конечно, в самом начале обучения большая часть из них остается непонятной. Но постарайтесь понять текст ошибки, вспомнить в каких случаях у вас возникала похожая ошибка. Очень часто этого оказывается достаточно чтобы понять причину ошибки даже если вы только-только начали изучать R. Ну а в дальнейшем я советую ознакомиться со средствами отладки кода в R для того, чтобы научиться справляться с ошибками в своем коде на более продвинутом уровне. 8.3 Когда и зачем создавать функции? Когда стоит создавать функции? Существует “правило трех” — если у вас есть три куска очень похожего кода, то самое время превратить код в функцию. Это очень условное правило, но, действительно, стоит избегать копипастинга в коде. В этом случае очень легко ошибиться, а сам код становится нечитаемым. Есть и другой подход к созданию функций: их стоит создавать не столько для того, чтобы использовать тот же код снова, сколько для абстрагирования от того, что происходит в отдельных строчках кода. Если несколько строчек кода были написаны для того, чтобы решить одну задачу, которой можно дать понятное название (например, подсчет какой-то особенной метрики, для которой нет готовой функции в R), то этот код стоит обернуть в функцию. Если функция работает корректно, то теперь не нужно думать над тем, что происходит внутри нее. Вы ее можете мысленно представить как операцию, которая имеет определенный вход и выход — как и встроенные функции в R. Отсюда следует важный вывод, что хорошее название для функции — это очень важно. Очень, очень, очень важно. 8.4 Функции как объекты первого порядка Ранее мы убедились, что арифметические операторы — это тоже функции. На самом деле, практически все в R — это функции. Даже function — это функция function(). Даже скобочки (, { — это функции! А сами функции — это объекты первого порядка в R. Это означает, что с функциями вы можете делать практически все то же самое, что и с другими объектами в R (векторами, датафреймами и т.д.). Небольшой пример, который может взорвать ваш мозг: list(mean, min, `{`) ## [[1]] ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x7f86c3224120&gt; ## &lt;environment: namespace:base&gt; ## ## [[2]] ## function (..., na.rm = FALSE) .Primitive(&quot;min&quot;) ## ## [[3]] ## .Primitive(&quot;{&quot;) Мы можем создать список из функций! Зачем — это другой вопрос, но ведь можем же! Еще можно создавать функции внутри функций,16 использовать функции в качестве аргументов функций, сохранять функции как переменные. Пожалуй, самое важное из этого всего - это то, что функция может быть аргументом в функции. Не просто название функции как строковая переменная, не результат выполнения функции, а именно сама функция. Это лежит в основе использования семейства функций apply() (@ref(apply_f) и многих фишек tidyverse. В Python дело обстоит похожим образом: функции там тоже являются объектами первого порядка, поэтому все эти фишки функционального программирования (с поправкой на синтаксис, конечно) будут работать и там. 8.5 Семейство функций apply() 8.5.1 Применение apply() для матриц Семейство? Да, их целое множество: apply(), lapply(),sapply(), vapply(),tapply(),mapply(), rapply()… Ладно, не пугайтесь, всех их знать не придется. Обычно достаточно первых двух-трех. Проще всего пояснить как они работают на простой матрице с числами: A &lt;- matrix(1:12, 3, 4) A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Функция apply() предназначена для работы с матрицами (или многомерными массивами). Если вы скормите функции apply() датафрейм, то этот датафрейм будет сначала превращен в матрицу. Главное отличие матрицы от датафрейма в том, что в матрице все значения одного типа, поэтому будьте готовы, что сработает имплицитное приведение к общему типу данных. Например, если среди колонок датафрейма есть хотя бы одна строковая колонка, то все колонки станут строковыми. Теперь представим, что нам нужно посчитать что-нибудь (например, сумму) по каждой из строк. С помощью функции apply() вы можете в буквальном смысле “применить” функцию к матрице или датафрейму. Синтаксис такой: apply(X, MARGIN, FUN, ...), где X — данные, MARGIN это 1 (для строк), 2 (для колонок), c(1,2) для строк и колонок (т.е. для каждого элемента по отдельности), а FUN — это функция, которую вы хотите применить! apply() будет брать строки/колонки из X в качестве первого аргумента для функции. apply Заметьте, мы вставляем функцию без скобок и кавычек как аргумент в функцию. Это как раз тот случай, когда аргументом в функции выступает сама функция, а не ее название или результат ее выполнения. Давайте разберем на примере: apply(A, 1, sum) #сумма по каждой строчке ## [1] 22 26 30 apply(A, 2, sum) #сумма по каждой колонке ## [1] 6 15 24 33 apply(A, c(1,2), sum) #кхм... сумма каждого элемента ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Конкретно для подсчета сумм и средних по столбцам и строкам в R есть функции colSums(), rowSums(), colMeans() и rowMeans(), которые можно использовать как альтернативы apply() в данном случае. Если же мы хотим прописать дополнительные аргументы для функции, то их можно перечислить через запятую после функции: apply(A, 1, sum, na.rm = TRUE) ## [1] 22 26 30 apply(A, 1, weighted.mean, w = c(0.2, 0.4, 0.3, 0.1)) ## [1] 4.9 5.9 6.9 8.5.2 Анонимные функции Что делать, если мы хотим сделать что-то более сложное, чем просто применить одну функцию? А если функция принимает не первым, а вторым аргументом данные из матрицы? В этом случае нам помогут анонимные функции. Анонимные функции - это функции, которые будут использоваться один раз и без названия. Питонистам знакомо понятие лямбда-функций. Да, это то же самое. Например, мы можем посчитать сумму квадратичных отклонений от среднего без называния этой функции: apply(A, 1, function(x) sum((x - mean(x))^2)) #отклонения от среднего по строчке ## [1] 45 45 45 apply(A, 2, function(x) sum((x - mean(x))^2)) #отклонения от среднего по столбцу ## [1] 2 2 2 2 apply(A, c(1, 2), function(x) sum((x - mean(x))^2)) #отклонения от одного значения, т.е. ноль ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 Как и в случае с обычной функцией, в качестве x выступает объект, с которым мы хотим что-то сделать, а дальше следует функция, которую мы собираемся применить к х. Можно использовать не х, а что угодно, как и в обычных функциях: apply(A, 1, function(whatevername) sum((whatevername - mean(whatevername))^2)) ## [1] 45 45 45 8.5.3 Другие функции семейства apply() Ок, с apply() разобрались. А что с остальными? Некоторые из них еще проще и не требуют индексов, например, lapply (для применения к каждому элементу списка) и sapply() - упрощенная версия lapply(), которая пытается по возможности “упростить” результат до вектора или матрицы. some_list &lt;- list(some = 1:10, list = letters) lapply(some_list, length) ## $some ## [1] 10 ## ## $list ## [1] 26 sapply(some_list, length) ## some list ## 10 26 Достаточно сложно предсказать, в каких именно случаях будет произведено упрощение, а в каких нет. Поэтому sapply() удобен в исследовании данных, но использовать эту функцию в скриптах не очень рекомендуется. Один из вариантов решения этой проблемы — это функция vapply(), которая позволяет управлять результатом lapply(), но гораздо более красиво эта проблема решена в пакете {purrr} (см. 10.4). Использование sapply() на векторе приводит к тем же результатам, что и просто применить векторизованную функцию обычным способом. sapply(1:10, sqrt) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 sqrt(1:10) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Зачем вообще тогда нужен sapply(), если мы можем просто применить векторизованную функцию? Ключевое слово здесь векторизованная функция. Если функция не векторизована, то sapply() становится удобным вариантом для того, чтобы избежать итерирования с помощью циклов for. Еще одна альтернатива - это векторизация невекторизованной функции с помощью Vectorize(). Эта функция просто оборачивает функцию одним из вариантов apply(). Можно применять функции lapply() и sapply() на датафреймах. Поскольку фактически датафрейм - это список из векторов одинаковой длины (см. 4.4), то итерироваться эти функции будут по колонкам: heroes &lt;- read.csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/heroes_information.csv&quot;, na.strings = c(&quot;-&quot;, &quot;-99&quot;)) sapply(heroes, class) ## X name Gender Eye.color Race Hair.color ## &quot;integer&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; ## Height Publisher Skin.color Alignment Weight ## &quot;numeric&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;integer&quot; Еще одна функция из семейства apply() - функция replicate() - самый простой способ повторить одну и ту же операцию много раз. Обычно эта функция используется при симуляции данных и моделировании. Например, давайте сделаем выборку из логнормального распределения: samp &lt;- rlnorm(30) hist(samp) А теперь давайте сделаем 1000 таких выборок и из каждой возьмем среднее: sampdist &lt;- replicate(1000, mean(rlnorm(30))) hist(sampdist) Про функции для генерации случайных чисел и про визуализацию мы поговорим в следующие дни. Если хотите познакомиться с семейством apply() чуточку ближе, то рекомендую вот этот туториал. В заключение стоит сказать, что семейство функций apply() — это очень сильное колдунство, но в tidyverse оно практически полностью перекрывается функциями из пакета purrr. Впрочем, если вы поняли логику apply(), то при желании вы легко сможете переключиться на альтернативы из пакета purrr (см. 10.4) Функция, которая создает другие функции, называется фабрикой функций.↩︎ "],["tidy-intro.html", "9 Введение в tidyverse 9.1 Вселенная tidyverse 9.2 Загрузка данных с помощью readr 9.3 magrittr::%&gt;% 9.4 Главные пакеты tidyverse: dplyr и tidyr 9.5 Работа с колонками тиббла 9.6 Работа со строками тиббла 9.7 Создание колонок: dplyr::mutate() и dplyr::transmute() 9.8 Агрегация данных в тиббле", " 9 Введение в tidyverse 9.1 Вселенная tidyverse tidyverse - это не один, а целое множество пакетов. Есть ключевые пакеты (ядро тайдиверса), а есть побочные - в основном для работы со специфическими видами данных. tidyverse — это набор пакетов: ggplot2, для визуализации tibble, для работы с тибблами, продвинутый вариант датафрейма tidyr, для формата tidy data readr, для чтения файлов в R purrr, для функционального программирования (замена семейства функций *apply()) dplyr, для преобразованиия данных stringr, для работы со строковыми переменными forcats, для работы с переменными-факторами Полезно также знать о следующих пакетах, не включенных в ядро, но также считающихся частью тайдиверса: vroom, для быстрой загрузки табоичных данных readxl, для чтения .xls и .xlsx jsonlite, для работы с JSON xml, для работы с XML DBI, для работы с базами данных rvest, для веб-скреппинга lubridate, для работы с временем tidytext, для работы с текстами и корпусами glue, для продвинутого объединения строк magrtittr, с несколькими вариантами pipe оператора tidymodels, для моделирования и машинного обучения17 dtplyr, для ускорения dplyr за счет перевод синтаксиса на data.table И это еще не все пакеты tidyverse! Есть еще много других небольших пакетов, которые тоже считаются частью tidyverse. Кроме официальных пакетов tidyverse есть множество пакетов, которые пытаются соответствовать принципам tidyverse и дополняют его. Все пакеты tidyverse объединены tidy философией и взаимосовместимым синтаксисом. Это означает, что, во многих случаях даже не нужно думать о том, из какого именно пакета тайдиверса пришла функция. Можно просто установить и загрузить пакет tidyverse. install.packages(&quot;tidyverse&quot;) Пакет tidyverse — это такой пакет с пакетами. library(&quot;tidyverse&quot;) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.6 ✓ dplyr 1.0.4 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Подключение пакета tidyverse автоматически приводит к подключению ядра tidyverse, остальные же пакеты нужно подключать дополнительно при необходимости. 9.2 Загрузка данных с помощью readr Стандартной функцией для чтения .csv файлов в R является функция read.csv(), но мы будем использовать функцию read_csv() из пакета readr. Синтаксис функции read_csv() очень похож на read.csv(): первым аргументом является путь к файлу (в том числе можно использовать URL), некоторые остальные параметры тоже совпадают. heroes &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## name = col_character(), ## Gender = col_character(), ## `Eye color` = col_character(), ## Race = col_character(), ## `Hair color` = col_character(), ## Height = col_double(), ## Publisher = col_character(), ## `Skin color` = col_character(), ## Alignment = col_character(), ## Weight = col_double() ## ) Подробнее про импорт данных, в том числе в tidyverse, смотри в @ref(real_data). ##tibble Когда мы загрузили данные с помощью read_csv(), то мы получили tibble, а не data.frame: class(heroes) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Тиббл (tibble) - это такой “усовершенствованный” data.frame. Почти все, что работает с data.frame, работает и с тибблами. Однако у тибблов есть свои дополнительные фишки. Самая очевидная из них - более аккуратный вывод в консоль: heroes Выводятся только первые 10 строк, если какие-то колонки не влезают на экран, то они просто перечислены внизу. Ну а тип данных написан прямо под названием колонки. Функции различных пакетов tidyverse сами конвертируют в тиббл при необходимости. Если же нужно это сделать самостоятельно, то можно это сделать так: heroes_df &lt;- as.data.frame(heroes) #создаем простой датафрейм class(heroes_df) ## [1] &quot;data.frame&quot; as_tibble(heroes_df) #превращаем обратно в тиббл В дальнейшем мы будем работать только с tidyverse, а это значит, что только с тибблами, а не обычными датафреймами. Тем не менее, тибблы и датафреймы будут в дальнейшем использоваться как синонимы. Можно создавать тибблы вручную с помощью функции tibble(), которая работает аналогично функции data.frame(): tibble( a = 1:3, b = letters[1:3] ) 9.3 magrittr::%&gt;% Оператор %&gt;% называется “пайпом” (pipe), т.е. “трубой.” Он означает, что следующая функция (справа от пайпа) принимает на вход в качестве первого аргумента результат выполнения предыдущей функции (той, что слева). Фактически, это примерно то же самое, что и вставлять результат выполнения функции в качестве первого аргумента в другую функцию. Просто выглядит это красивее и читабельнее. Как будто данные пропускаются через трубы функций или конвеерную ленту на заводе, если хотите. А то, что первый параметр функции - это почти всегда данные, работает нам здесь на руку. Этот оператор взят из пакета magrittr18. Возможно, даже если вы не захотите пользоваться tidyverse, использование пайпов Вам понравится. Важно понимать, что пайп не дает какой-то дополнительной функциональности или дополнительной скорости работы19. Он создан исключительно для читабельности и комфорта. С помощью пайпов вот эту команду… sum(sqrt(abs(sin(1:22)))) ## [1] 16.72656 …можно переписать вот так: 1:22 %&gt;% sin() %&gt;% abs() %&gt;% sqrt() %&gt;% sum() ## [1] 16.72656 В очень редких случаях результат выполнения функции нужно вставить не на первую позицию (или же мы хотим использовать его несколько раз). В этих случаях можно использовать ., чтобы обозначить, куда мы хотим вставить результат выполнения выражения слева от %&gt;%. &quot;Всем привет!&quot; %&gt;% c(&quot;--&quot;, ., &quot;--&quot;) ## [1] &quot;--&quot; &quot;Всем привет!&quot; &quot;--&quot; Основные функции в tidyverse … 9.4 Главные пакеты tidyverse: dplyr и tidyr dplyr20 — это самая основа всего tidyverse. Этот пакет предоставляет основные функции для манипуляции с тибблами. Пакет dplyr является наследником и более усовершенствованной версией plyr, так что если увидите использование пакета plyr, то, скорее всего, скрипт был написан очень давно. Пакет tidyr дополняет dplyr, предоставляя полезные функции для тайдификации тибблов. Тайдификация (“аккуратизация”) данных означает приведение табличных данных к такому формату, в котором: Каждая переменная имеет собственный столбец Каждый наблюдение имеет собственную строку Каждое значение имеет свою собственную ячейку Впрочем, многие функции dplyr часто используются при тайдификации, так же как и многие функции tidyr имеет применение вне тайдификации. В общем, функционал этих двух пакетов несколько смешался, поэтому мы будем рассматривать их вместе. А чтобы представлять, какая функция относится к какому пакету (хотя запоминать это необязательно), я буду использовать запись с двумя двоеточиями ::, которая обычно используется для использования функции без подгрузки всего пакета, при первом упоминании функции. Пакет tidyr — это более усовершенствованная версия пакета reshape2, который в свою очередь является усовершенствованной версией reshape. По аналогии с plyr, если вы видите использование этих пакетов, то это указывает на то, что перед вами морально устаревший код. Код с использованием dplyr и tidyrсильно непохож на то, что мы видели раньше. Большинство функций dplyr и tidyr работают с целым тибблом сразу, принимая его в качестве первого аргумента и возвращая измененный тиббл. Это позволяет превратить весь код в последовательный набор применяемых функций, соединенный пайпами. На практике это выглядит очень элегантно, и вы в этом скоро убедитесь. 9.5 Работа с колонками тиббла 9.5.1 Выбор колонок: dplyr::select() Функция dplyr::select() позволяет выбирать колонки по номеру или имени (кавычки не нужны). heroes %&gt;% select(1,5) heroes %&gt;% select(name, Race, Publisher, `Hair color`) Обратите внимание, если в названии колонки присутствует пробел или, например, колонка начинается с цифры или точки и цифры, то это синтаксически невалидное имя (2.6). Это не значит, что такие названия колонок недопустимы. Но такие названия колонок нужно обособлять ` грависом (правый штрих, на клавиатуре находится там же где и буква ё и ~). Еще обратите внимание на то, что функции tidyverse не изменяют сами изначальные тибблы/датафреймы. Это означает, что если вы хотите полученный результат сохранить, то нужно добавить присвоение: heroes_some_cols &lt;- heroes %&gt;% select(name, Race, Publisher, `Hair color`) heroes_some_cols 9.5.2 Мини-язык tidyselect для выбора колонок Для выбора столбцов (не только в select(), но и для других функций tidyverse) используется специальный мини-язык tidyselect из одноименного пакета21. tidyselect дает очень широкие возможности для выбора колонок. Можно использовать оператор : для выбора нескольких соседних колонок (по аналогии с созданием числового вектора с шагом 1). heroes %&gt;% select(name:Publisher) heroes %&gt;% select(name:`Eye color`, Publisher:Weight) Используя ! можно вырезать ненужные колонки. heroes %&gt;% select(!X1) heroes %&gt;% select(!(Gender:Height)) Другие известные нам логические операторы (&amp; и |) тоже работают в tidyselect. В дополнение к логическим операторам и :, в tidyselect есть набор вспомогательных функций, работающих исключительно в контексте выбора колонок с помощью tidyselect. Вспомогательная функция last_col() позволит обратиться к последней колонке тиббла: heroes %&gt;% select(name:last_col()) А функция everything() позволяет выбрать все колонки. heroes %&gt;% select(everything()) При этом everything() не будет дублировать выбранные колонки, поэтому можно использовать everything() для перестановки колонок в тиббле: heroes %&gt;% select(name, Publisher, everything()) Впрочем, для перестановки колонок удобнее использовать специальную функцию relocate() (@ref(tidy_relocate)) Можно даже выбирать колонки по паттернам в названиях. Например, с помощью ends_with() можно выбрать все колонки, заканчивающиеся одинаковым суффиксом: heroes %&gt;% select(ends_with(&quot;color&quot;)) Аналогично, с помощью функции starts_with() можно найти колонки с одинаковым префиксом, с помощью contains() — все колонки с выбранным паттерном в любой части названия колонки22. heroes %&gt;% select(starts_with(&quot;Eye&quot;) &amp; ends_with(&quot;color&quot;)) heroes %&gt;% select(contains(&quot;eight&quot;)) Ну и наконец, можно выбирать по содержимому колонок с помощью where(). Это напоминает применение sapply()(@ref(apply_other)) на датафрейме для индексирования колонок: в качестве аргумента для where принимается функция, которая применяется для каждой из колонок, после чего выбираются только те колонки, для которых было получено TRUE. heroes %&gt;% select(where(is.numeric)) Функция where() дает невиданную мощь. Например, можно выбрать все колонки без NA: heroes %&gt;% select(where(function(x) !any(is.na(x)))) ###Переименование колонок: dplyr::rename() Внутри select() можно не только выбирать колонки, но и переименовывать их: heroes %&gt;% select(id = X1) Однако удобнее для этого использовать специальную функцию dplyr::rename(). Синтаксис у нее такой же, как и у select(), но rename() не выбрасывает колонки, которые не были упомянуты. heroes %&gt;% rename(id = X1) Для массового переименования колонок можно использовать функцию rename_with(). Эта функция так же использует tidyselect синтаксис для выбора колонок (по умолчанию выбираются все колонки) и применяет функцию в качестве аргумента, которая изменяет heroes %&gt;% rename_with(make.names) ###Перестановка колонок: dplyr::relocate() {#tidy_relocate} Для изменения порядка колонок можно использовать функцию relocate(). Она тоже работает похожим образом на select() и rename()23. Как и rename(), функция relocate() не выкидывает неиспользованные колонки: heroes %&gt;% relocate(Publisher) При этом relocate() имеет дополнительные параметры .after = и .before =, которые позволяют выбирать, куда поместить выбранные колонки. heroes %&gt;% relocate(Publisher, .after = name) relocate() очень хорошо работает в сочетании с выбором колонок с помощью tidyselect. Например, можно передвинуть в одно место все колонки с одним типом данных: heroes %&gt;% relocate(Publisher, where(is.numeric), .after = name) Последняя важная функция для выбора колонок — pull(). Эта функция делает то же самое, что и индексирование с помощью $, т.е. вытаскивает из тиббла вектор с выбранным названием. Это лучше вписывается в логику tidyverse, поскольку позволяет извлечь колонку из тиббла с использованием пайпа: heroes %&gt;% select(Height) %&gt;% pull() %&gt;% head() ## [1] 203 191 185 203 NA 193 heroes %&gt;% pull(Height) %&gt;% head() ## [1] 203 191 185 203 NA 193 У функции pull() есть аргумент name =, который позволяет создать проименованный вектор: heroes %&gt;% pull(Height, name) %&gt;% head() ## A-Bomb Abe Sapien Abin Sur Abomination Abraxas ## 203 191 185 203 NA ## Absorbing Man ## 193 В отличие от базового R, tidyverse нигде не сокращает имплицитно результат вычислений до вектора, поэтому функция pull() - это основной способ извлечения колонки из тиббла как вектора. 9.6 Работа со строками тиббла 9.6.1 Выбор строк по номеру: dplyr::slice() Начнем с выбора строк. Функция dplyr::slice() выбирает строчки по их числовому индексу. heroes %&gt;% slice(1:3) 9.6.2 Выбор строк по условию: dplyr::filter() Функция dplyr::filter() делает то же самое, что и slice(), но уже по условию. Причем для условий нужно использовать не векторы из тиббла, а название колонок (без кавычек) как будто бы они были переменными в окружении. heroes %&gt;% filter(Publisher == &quot;DC Comics&quot;) 9.6.3 Семейство функций slice() У функции slice() есть множество родственников, которые объединяют функционал обычного slice() и filter(). Например, с помощью функций dplyr::slice_max() и dplyr::slice_min() можно выбрать заданное количество строк, содержащих наибольшие или наименьшие значения по колонке соответственно: heroes %&gt;% slice_max(Weight, n = 3) heroes %&gt;% slice_min(Weight, n = 3) Функция slice_sample() позволяет выбирать заданное количество случайных строчек: heroes %&gt;% slice_sample(n = 3) Или же долю строчек: heroes %&gt;% slice_sample(prop = .01) Если поставить значение параметра prop = равным 1, то таким образом можно перемешать порядок строчек в тиббле: heroes %&gt;% slice_sample(prop = 1) 9.6.4 Удаление строчек с NA: tidyr::drop_na() Если нужно выбрать только строчки без пропущенных значений, то можно воспользоваться удобной функцией tidyr::drop_na(). heroes %&gt;% drop_na() Можно выбрать колонки, наличие NA в которых будет приводить к удалению соответствующих строчек (не затрагивая другие строчки, в которых есть NA в остальных столбцах). heroes %&gt;% drop_na(Weight) Для выбора колонок в drop_na() используется tidyselect, с которым мы недавно познакомились (9.5.2). 9.6.5 Сортировка строк: dplyr::arrange() Функция dplyr::arrange() сортирует строчки от меньшего к большему (или по алфавиту - для текстовых значений) по выбранной колонке. heroes %&gt;% arrange(Weight) Чтобы отсортировать в обратном порядке, воспользуйтесь функцией desc(). heroes %&gt;% arrange(desc(Weight)) Можно сортировать по нескольким колонкам сразу. В таких случаях удобно в качестве первой переменной выбирать переменную, обозначающую принадлежность к группе, а в качестве второй — континуальную числовую переменную: heroes %&gt;% arrange(Gender, desc(Weight)) 9.7 Создание колонок: dplyr::mutate() и dplyr::transmute() Функция dplyr::mutate() позволяет создавать новые колонки в тиббле. heroes %&gt;% mutate(imt = Weight/(Height/100)^2) %&gt;% select(name, imt) %&gt;% arrange(desc(imt)) dplyr::transmute() - это аналог mutate(), который не только создает новые колонки, но и сразу же выкидывает все старые: heroes %&gt;% transmute(imt = Weight/(Height/100)^2) Внутри mutate() и transmute() мы можем использовать либо векторизованные операции (длина новой колонки должна равняться длине датафрейма), либо операции, которые возвращают одно значение. В последнем случае значение будет одинаковым на всю колонку, т.е. будет работать правило ресайклинга (3.4): heroes %&gt;% transmute(name, weight_mean = mean(Weight, na.rm = TRUE)) Однако в функциях mutate() и transmute() правило ресайклинга не будет работать в остальных случаях: если полученный вектор будет не равен 1 или длине датафрейма, то мы получим ошибку. heroes %&gt;% mutate(one_and_two = 1:2) ## Error: Problem with `mutate()` input `one_and_two`. ## x Input `one_and_two` can&#39;t be recycled to size 734. ## ℹ Input `one_and_two` is `1:2`. ## ℹ Input `one_and_two` must be size 734 or 1, not 2. Это не баг, а фича: авторы пакета dplyr считают, что ресайклинг кратных друг другу векторов — это слишком удобное место для выстрелов себе в ногу. Поэтому в таких случаях разработчики dplyr рекомендуют использовать функцию rep(), знакомую нам уже очень давно (3.1). heroes %&gt;% mutate(one_and_two = rep(1:2, length.out = nrow(.))) 9.8 Агрегация данных в тиббле 9.8.1 Подытоживание: summarise() Аггрегация по группам - это очень часто возникающая задача, например, это может использоваться для усреднения данных по испытуемым или условиям. Сделать аггрегацию в датафрейме удобной Хэдли Уикхэм пытался еще в предшественнике dplyr, пакете plyr. dplyr позволяет делать аггрегацию очень симпатичным и понятным способым. Аггрегация в dplyr состоит из двух этапов: группировки (group_by()) и подытоживания (summarise()). Начнем с последнего. Функция dplyr::summarise()24 позволяет аггрегировать данные в тиббле. Работает она очень похоже на mutate(), но если внутри mutate() используются векторизованные функции, возвращающие вектор такой же длины, что и колонки, использовавшиеся для расчетов, то в summarise() используются функции, которые возвращают вектор длиной 1. Например, min(), mean(), max() и т.д. Можно создавать несколько колонок через запятую (это работает и для mutate()). heroes %&gt;% mutate(imt = Weight/(Height/100)^2) %&gt;% summarise(min(imt, na.rm = TRUE), max(imt, na.rm = TRUE)) В dplyr есть дополнительные суммирующие функции для более удобного индексирования в стиле tidyverse. Например, функции dplyr::nth(), dplyr::first() и dplyr::last(), которые позволяют вытаскивать значения из вектора по индексу (что-то вроде slice(), но для векторов) heroes %&gt;% mutate(imt = Weight/(Height/100)^2) %&gt;% arrange(imt) %&gt;% summarise(first = first(imt), tenth = nth(imt, 10), last = last(imt)) В отличие от mutate(), функции внутри summarise() вполне позволяют функциям внутри возвращать вектор из нескольких значений, создавая тиббл такой же длины, как и получившийся вектор. heroes %&gt;% mutate(imt = Weight/(Height/100)^2) %&gt;% summarise(imt_range = range(imt, na.rm = TRUE)) #функция range() возвращает вектор из двух значений: минимальное и максимальное 9.8.2 Группировка: group_by() dplyr::group_by() - это функция для группировки данных в тиббле по дискретной переменной для дальнейшей аггрегации с помощью summarise(). После применения group_by() тиббл будет выглядеть так же, но у него появятся атрибут groups25: heroes %&gt;% group_by(Gender) Если после этого применить на тиббле функцию summarise(), то мы получим не тиббл длиной один, а тиббл со значением для каждой из групп. heroes %&gt;% mutate(imt = Weight/(Height/100)^2) %&gt;% group_by(Gender) %&gt;% summarise(min(imt, na.rm = TRUE), max(imt, na.rm = TRUE)) Схематически это выглядит вот так: 9.8.3 Подсчет строк: dplyr::n(), dplyr::count() Для подсчет количества значений можно воспользоваться функцией n(). heroes %&gt;% group_by(Gender) %&gt;% summarise(n = n()) Функция n() вместе с group_by() внутри filter() позволяет удобным образом “отрезать” от тиббла редкие группы… heroes %&gt;% group_by(Race) %&gt;% filter(n() &gt; 10) %&gt;% select(name, Race) или же наоборот, выделить только маленькие группы: heroes %&gt;% group_by(Race) %&gt;% filter(n() == 1) %&gt;% select(name, Race) Таблицу частот можно создать без group_by() и summarise(n = n()). Функция count() заменяет эту конструкцию: heroes %&gt;% count(Gender) Эту таблицу частот удобно сразу проранжировать, указав в параметре sort = значение TRUE. heroes %&gt;% count(Gender, sort = TRUE) Функция count(), несмотря на свою простоту, является одной из наиболее используемых в tidyverse. 9.8.4 Уникальные значения: dplyr::distinct() dplyr::distinct() - это более быстрый аналог unique(), позволяет извлекать уникальные значения для одной или нескольких колонок. heroes %&gt;% distinct(Gender) heroes %&gt;% distinct(Gender, Race) Иногда нужно аггрегировать данные, но при этом сохранить исходную структуру тиббла. Например, нужно посчитать размер групп или посчитать средние значения по группе для последующего сравнения с индивидуальными значениями. 9.8.5 Создание колонок с группировкой В tidyverse это можно сделать с помощью сочетания group_by() и mutate() (вместо summarise()): heroes %&gt;% group_by(Race) %&gt;% mutate(Race_n = n()) %&gt;% select(Race, name, Gender, Race_n) Результаты аггрегации были записаны в отдельную колонку, при этом значения этой колонки внутри одной группы повторяются: Как и пакет tidyverse, tidymodels — это пакет с несколькими пакетами.↩︎ Если быть точным, то оператор %&gt;% был импортирован во все основные пакеты tidyverse, а сам пакет magrittr не входит в базовый набор tidyverse. Тем не менее, в самом magrittr есть еще несколько интересных операторов.↩︎ Даже наоборот, использование пайпов незначительно снижает скорость выполнения команды.↩︎ Форма F-распределения будет сильно зависеть от числа степеней свободы. Но оно всегда определено от 0 до плюс бесконечности: в числителе и знаменателе всегда неотрицательные числа.↩︎ Как и в случае с magrittr, пакет tidyselect не содержатся в базовом tidyverse, но функции импортируются основыми пакетами tidyverse.↩︎ Выбранный паттерн будет найден посимвольно, если же вы хотите искать по регулярным выражениям, то вместо contains() нужно использовать matches().↩︎ relocate() не позволяет переименовывать колонки в отличие от select() и rename()↩︎ У функции dplyr::summarise() есть синоним dplyr::summarize(), которая делает абсолбтно то же самое. Просто потому что в американском английском и британском английском это слово пишется по-разному.↩︎ Снять группировку можно с помощью функции ungroup().↩︎ "],["tidyverse-advanced.html", "10 Продвинутый tidyverse 10.1 Объединение нескольких датафреймов 10.2 Tidy data: tidyr::pivot_longer(), tidyr::pivot_wider() 10.3 Трансформация нескольких колонок: dplyr::across() 10.4 Функциональное программирование: purrr 10.5 Колонки-списки и нестинг: nest()", " 10 Продвинутый tidyverse 10.1 Объединение нескольких датафреймов 10.1.1 Соединение структурно схожих датафреймов: bind_rows(), bind_cols() Для начала создадим следующие тибблы и сохраним их как dc, marvel и other_publishers: dc &lt;- heroes %&gt;% filter(Publisher == &quot;DC Comics&quot;) %&gt;% group_by(Gender) %&gt;% summarise(weight_mean = mean(Weight, na.rm = TRUE)) dc marvel &lt;- heroes %&gt;% filter(Publisher == &quot;Marvel Comics&quot;) %&gt;% group_by(Gender) %&gt;% summarise(weight_mean = mean(Weight, na.rm = TRUE)) marvel other_publishers &lt;- heroes %&gt;% filter(!(Publisher %in% c(&quot;DC Comics&quot;,&quot;Marvel Comics&quot;))) %&gt;% group_by(Gender) %&gt;% summarise(weight_mean = mean(Weight, na.rm = TRUE)) other_publishers Несколько тибблов можно объединить вертикально с помощью функции bind_rows(). Для корректного объединения тибблы должны иметь одинаковые названия колонок. bind_rows(dc, marvel) Чтобы соединить тибблы горизонтально, воспользуйтесь функцией bind_cols(). bind_cols(dc, marvel) ## New names: ## * Gender -&gt; Gender...1 ## * weight_mean -&gt; weight_mean...2 ## * Gender -&gt; Gender...3 ## * weight_mean -&gt; weight_mean...4 Функции bind_rows() и bind_cols() могут работать не только с двумя, но сразу с несколькими датафреймами. bind_rows(dc, marvel, other_publishers) На входе в функции bind_rows() и bind_cold() можно подавать как сами датафреймы или тибблы через запятую, так и список из датафреймов/тибблов. heroes_list_of_df &lt;- list(DC = dc, Marvel = marvel, Other = other_publishers) bind_rows(heroes_list_of_df) Чтобы не потерять, из какого датафрейма какие данные, можно указать любое строковое значение (название будущей колонки) для необязательного аргумента .id =. bind_rows(heroes_list_of_df, .id = &quot;Publisher&quot;) bind_rows() обычно используется, когда ваши данные находятся в разных файлах с одинаковой структурой. Тогда вы можете прочитать все таблицы в папке, сохранить их в качестве списка из датафреймов и объединить в один датафрейм с помощью bind_rows(). 10.1.2 Реляционные данные: *_join() В реальности иногда возникает ситуация, когда нужно соединить две таблички, у которых есть общий столбец (или несколько столбцов), но все остальные столбцы различаются. Табличек может быть и больше, это может быть целая сеть таблиц, некоторые из которых содержат основные данные, а некоторые - дополнительные, которые необходимо на определенном этапе “включить” в анализ. Например, таблица с расшифровкой аббревиатур или сокращений вроде коротких названий стран или таблица телефонных кодов разных стран. Совокупность нескольких связанных друг с другом таблиц называют реляционными данными. В случае с реляционными данными простых bind_rows() и bind_cols() становится недостаточно. Эти две таблички нужно объединить (join). Эта задача обычно возникает не очень часто, обычно это происходит один-два раза в одном проекте, когда нужно дополнить имеющиеся данные дополнительной информацией извне или объединить два набора данных, обрабатывавшихся в разных программах. Всякий раз, когда такая задача возникает, это доставляет много боли. dplyr предлагает интуитивно понятный инструмент для объединения реляционных данных - семейство функций *_join(). Возьмем для примера два тиббла band_members и band_instruments, встроенных в dplyr специально для демонстрации работы функций *_join(). band_members band_instruments У этих двух тибблов есть колонка с одинаковым названием, которая по своему смыслу соединяет данные обоих тибблов. Такая колонка называется ключом. Ключ должен однозначно идентифицировать наблюдения26. Давайте попробуем посоединять band_members и band_instruments разными вариантами *_join() и посмотрим, что у нас получится. Все эти функции имеют на входе два обязательных аргумента (x = и y =) в которые мы должны подставить два датафрейма/тиббла которые мы хотим объединить. Главное различие между этими функциями заключается в том, что они будут делать, если уникальные значения в ключах x и y не соответствуют друг другу. left_join(): band_members %&gt;% left_join(band_instruments) ## Joining, by = &quot;name&quot; left_join() - это самая простая для понимания и самая используемая функция из семейства *_join(). Она как бы “дополняет” информацию из первого тиббла вторым тибблом. В этом случае сохраняются все уникальные наблюдения в x, но отбрасываются лишние наблюдения в тиббле y. Тем значениям, которым не нашлось соотвествия в y, в колонках, взятых их y, ставятся значения NA. Вы можете сами задать колонки-ключи параметром by =, по умолчанию это все колонки с одинаковыми названиями в двух тибблах. band_members %&gt;% left_join(band_instruments, by = &quot;name&quot;) Часто случается, что колонки-ключи называются по-разному в двух тибблах. Их необязательно переименовывать, можно поставить соответстие вручную используя проименованный вектор: band_members %&gt;% left_join(band_instruments2, by = c(&quot;name&quot; = &quot;artist&quot;)) right_join(): band_members %&gt;% right_join(band_instruments) ## Joining, by = &quot;name&quot; right_join() отбрасывает строчки в x, которых не было в y, но сохраняет соответствующие строчки y - left_join() наоборот. full_join(): band_members %&gt;% full_join(band_instruments) ## Joining, by = &quot;name&quot; Функция full_join() сохраняет все строчки и из x и y. Пожалуй, наиболее используемая функция после left_join() — благодаря full_join() вы точно ничего не потеряете при объединении. inner_join(): band_members %&gt;% inner_join(band_instruments) ## Joining, by = &quot;name&quot; Функция full_join() сохраняет только строчки, которые присутствуют и в x, и в y. semi_join(): band_members %&gt;% semi_join(band_instruments) ## Joining, by = &quot;name&quot; anti_join(): band_members %&gt;% anti_join(band_instruments) ## Joining, by = &quot;name&quot; Функции semi_join() и anti_join() не присоединяют второй датафрейм/тиббл (y) к первому. Вместо этого они используются как некоторый словарь-фильтр для отделения только тех значений в x, которые есть в y (semi_join()) или, наоборот, которых нет в y (anti_join()). 10.2 Tidy data: tidyr::pivot_longer(), tidyr::pivot_wider() Принцип tidy data предполагает, что каждая строчка содержит в себе одно измерение, а каждая колонка - одну характеристику. Тем не менее, это не говорит однозначно о том, как именно хранить повторные измерения. Их можно хранить как одну колонку для каждого измерения (широкий формат) и как две колонки: одна колонка - для идентификатора измерения, другая колонка - для записи самого измерения. Это лучше понять на примере. Например, вес до и после прохождения курса. Как это лучше записать - как два числовых столбца (один испытуемый - одна строка) или же создать отдельную “группирующую” колонку, в которой будет написано время измерения, а в другой - измеренные значения (одно измерение - одна строка)? Широкий формат: Студент До курса по R После курса по R Маша 70 63 Рома 80 74 Антонина 86 71 Длинный\" формат: Студент Время измерения Масса (кг) Маша До курса по R 70 Рома До курса по R 80 Антонина До курса по R 86 Маша После курса по R 63 Рома После курса по R 74 Антонина После курса по R 71 На самом деле, оба варианта приемлимы, оба варианта возможны в реальных данных, а разные функции и статистические пакеты могут требовать от вас как длинный, так и широкий форматы. Таким образом, нам нужно научиться переводить из широкого формата в длинный и наоборот. tidyr::pivot_longer(): из широкого в длинный формат tidyr::pivot_wider(): из длинного в широкий формат new_diet &lt;- tibble( student = c(&quot;Маша&quot;, &quot;Рома&quot;, &quot;Антонина&quot;), before_r_course = c(70, 80, 86), after_r_course = c(63, 74, 71) ) new_diet Тиббл new_diet - это пример широкого формата данных. Превратим тиббл new_diet длинный: new_diet %&gt;% pivot_longer(cols = before_r_course:after_r_course, names_to = &quot;measurement_time&quot;, values_to = &quot;weight_kg&quot;) А теперь обратно в короткий: new_diet %&gt;% pivot_longer(cols = before_r_course:after_r_course, names_to = &quot;measurement_time&quot;, values_to = &quot;weight_kg&quot;) %&gt;% pivot_wider(names_from = &quot;measurement_time&quot;, values_from = &quot;weight_kg&quot;) 10.3 Трансформация нескольких колонок: dplyr::across() Допустим, вы хотите посчитать среднюю массу и рост, группируя по полу супергероев. Можно посчитать это внутри одного summarise(), использую запятую: heroes %&gt;% group_by(Gender) %&gt;% summarise(height = mean(Height, na.rm = TRUE), weight = mean(Weight, na.rm = TRUE)) Если таких колонок будет много, то это уже станет сильно неудобным, нам придется много копировать код, а это чревато ошибками и очень скучно. Поэтому в dplyr есть функция для операций над несколькими колонками сразу: dplyr::across()27. Эта функция работает похожим образом на функции семейства apply() и использует tidyselect для выбора колонок. Таким образом, конструкции с функцией across() можно разбить на три части: Выбор колонок с помощью tidyselect. Здесь работают все те приемы, которые мы изучили при выборе колонок (9.5.2). Собственно применение функции across(). Первый аргумент .col — колонки, выбранные на первом этапе с помощью tidyselect, по умолчанию это everything(), т.е. все колонки. Второй аргумент .fns — это функция или целый список из функций, которые будут применены к выбранным колонкам. Если функции требуют дополнительных аргументов, то они могут быть перечислены внутри across(). Использование summarise() или другой функции dplyr. В этом случае в качестве аргумента для функции используется результат работы функции across(). Вот такой вот бутерброд выходит. Давайте посмотрим, как это работает на практике и посчитаем среднее значение по колонкам Height и Weight. heroes %&gt;% group_by(Gender) %&gt;% summarise(across(c(Height,Weight), mean)) Здесь мы столкнулись с уже известной нам проблемой: функция mean() при столкновении хотя бы с одним NA будет возвращать NA, если мы не изменим параметр na.rm =. Как и в случае с функциями семейства apply() (@ref(apply_f)), дополнительные параметры для функции можно перечислить через запятую после самой функции: heroes %&gt;% group_by(Gender) %&gt;% summarise(across(c(Height, Weight), mean, na.rm = TRUE)) До этого мы просто использовали выбор колонок по их названию. Но именно внутри across() использование tidyselect раскрывается как удивительно элегантный и мощный инструмент. Например, можно посчитать среднее для всех numeric колонок: heroes %&gt;% drop_na(Height, Weight) %&gt;% group_by(Gender) %&gt;% summarise(across(where(is.numeric), mean, na.rm = TRUE)) Или длину строк для строковых колонок. Для этого нам понадобится вспомнить, как создавать анонимные функции (@ref(anon_f)). heroes %&gt;% group_by(Gender) %&gt;% summarise(across(where(is.character), function(x) mean(nchar(x), na.rm = TRUE))) Или же даже посчитать и то, и другое внутри одного summarise()! heroes %&gt;% group_by(Gender) %&gt;% summarise(across(where(is.numeric), mean, na.rm = TRUE), across(where(is.character), function(x) mean(nchar(x), na.rm = TRUE))) Внутри одного across() можно применить не одну функцию к каждой из выбранных колонок, а сразу несколько функций для каждой из колонок. Для этого нам нужно использовать список функций (желательно - проименованный). heroes %&gt;% group_by(Gender) %&gt;% summarise(across(c(Height, Weight), list(minimum = min, average = mean, maximum = max), na.rm = TRUE)) Вот нам и понадобился список функций (@ref(functions_objects))! heroes %&gt;% group_by(Gender) %&gt;% summarise(across(c(Height, Weight), list(min = function(x) min(x, na.rm = TRUE), mean = function(x) mean(x, na.rm = TRUE), max = function(x) max(x, na.rm = TRUE), na_n = function(x, ...) sum(is.na(x))) ) ) Хотя основное применение функции across() — это массовое подытоживание с помощью summarise(), across() можно использовать и с другими функциями dplyr. Например, можно делать массовые операции с колонками с помощью mutate(): heroes %&gt;% mutate(across(where(is.character), as.factor)) Конструкция across() работает не только внутри summarise() и mutate(), можно применять across() и с другими функциями, которые используют data-masking. Например, можно использовать across() внутри count() вместе с функцией n_distinct(), которая считает количество уникальных значений в векторе. Это позволяет посмотреть таблицу частот для группирующих переменных: heroes %&gt;% count(across(where(function(x) n_distinct(x) &lt;= 6))) 10.4 Функциональное программирование: purrr purrr — это пакет для функционального программирования в tidyverse. Как и многие пакеты tidyverse, purrr пытается заменить собой базовый функционал R на более понятный и удобный. В данном случае, речь в первую очередь идет о функциях семейства apply(), с которыми мы работали ранее (@ref(apply_f)). Давайте вспомним, как работает lapply(). В качестве первого аргумента функция lapply() принимает список (или то, что может быть в него превращено, например, датафрейм), в качестве второго - функцию, которая будет применена к каждому элементу списка. На выходе мы получим список такой же длины. lapply(heroes, class) ## $X1 ## [1] &quot;numeric&quot; ## ## $name ## [1] &quot;character&quot; ## ## $Gender ## [1] &quot;character&quot; ## ## $`Eye color` ## [1] &quot;character&quot; ## ## $Race ## [1] &quot;character&quot; ## ## $`Hair color` ## [1] &quot;character&quot; ## ## $Height ## [1] &quot;numeric&quot; ## ## $Publisher ## [1] &quot;character&quot; ## ## $`Skin color` ## [1] &quot;character&quot; ## ## $Alignment ## [1] &quot;character&quot; ## ## $Weight ## [1] &quot;numeric&quot; Функция purrr::map() работает по тому же принципу: можно просто заменить lapply() на map(), и мы получим тот же результат. map(heroes, class) ## $X1 ## [1] &quot;numeric&quot; ## ## $name ## [1] &quot;character&quot; ## ## $Gender ## [1] &quot;character&quot; ## ## $`Eye color` ## [1] &quot;character&quot; ## ## $Race ## [1] &quot;character&quot; ## ## $`Hair color` ## [1] &quot;character&quot; ## ## $Height ## [1] &quot;numeric&quot; ## ## $Publisher ## [1] &quot;character&quot; ## ## $`Skin color` ## [1] &quot;character&quot; ## ## $Alignment ## [1] &quot;character&quot; ## ## $Weight ## [1] &quot;numeric&quot; map() можно встроить в канал с пайпом (впрочем, как и lapply()): heroes %&gt;% map(class) ## $X1 ## [1] &quot;numeric&quot; ## ## $name ## [1] &quot;character&quot; ## ## $Gender ## [1] &quot;character&quot; ## ## $`Eye color` ## [1] &quot;character&quot; ## ## $Race ## [1] &quot;character&quot; ## ## $`Hair color` ## [1] &quot;character&quot; ## ## $Height ## [1] &quot;numeric&quot; ## ## $Publisher ## [1] &quot;character&quot; ## ## $`Skin color` ## [1] &quot;character&quot; ## ## $Alignment ## [1] &quot;character&quot; ## ## $Weight ## [1] &quot;numeric&quot; Как и lapply(), map() всегда возвращает список. Из-за этого мы больше пользовались функцией sapply(), а не lapply(). Функция sapply() упрощала результат до вектора, если это возможно. Подобное упрощение может показаться удобным пока не сталкиваешься с тем, что иногда очень сложно предсказать, какой тип данных получится на выходе. Есть функция vapply() в которой можно управлять типом данных на выходе, но она не очень удобная. В purrr эта проблема решена просто: есть множество функций map_*(), где вместо звездочки - нужный формат на выходе. Например, если мы хотим получить строковый вектор на выходе, то нам нужна функция map_chr(). heroes %&gt;% map_chr(class) ## X1 name Gender Eye color Race Hair color ## &quot;numeric&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; ## Height Publisher Skin color Alignment Weight ## &quot;numeric&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;numeric&quot; Можно превратить результат в датафрейм с помощью map_df(). heroes %&gt;% map_df(class) Так же как и функции семейства apply(), функции map_*() отлично сочетаются с анонимными функциями. heroes %&gt;% map_int(function(x) sum(is.na(x))) ## X1 name Gender Eye color Race Hair color Height ## 0 0 29 172 304 172 217 ## Publisher Skin color Alignment Weight ## 0 662 7 239 Однако у purrr есть свой, более короткий способ записи анонимных функций: function(arg) заменяется на ~, а arg на .. heroes %&gt;% map_int(~sum(is.na(.))) ## X1 name Gender Eye color Race Hair color Height ## 0 0 29 172 304 172 217 ## Publisher Skin color Alignment Weight ## 0 662 7 239 Если нужно итерироваться сразу по нескольким спискам, то есть функции map2_*() (для двух списков) и pmap_*() (для нескольких списков). 10.5 Колонки-списки и нестинг: nest() Ранее мы говорили о том, что датафрейм — это по своей сути список из векторов разной длины. На самом деле, это не совсем так: колонки обычного датафрейма вполне могут быть списками. Однако делать так обычно не рекомендуется, пусть R это и не запрещает создавать такие колонки: многие функции предполагают, что все колонки датафрейма являются векторами. tidyverse гораздо дружелюбнее относится к использовании списка в качестве колонки. Такие колонки называются колонками-списками (list columns). Основной способ их создания - использования функции tidyr::nest(). С помощью tidyselect нужно выбрать сжимаемые колонки, которые будут агрегированы по невыбранным колонками. Это и называется нестингом. heroes %&gt;% nest(!Gender) ## Warning: All elements of `...` must be named. ## Did you want `data = c(X1, name, `Eye color`, Race, `Hair color`, Height, Publisher, ## `Skin color`, Alignment, Weight)`? Заметьте, у нас появилась колонка data, в которой содержатся тибблы. Туда и спрятались все наши данные. Нестинг похож на агрегирование с помощью group_by(). Если сделать нестинг сгруппированного с помощью group_by() тиббла, то сожмутся все колонки кроме тех, которые выступают в качестве групп: heroes %&gt;% group_by(Gender) %&gt;% nest() Теперь можно работать с колонкой-списком как с обычной колонкой. Например, применять функцию для каждой строчки (то есть для каждого тиббла) с помощью map() и записывать результат в новую колонку с помощью mutate(). heroes %&gt;% group_by(Gender) %&gt;% nest() %&gt;% mutate(dim = map(data, dim)) В конце концов нам нужно “разжать” сжатую колонку-список. Сделать это можно с помощью unnest(), выбрав с помощью tidyselect нужные колонки. heroes %&gt;% group_by(Gender) %&gt;% nest() %&gt;% mutate(dim = map(data, dim)) %&gt;% unnest(dim) Разжатая колонка обычно больше сжатой, поэтому разжатие привело к удлинению тиббла. Вместо удлинения тиббла, его можно расширить с помощью unnest_wider(). heroes %&gt;% group_by(Gender) %&gt;% nest() %&gt;% mutate(dim = map(data, dim)) %&gt;% unnest_wider(dim, names_sep = &quot;_&quot;) Нестинг - это мощный инструмент tidyverse, хотя во многих случаях можно обойтись и без него. Наиболее эффективна эта конструкция именно в тех ситуациях, где вы делаете операции над целыми тибблами. Поэтому наибольшее распространение нестинг получил в смычке с пакетом broom для расчета множественных статистических моделей. Если ключи будут неуникальными, то функции *_join() не будут выдавать ошибку. Вместо этого они добавят в итоговую таблицу все возможные пересечения повторяющихся ключей. С этим нужно быть очень осторожным, поэтому рекомендуется, во-первых, проверять уникальность ключей на входе и, во-вторых, проверять тиббл на выходе. Ну или использовать эту особенность работы функции *_join() себе во благо.↩︎ Функция across() появилась в пакете dplyr относительно недавно, до этого для работы с множественными колонками в tidyverse использовались многочисленные функции *_at(), *_if(), *_all(), например, summarise_at(), summarise_if(), summarize_all(). Эти функции до сих пор присутствуют в dplyr, но считаются устаревшими. Другая альтернатива - использование пакета purrr (10.4) или семейства функций apply() (@ref(apply_f)).↩︎ "],["vdesc.html", "11 Описательная статистика 11.1 Описательная статистика и статистика вывода 11.2 Типы шкал 11.3 Меры центральной тенденции 11.4 Меры рассеяния 11.5 Ассиметрия и эксцесс 11.6 Квантили 11.7 А теперь все вместе! 11.8 История грамматики графики 11.9 Основы грамматики графики 11.10 Пример №0: пайчарт с распределение по полу 11.11 Пример №1: Education and IQ meta-analysis 11.12 Расширения ggplot2 11.13 Динамические визуализации в R", " 11 Описательная статистика 11.1 Описательная статистика и статистика вывода Статистика делится на описательную статистику (descriptive statistics) и статистику вывода (inferential statistics). Описательная статистика пытается описать нашу выборку (sample, т.е. те данные, что у нас на руках) различными способами. Проблема в том, что описательная статистика может описать только то, что у нас есть, но не позволяет сделать выводы о генеральной совокупности (population) - это уже цель статистики вывода. Цель описательной статистики - “ужать” данные для их обобщенного понимания с помощью статистик. Заметьте, у выборки (sample) мы считаем статистики (statistics), а у генеральной совокупности (Population) есть параметры (Parameters). Вот такая вот мнемотехника. Статистики часто выступают в роли точечной оценки (point estimators) параметров, так что в этом легко запутаться. Например, среднее (в выборке) - это оценка среднего (в генеральной совокупности). Да, можно свихнуться. Мы это будем разбирать подробнее в следующие занятия (это действительно важно, поверьте), пока что остановимся только на описании выборки. 11.2 Типы шкал Перед тем, как начать речь об описательных статистиках, нужно разобраться с существующими типами шкал. Типы шкал классифицируются на основании типа измеряемых данных, которые задают допустимые для данной шкалы отношения. - Шкала наименований (номинальная шкала) — самая простая шкала, где единственное отношение между элементами — это отношения равенства и неравенства. Это любая качественная шкала, между элементами которой не могут быть установлены отношения “больше — меньше.” Это большинство группирующих переменных (экспериментальная группа, пол, политическая партия, страна), переменные с id. Еще один пример - номера на майках у футболистов. - Шкала порядка (ранговая шкала) — шкала следующего уровня, для которой можно установить отношения “больше — меньше,” причем если B больше A, а C больше B, то и C должно быть больше A. Если это верно, то мы можем выстроить последовательность значений. Однако мы еще не можем говорить о разнице между значениями. Ответы на вопросы “Как часто вы курите?” по шкале “Никогда,” “Редко” и “Часто” являются примером ранговой шкалы. “Часто” — это чаще, чем “Редко,” “Редко” — это чаще чем “Никогда,” и, соотвественно, “Часто” — это чаще, чем “Никогда.” Но мы не можем сказать, что разница между “Часто” и “Редко” такая же, как и между “Редко” и “Никогда.” Соответственно, даже если мы обозначим “Часто,” “Редко” и “Никогда” как 3, 2 и 1 соответственно, то многого не можем сделать с этой шкалой, Например, мы не можем посчитать арифметическое среднее для такой шкалы. - Шкала разностей (интервальная шкала) — шкала, для которой мы уже можем говорить про разницы между интервалами. Например, разница между 10 Cº и 20 Cº такая же как и между 80 Cº и 90 Cº. Для шкалы разностей уже можно сравнивать средние, но операции умножения и деления не имеют смысл, потому что ноль в шкале разностей относительный. Например, мы не можем сказать, что 20 Cº — это в два раза теплее, чем 10 Cº, потому что 0 Cº — это просто условно взятая точка — температура плавления льда. - Шкала отношений (абсолютная шкала) — самая “полноценная” шкала, которая отличается от интервальной наличием естественного и однозначного начала координат. Например, масса в килограммах или та же температура, но в градусах Кельвина, а не Цельсия. 11.3 Меры центральной тенденции Для примера мы возьмем массу супергероев, предварительно удалив из нее все NA для удобства. weight &lt;- heroes %&gt;% drop_na(Weight) %&gt;% pull(Weight) Мера центральной тенденции - это число для описания центра распределения. 11.3.1 Арифметическое среднее Самая распространенная мера центральных тенденций - арифметическое среднее, то самое, которые мы считаем с помощью функции mean(). \\[\\overline{x}= \\frac{\\sum\\limits_{i=1}^{n} x_{i}} {n}\\] Не пугайтесь значка \\[\\sum\\limits_{i=1}^{n}\\] - это означает сумму от i = 1 до n. Что-то вроде цикла for! В качестве упражнения попробуйте самостоятельно превратить эту формулу в функцию mymean() c помощью sum() и length(). Можете убирать NA по дефолту! Сравните с результатом функции mean(). mean(weight) ## [1] 112.2525 11.3.2 Медиана Медиана - это середина распределения. Представим, что мы расставили значения по порядку (от меньшего к большему) и взяли значение по середине. Если у нас четное количество значений, то берется среднее значение между теми двумя, что по середине. Для расчета медианы есть функция median(): median(weight) ## [1] 81 Разница медианы со средним не очень существенная. Это значит, что распределение довольно “симметричное.” Но бывает и по-другому. Представьте себе, что кто-то говорит про среднюю зарплату в Москве. Но ведь эта средняя зарплата становится гораздо больше, если учитывать относительно небольшое количество мультимиллионеров и миллиардеров! А вот медианная зарплата будет гораздо меньше. Представьте себе, что в среде супергероев поялвяется кто-то, кто весит 9000 килограммов! Тогда среднее сильно изменится: mean(c(weight, 9000)) ## [1] 130.1714 А вот медиана останется той же. median(c(weight, 9000)) ## [1] 81 Таким образом, экстремально большие или маленькие значения оказывают сильное влияние на арифметическое среднее, но не на медиану. Поэтому медиана считается более “робастной” оценкой, т.е. более устойчивой к выбросам и крайним значениям. 11.3.3 Усеченное среднее (trimmed mean) Если про среднее и медиану слышали все, то про усеченное (тримленное) среднее известно гораздо меньше. Тем не менее, на практике это довольно удобная штука, потому что представляет собой некий компромисс между арифметическим средним и медианой. В усеченном среднем значения ранжируются так же, как и для медианы, но отбрасывается только какой-то процент крайних значений. Усеченное среднее можно посчитать с помощью обычной функции mean(), поставив нужное значение параметра trim =: mean(weight, trim = 0.1) ## [1] 89.56423 trim = 0.1 означает, что мы отбросили 10% слева и 10% справа. trim может принимать значения от 0 до 0.5. Что будет, если trim = 0? mean(weight, trim = 0) ## [1] 112.2525 Обычное арифметическое среднее! А если trim = 0.5? mean(weight, trim = 0.5) ## [1] 81 Медиана! 11.3.4 Мода Мода (mode) - это самое частое значение. Обычно используется для номинальных переменных, для континуальных данных мода неприменима. Что интересно, в R нет встроенной функции для подсчета моды. Обычно она и не нужна: мы можем посчитать таблицу частот и даже проранжировать ее (и мы уже умеем это делать разными способами). heroes %&gt;% count(Gender, sort = TRUE) Можете попробовать написать свою функцию для моды! 11.4 Меры рассеяния Начинающий статистик пытался перейти в брод реку, средняя глубина которой 1 метр. И утонул. В чем была его ошибка? Он не учитывал разброс значений глубины! Мер центральной тенденции недостаточно, чтобы описать выборку. Необходимо знать ее вариабельность. ### Размах {range} Самое очевидное - посчитать размах (range), то есть разницу между минимальным и максимальным значением. В R есть функция для вывода максимального и минимального значений: range(weight) ## [1] 2 900 Осталось посчитать разницу между ними: diff(range(weight)) ## [1] 898 Естественно, крайние значения очень сильно влияют на этот размах, поэтому на практике он не очень-то используется. 11.4.1 Дисперсия Дисперсия (variance) вычисляется по следующей формуле: \\[s^2= \\frac{\\sum\\limits_{i=1}^{n} (x_{i} - \\overline{x})^2} {n}\\] Попробуйте превратить это в функцию myvar()! myvar &lt;- function(x) mean((x - mean(x))^2) Естественно, в R уже есть готовая функция var(). Но, заметьте, ее результат немного отличается от нашего: myvar(weight) ## [1] 10825.55 var(weight) ## [1] 10847.46 Дело в том, что встроенная функция var() делит не на \\(n\\), а на \\(n-1\\). Это связано с тем, что эта функция пытается оценить дисперсию в генеральной совокупности, т.е. относится уже к статистике вывода. Про это мы будем говорить в дальнейших занятиях, сейчас нам нужно только отметить то, что здесь есть небольшое различие. 11.4.2 Стандартное отклонение Если вы заметили, значение дисперсии очень большое. Чтобы вернуться к единицам измерения, соответствующих нашим данным используется корень из дисперсии, то есть стандартное отклонение (standard deviation): \\[s= \\sqrt\\frac{\\sum\\limits_{i=1}^{n} (x_{i} - \\overline{x})^2} {n}\\] Для этого есть функция sd(): sd(weight) ## [1] 104.1511 Что то же самое, что и: sqrt(var(weight)) ## [1] 104.1511 11.4.3 Медианное абсолютное отклонение Поскольку стандартное отклонение не устойчиво к выбросам, то иногда используют его альтернативу, которая устойчива к выбросам (особенно если эти выбросы нам как раз и нужно удалить) - медианное абсолютное отклонение (median absolute deviation): \\[mad= median(|x_{i} - median(x)|)\\] Для этого есть функция mad(): mad(weight) ## [1] 32.6172 11.4.4 Межквартильный размах Другой вариант рабостной оценки вариабельности данных является межквартильный размах (interquartile range, IQR). Это разница между третьим и первым квартилем28 - значением, которое больше 75% значений в выборке, и значением, которое больше 25% значений в выборке. IQR(weight) ## [1] 47 Ну а второй квартиль - это медиана! 11.5 Ассиметрия и эксцесс 11.5.1 Ассиметрия Ассиметрия (skewness) измеряет симметричность распределения. Положительный показатель ассиметрии (“Right-skewed” или positive skewness) означает, что хвосты с правой части распределения длиннее. Негативный показатель ассиметрии (“Left-skewed” или negative skewness) означает, что левый хвост длиннее. Например, в психологии положительная ассиметрия встречается очень часто. Например, время реакции: оно ограничено снизу 0 мс (а по факту не меньше 100 мс - быстрее сигнал не успеет по нервной системе пройти до пальцев), а вот с другой стороны оно никак не ограничено. Испытуемый может на полчаса перед монитором затупить, ага. 11.5.2 Эксцесс Эксцесс (kurtosis) - это мера “вытянутости” распределения: Положительные показатели эксцесса означают “вытянутое” распределение, а отрицательные - “плоское.” 11.5.3 Ассиметрия и эксцесс в R К сожалению, в базовом R нет функций для ассиметрии и эксцесса. Зато есть замечательный пакет psych (да-да, специально для психологов). install.packages(&quot;psych&quot;) library(&quot;psych&quot;) ## ## Присоединяю пакет: &#39;psych&#39; ## Следующие объекты скрыты от &#39;package:ggplot2&#39;: ## ## %+%, alpha В нем есть функции skew() и kurtosi(): skew(weight) ## [1] 3.874557 kurtosi(weight) ## [1] 19.45699 Ассиметрия положительная, это значит что распределение выборки асимметричное, хвосты с правой части длиннее. Эксцесс значительно выше нуля - значит распределение довольно “вытянутое.” 11.6 Квантили В жизни мы постоянно проходим какие-нибудь тесты, получаем баллы и рано или поздно встает вопрос: ну а как оно у других? Как бы нас ни учили книжки по саморазвитию, что не стоит сравнивать себя с другими, от этого вопроса очень сложно избавиться. А иногда и вовсе не нужно. Допустим, вы проходите профессиональный тест с задачами одинаковой сложности. Как понять, если вы решили 10 из 20 задач (допустим, что задачи одинаковой сложности), то это много или мало? Мы договорились, что задачи одинаковой сложности, но не сказали какой. Если все 20 задач очень легкие, то 10 – это мало, а если сложные – то много. В этой ситуации может быть важен относительный успех: сколько людей справились с тестом хуже вас, а сколько - лучше вас. Вот это и позволяют посчитать процентили (percentile rank) – процент значений в распределении ниже заданного значения. То есть 90ый процентиль означает, что вы справились лучше, чем 90% людей, который прошли тот же тест. То есть вы находитесь в 10% самых-самых! Поэтому настоящие понторезы должны меряться не абсолютными значениями, а процентилями. Здесь сразу нужно оговориться, что понятие процентиля имеет несколько неоднозначностей. В английском принято разделять percentile и percentile rank. Percentile rank – это процент значений в распределении ниже заданного, то просто percentile – это само значение, ниже которого находится соответствующий процент значений. А иногда и вовсе процентилем называют сам интервал между процентильными границами. Все эти понятия взаимосвязаны, поэтому о том, в каком именно значении используется понятие “процентиль” можно догадаться из контекста. Другая неоднозначность понятия процентиля связана с тем, в какой процентиль относить пограничные значения. Эта проблема породила целых девять различных подходов к расчету процентилей! Однако если шкала континуальная и имеет достаточно много значений, то разницы между этими подходами не будет. Можно делить значения не на 100 интервалов, а на меньшее количество. Например, на 4. Для этого нам нужно три точки: одна отделяет 25% наименьших значений, вторая отделяет нижнее 50% от верхних 50% (то есть это медиана!), третья – верхние 25% отнижних 75%. Эти точки и интервалы, разделяемые ими, называются квартилями. Кроме процентилей и квартилей есть еще децили, квинтили, секстили, септили и что угодно -тили, хотя и используются они гораздо реже. Общее название для всех них – квантили. 11.7 А теперь все вместе! В базовом R есть функция summary(), которая позволяет получить сразу неплохой набор описательных статистик. summary(weight) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.0 61.0 81.0 112.3 108.0 900.0 Функция summary() - это универсальная (generic) функция. Это означает, что Вы можете ее применять для разных объектов и получать разные результаты. Попробуйте применить ее к векторам с разными типами данных и даже к дата.фреймам и дата.тейблам. Посмотрите, что получится. В пакете psych есть еще и замечательная функция describe(), которая даст Вам еще больше статистик, включая ассиметрию и куртозис: psych::describe(weight) Даже усеченное (trimmed) среднее есть (с trim = 0.1)! Все кроме se мы уже знаем. А про этот se узнаем немного позже. Эта функция хорошо работает в сочетании с group_by(): heroes %&gt;% group_by(Gender) %&gt;% summarise(describe(Weight)) Другой интересный пакет для получения описательных статистик для всего датафрейма — skimr. install.packages(&quot;skimr&quot;) Его основная функция — skim(), выводит симпатичную сводную таблицу для датафрейма. skimr::skim(heroes) Таблица 11.1: Data summary Name heroes Number of rows 734 Number of columns 11 _______________________ Column type frequency: character 8 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace name 0 1.00 1 25 0 715 0 Gender 29 0.96 4 6 0 2 0 Eye color 172 0.77 3 23 0 22 0 Race 304 0.59 5 18 0 61 0 Hair color 172 0.77 3 16 0 29 0 Publisher 0 1.00 0 17 15 25 0 Skin color 662 0.10 3 14 0 16 0 Alignment 7 0.99 3 7 0 3 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist X1 0 1.00 366.50 212.03 0.0 183.25 366.5 549.75 733 ▇▇▇▇▇ Height 217 0.70 186.73 59.25 15.2 173.00 183.0 191.00 975 ▇▁▁▁▁ Weight 239 0.67 112.25 104.15 2.0 61.00 81.0 108.00 900 ▇▁▁▁▁ Здесь количество и доля пропущенных значений, среднее, стандартное отклонение, минимальное и максимальное значение (p0 и p100 соответственно), квартили. Ну и вишенкой на торте выступает маленькая гистограмма для каждой колонки! Кроме того, skimr адаптирован под tidyverse. В нем можно выбирать колонки с помощью tidyselect (9.5.2) прямо внутри функции skim(). heroes %&gt;% skimr::skim(ends_with(&quot;color&quot;)) Таблица 11.2: Data summary Name Piped data Number of rows 734 Number of columns 11 _______________________ Column type frequency: character 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Eye color 172 0.77 3 23 0 22 0 Hair color 172 0.77 3 16 0 29 0 Skin color 662 0.10 3 14 0 16 0 А еще можно сочетать с группировкой с помощью group_by(). heroes %&gt;% group_by(Gender) %&gt;% skimr::skim(ends_with(&quot;color&quot;)) Таблица 11.3: Data summary Name Piped data Number of rows 734 Number of columns 11 _______________________ Column type frequency: character 3 ________________________ Group variables Gender Variable type: character skim_variable Gender n_missing complete_rate min max empty n_unique whitespace Eye color Female 41 0.80 3 23 0 14 0 Eye color Male 121 0.76 3 12 0 18 0 Eye color NA 10 0.66 3 23 0 7 0 Hair color Female 38 0.81 3 16 0 18 0 Hair color Male 123 0.76 3 16 0 23 0 Hair color NA 11 0.62 4 14 0 10 0 Skin color Female 186 0.07 4 6 0 7 0 Skin color Male 449 0.11 3 14 0 14 0 Skin color NA 27 0.07 4 4 0 2 0 ###Описательных статистик недостаточно {#datasaurus} Я в тайне от Вас загрузил данные в переменную xxx (можете найти этот набор данных здесь, если интересно). Выглядят они примерно так: head(xxx) str(xxx) ## spec_tbl_df [142 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ x: num [1:142] 55.4 51.5 46.2 42.8 40.8 ... ## $ y: num [1:142] 97.2 96 94.5 91.4 88.3 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. x = col_double(), ## .. y = col_double() ## .. ) Надеюсь, Вы уже понимаете, как это интерпретировать - два столбца с 142 числами каждый. Представьте себе, как выглядят эти точки на плоскости, если каждая строчка означают координаты одной точки по осям x и y (это называется диаграмма рассеяния, точечная диаграмма или scatterplot). Применим разные функции, которые мы выучили: mean(xxx$x) ## [1] 54.26327 mean(xxx$y) ## [1] 47.83225 median(xxx$x) ## [1] 53.3333 median(xxx$y) ## [1] 46.0256 Средние и медианы примерно одинаковые, при этом по х они около 53-54, а по у - примерно 46-47. Попытайтесь представить это. Идем дальше: sd(xxx$x) ## [1] 16.76514 sd(xxx$y) ## [1] 26.9354 Похоже, разброс по у несколько больше, верно? skew(xxx$x) ## [1] 0.2807568 skew(xxx$y) ## [1] 0.2472603 kurtosi(xxx$x) ## [1] -0.2854912 kurtosi(xxx$y) ## [1] -1.063552 Похоже, оба распределения немного право-ассиметричны и довольно “плоские.” Давайте еще посчитаем коэффициент корреляции (correlation coefficient). Мы про него будем говорить позже гораздо подробнее. Пока что нам нужно знать, что она говорит о линейной связи двух переменных. Если коэффициент корреляции положительный (максимум равен 1), то чем больше х, тем больше у. Если отрицательный (минимум равен -1), то чем больше х, тем меньше у. Если же коэффициент корреляции равна нулю, то такая линейная зависимость отсутствует. cor(xxx$x, xxx$y) ## [1] -0.06447185 Коэффициент корреляции очень близка к нулю (делайте выводы и представляйте). Давайте напоследок воспользуемся функцией describe() из psych: psych::describe(xxx) skimr::skim(xxx) Таблица 11.4: Data summary Name xxx Number of rows 142 Number of columns 2 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist x 0 1 54.26 16.77 22.31 44.10 53.33 64.74 98.21 ▅▇▇▅▂ y 0 1 47.83 26.94 2.95 25.29 46.03 68.53 99.49 ▇▇▇▅▆ Готовы узнать, как выглядят эти данные на самом деле?! Жмите сюда если готовы! Из этого можно сделать важный вывод: не стоит слепо доверять описательным статистикам. Нужно визуализировать данные, иначе можно попасть в такую ситуацию в реальности. По словам знаменитого статитстика Джона Тьюки, величайшая ценность картинки в том, что она заставляет нас заметить то, что мы не ожидали заметить. Поэтому графики — это не просто метод коммуникации — представления ваших результатов сообществу в понятном виде (хотя и это, конечно, тоже), но и сам по себе очень важный метод анализа данных. #Визуализация в R {#r_vis} ## Базовые функции для графики {#base_vis} В R есть достаточно мощный встроенный инструмент для визуализации. Я приведу три простых примера. Во-первых, это та самая диаграмма рассеяния. Здесь все просто: функция plot(), вектора x и у, дополнительные параметры для цвета, размера, формы точек. Для примера возьмем датасет heroes с Height по оси x и Weight по оси y. plot(heroes$Height, heroes$Weight) Между прочим, функция plot() - это тоже универсальная (generic) функция, как и summary(). В качестве аргумента можете ей скормить просто один вектор, матрицу, датафрейм. Более того, многие пакеты добавляют новые методы plot() для новых объектов из этих пакетов. Другая распространенная функция — hist() — гистограмма (histogram): hist(weight) Ну и закончим на суперзвезде прошлого века под названием ящик с усами (boxplot with whiskers): boxplot(Weight ~ Gender, heroes) Здесь мы использовали уже знакомый нам класс формул. Они еще будут нам встречаться дальше, обычно они используются следующим образом: слева от ~ находится зависимая переменная, а справа - “предикторы.” Эта интуиция работает и здесь: мы хотим посмотреть, как различается вес в зависимости от пола. 11.8 История грамматики графики Встроенные возможности для визуализации в R довольно обширны, но дополнительные пакеты значительно ее расширяют. Среди этих пакетов, есть один, который занимает совершенно особенное место – ggplot2. ggplot2 — это не просто пакет, который рисует красивые графики. Красивые графики можно рисовать и в базовом R. Чтобы понять, почему пакет ggplot2 занимает особенное место среди пакетов для визуализации (и не только среди пакетов для R, а вообще!), нужно расшифровать gg в его названии. gg означает грамматику графики (Grammar of Graphics), язык для описания графиков, изложенный в одноименной книге Леланда Уилкинсона (Wilkinson 2005). Грамматика графики позволяет описывать графики не в терминах типологии (вот есть пайчарт, есть барплот, есть гистограмма, а есть ящик с усами), а с помощью специального разработанного языка. Этот язык позволяет с помощью грамматики и небольшого количества “слов” языка описывать и создавать практически любые графики и даже придумывать новые! Это дает огромную свободу в создании именно той визуализации, что необходима для текущей задачи. Хэдли Уикхэм (да, снова он) немного дополнил идею грамматики графики в статье “A Layered grammar of graphics” (Wickham 2010), которую сопроводил пакетом ggplot2 с реализацией идей Уилкинсона и своих. 11.9 Основы грамматики графики Каждый график состоит из одного или нескольких слоев (layers). Если слоев несколько, то они располагаются один над другим, при этом верхние слои “перекрывают” нижние, примерно как это происходит в программах вроде Adobe Photoshop. У каждого слоя есть три обязательных элемента: данные (data), геом (geom), эстетики (aestetics); и два вспомогательных: статистические трансформации (stat). и регулировка положения (position adjustment). Данные (data). Собственно, сами данные в виде датафрейма, используемые в данном слое. Геом (geom). Геом — это сокращение от “геометрический объект.” Собственно, в какой геометрический объект мы собираемся превращать данные. Например, в точки, прямоугольники или линии. Отображение (mapping). Эстетические отображения или просто эстетики (aestetics) — это набор правил, как различные переменные превращаются в визуальные особенности геометрии. Без эстетик остается непонятно, какие именно колонки в используемом датафрейме превращаются в различные особенности геомов: позицию, размер, цвет и т.д. У каждой геометрии свой набор эстетик, но многие из них совпадают у разных геомов, например, x, y, colour, fill, size. Без некоторых эстетик геом не будет работать. Например, геометрия в виде точек не будет работать без двух координат этих точек (x и y). Другие эстетики необязательны и имеют значения по умолчанию. Например, по умолчанию точки будут черными, но можно сделать их цвет зависимым от выбранной колонки в датафрейме с помощью эстетики colour. Статистические трансформации (stat). Название используемой статистической трансформации (или просто — статистики). Да, статистические трансформации можно делать прямо внутри ggplot2! Это дает дополнительную свободу в выборе инструментов, потому что обычно те же статистические трансформации можно сделать вне ggplot2 в процессе препроцессинга. Формально, статистические трансформации — это обязательный элемент геома, но если вы не хотите преобразовывать данные, то можете выбрать “identity” преобразование, которое оставляет все как есть. В ggplot2 у каждого геома есть статистика по умолчанию, а у каждой статистики - свой геом по умолчанию. И не всегда статистика по умолчанию — это “identity” статистика. Например, для барплота (geom_barplot()) используется статистика “count,”29 которая считает частоты, ведь именно частоты затем трансформируются в высоту барплотов. Регулировка положения (position adjustment). Регуляровка положения — это небольшое улучшение позиции геометрий для части элементов. Например, можно добавить немного случайного шума (“jitter”) в позицию точек, чтобы они не перекрывали друг друга. Или “раздвинуть” (“dodge”) два барплота, чтобы один не загораживал другой. Как и в случае со статистическими трансформациями, в большинстве случаев значение по умолчанию — “identity.” Кроме слоев, у графика есть: Координатная система (coord). Если мы задали координаты, то нам нужно задать и координатную плоскость, верно? Конечно, в большинстве случаев используется декартова система координат (Cartesian coordinate system),30 т.е. стандартная прямоугольная система координат, но можно использовать и другие, например, полярную систему координат или картографическую проекцию. Шкалы (scales). Шкалы задают то, как именно значения превращаются в эстетики. Например, если мы задали, что разные значения в колонке будут влиять на цвет точки, то какая именно палитра будет использоваться? В какие конкретно цвета будут превращаться числовые, логические или строковые значения в колонке? В ggplot2 есть правила по умолчанию для всех эстестик, и они отличные, но самостоятельная настройка шкал может значительно улучшить график. Фасетки (facets). Фасетки — это одно из нововведений Уикхэма в грамматику графики. Фасетки повзоляют разбить график на множество похожих, задав переменную, по которой график будет разделен. Это очень напоминает использование группировки с помощью group_by(). Тема (theme). Тема — это зрительное оформление “подложки” графика, не относящийся к содержанию графика: размер шрифта, цвет фона, размер и цвет линий на фоне и т.д. и т.п. В ggplot2 есть несколько встроенных тем, а также есть множество пакетов, которые добавляют дополнительные темы. Кроме того, их можно настраивать самостоятельно! Значения по умолчанию (defaults). Если в графике используется несколько слоев, то часто все они используют одни и те же данные и эстетики. Можно задать данные и эстетики по умолчанию для всего графика, чтобы не повторять код. 11.10 Пример №0: пайчарт с распределение по полу Сейчас мы попробуем сделать простой пример в ggplot2, похожий на пример, который использует в своей книге Леланд Уилкинсон, чтобы показать мощь грамматики графики (Wilkinson 2005). Приготовьтесь, этот пример перевернет ваши представления о графиках! Но сначала взглянем на структуру кода в ggplot(). Как видно, код чем-то напоминает стандартный код tidyverse, но с + вместо пайпов. Когда был написан ggplot2, Хэдли Уикхэм еще не знал про %&gt;% из magrittr, хотя по смыслу + означает примерно то же самое. library(tidyverse) heroes &lt;- read_csv(&quot;data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## name = col_character(), ## Gender = col_character(), ## `Eye color` = col_character(), ## Race = col_character(), ## `Hair color` = col_character(), ## Height = col_double(), ## Publisher = col_character(), ## `Skin color` = col_character(), ## Alignment = col_character(), ## Weight = col_double() ## ) Итак, запустим функцию ggplot(), задав наш тиббл heroes в качестве данных. ggplot(data = heroes) Мы ничего не получили! Это естественно, ведь мы задали только данные по умолчанию, но не задали геометрию и эстетики. Функция ggplot() не просто отрисовывает график, эта функция создает объект класса ggplot, который можно сохранить и модифицировать в дальнейшем: almost_empty_ggplot &lt;- ggplot(data = heroes) almost_empty_ggplot Возьмем geom_bar() для отрисовки барплота. В качестве эстетик поставим x = Gender и fill = Gender. Поскольку это эстетики, они обозначаются внутри функции параметра mapping = aes() или просто внутри функции aes(). По умолчанию, geom_bar() имеет статистику “count,” что нас полностью устраивает: geom_bar() сам посчитает табличу частот и использует значения Gender для обозначения позиций и заливки, а посчитанные частоты будет использовать для задания высоты столбцов. ggplot(data = heroes) + geom_bar(aes(x = Gender, fill = Gender)) Сейчас мы сделаем один хитрый трюк: поставим значение эстетики x = \"\", чтобы собрать все столбики в один. ggplot(data = heroes) + geom_bar(aes(x = &quot;&quot;, fill = Gender)) Получилось что-то не очень симпатичное, но вполне осмысленное: доли столбца обозначают относительную частоту. Можно настроить общие параметры геома, не зависящие от данных. Это нужно делать вне функции aes(), но внутри функции для геома. ggplot(data = heroes) + geom_bar(aes(x = &quot;&quot;, fill = Gender), width = .2) Казалось бы, причем здесь Minecraft? А теперь внимание! Подумайте, какого действия нам не хватает, чтобы из имеющегося графика получить пайчарт? ggplot(data = heroes) + geom_bar(aes(x = &quot;&quot;, fill = Gender)) + coord_polar(theta = &quot;y&quot;) Нам нужно было всего-лишь поменять систему координат с декартовой на полярную (круговую)! Иначе говоря, пайчарт - это барплот в полярной системе координат. Именно в этом основная сила грамматики графики и ее реализации в ggplot2 — вместо того, чтобы описывать и рисовать огромное количество типов графиков, можно описать практически любой график через небольшой количество элементарных элементов и правила их соединения. Получившийся пайчарт осталось подретушировать, убрав все лишние элементы подложки с помощью самой минималистичной темы theme_void() и добавив название графика: ggplot(data = heroes) + geom_bar(aes(x = &quot;&quot;, fill = Gender)) + coord_polar(theta = &quot;y&quot;) + theme_void() + labs(title = &quot;Gender distributions for superheroes&quot;) Это был интересный, но немного шуточный пример. Все-таки пайчарт — это довольно спорный способ визуализировать данные, вызывающий много вполне справедливой критики. Поэтому сейчас мы перейдем к гораздо более реалистичному примеру. 11.11 Пример №1: Education and IQ meta-analysis Для этого примера мы возьмем мета-анализ связи количества лет обучения и интеллекта: “How Much Does Education Improve Intelligence? A Meta-Analysis” (Ritchie and Tucker-Drob 2018). Мета-анализ — это группа статистических методов, которые позволяют объединить результаты нескольких исследований с похожим планом исследованием и тематикой, чтобы посчитать средний эффект между несколькими статьями сразу. Данные и скрипт для анализа данных в этой статье находятся в открытом доступе: https://osf.io/r8a24/ Полный текст статьи доступен по ссылке. Существует положительная корреляция между количеством лет, который человек потратил на обучение, и интеллектом. Это может объясняться по-разному: как то, что обучение повышает интеллект, и как то, что люди с высоким интеллекте стремятся получать больше образования. Напрямую в эксперименте это проверить нельзя, поэтому есть несколько квази-экспериментальных планов, которые косвенно указывают на верность той или иной гипотезу. Например, если в стране изменилось количество лет обязательного школьного образования, то повлияло ли это на интеллект целого поколения? Или все-таки дело в Моргенштерне Данная картинка показывает, насколько размер эффекта (выраженный в баллах IQ) зависит от того, какой средний возраст участвоваших в исследовании испытуемых. Каждая точка на этом графике — это отдельное исследование, положение по оси x — средний возраст респондентов, а положение по оси y - средний прирост интеллекта согласно исследованию. Размер точки отражает “точность” исследования (грубо говоря, чем больше выборка, тем больше точка). Два графика обозначают два квазиэкспериментальных плана. Мы сфокусируемся на нижней картинке с “Policy change” — это как раз исследования, в которых изучается изменения интеллекта в возрастных группах после изменения количества лет обучения в школе. Мы полностью воспроизведем код library(tidyverse) Заметьте, данный датасет использует немного непривычный для нас формат хранения данных. Попытайтесь самостоятельно прочитать его. df &lt;- read_tsv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/meta_dataset.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Study_ID = col_double(), ## Data_ID = col_double(), ## k = col_double(), ## Country = col_character(), ## n = col_double(), ## Design = col_character(), ## outcome_test_cat = col_double(), ## Effect_size = col_double(), ## SE = col_double(), ## Outcome_age = col_double(), ## quasi_age = col_double(), ## cpiq_early_age = col_double(), ## cpiq_age_diff = col_double(), ## ses_funnel = col_double(), ## published = col_double(), ## Male_only = col_double(), ## Achievement = col_double() ## ) Давайте посмотрим, как устроен датафрейм df: df Каждая строчка — это результат отдельного исследования, при этом одна статья может включать несколько исследований, В дальнейшем мы будем использовать код авторов статьи и смотреть, строчка за строчкой, как он будет работать. cpiq &lt;- subset(df, subset=(Design==&quot;Control Prior IQ&quot;)) poli &lt;- subset(df, subset=(Design==&quot;Policy Change&quot;)) Авторы исследования используют subset(), это функция базового R, принцип которой очень похож на filter().31 Итак, начнем рисовать сам график. Сначала иницируем объект ggplot с данными poli по умолчанию. ggplot(data=poli) Теперь добавим в качестве эстетик по умолчанию координаты: aes(x=Outcome_age, y=Effect_size). ggplot(aes(x=Outcome_age, y=Effect_size), data=poli) Что изменилось? Появилась координатная ось и шкалы. Заметьте, масштаб неслучаен: он строится на основе разброса значений в выбранных колонках. Однако этого недостаточно для отрисовки графика, нехватает геометрии: нужно задать, в какую географическую сущность отобразятся данные. ggplot(aes(x=Outcome_age, y=Effect_size), data=poli) + geom_point() Готово! Это и есть основа картинки. Добавляем размер: ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point() Перед нами возникла проблема оверплоттинга: некоторые точки перекрывают друг друга, поскольку имеют очень близкие координат. Авторы графика решают эту проблему очевидным способом: добавляют прозрачности точкам. Заметьте, прозрачность задается для всех точек одним значением, поэтому параметр alpha задается вне функции aes(). ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55) Совершенно так же задается и цвет. Он задается одинаковым для всех точек с помощью HEX-кода. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) Теперь добавим регрессионную прямую с доверительными интервалами на график. Это специальный геом geom_smooth() со специальной статистикой, который займет второй слой данного графика. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; По умолчанию geom_smooth() строит кривую линию. Поставим method = \"lm\" для прямой. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Теперь нужно поменять цвет: ярко синий цвет, используемый по умолчанию здесь попросту мешает восприятию графика. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Авторы графика перекрашивают серую полупрозначную область тоже. В этом случае используется параметр fill =, а не colour =, но цвет используется тот же. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Регрессионную линию авторы немного утоньшают с помощью параметра size =. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5) ## `geom_smooth()` using formula &#39;y ~ x&#39; Чтобы сместить фокус в сторону точек, авторы добавляют прозрачности для всего geom_smooth(). ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) ## `geom_smooth()` using formula &#39;y ~ x&#39; На шкале присутствует 0, и по умолчанию он никак не обозначен. Это легко исправить с помощью вспомогательного геома geom_hline(). ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) ## `geom_smooth()` using formula &#39;y ~ x&#39; Оттенить эту линию можно, сделав ее пунктирной. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) ## `geom_smooth()` using formula &#39;y ~ x&#39; Авторы графика вручную задают деления шкалы по оси x. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) ## `geom_smooth()` using formula &#39;y ~ x&#39; С помощью функции guides() убирают легенду с картинки. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) ## `geom_smooth()` using formula &#39;y ~ x&#39; Следующим этапом авторы добавляют подписи шкал и название картинки. Обратите внимание на \\n внутри подписи к оси y, которая задает перенос на следующую строку. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Policy Change&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Теперь пришло время сделать график более красивым и понятным с помощью изменения подложки, т.е. работы с темой графика. Здесь тема задается сначала как theme_bw() — встроенная в ggplot2 минималистичная тема, а потом через функцию theme(), через которую можно управлять конкретными элементами темы. Здесь это сделано, чтобы передвинуть название графика к центру. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=poli) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Policy Change&quot;)+ theme(plot.title = element_text(hjust=0.5)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Готово! Мы полностью воспроизвели график авторов статьи с помощью их открытого кода. Если вы помните, то в изначальном графике было две картинки. Авторы делают их отдельно, с помощью почти идентичного кода. Нечто похожее можно сделать по-другому, применяя фасетки. Для этого мы возьмем неотфильтрованный датасет df, а с помощью колонки Design, на основании которой разделялся датасет для графиков, произведем разделение графиков внутри самого ggplot объекта. Для этого нам понадобится функция facet_wrap(), в которой с помощью формулы можно задать колонки, по которым будут разделены картинки по вертикали (слева от ~) и горизонтально (справа от ~). Пробуем разделить графики горизонтально: ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=df) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Policy Change&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_wrap(~Design) ## `geom_smooth()` using formula &#39;y ~ x&#39; Здесь становится очевидно, почему авторы не включали данные \"School Age Cutoff\" третьим графиком: средний возраст участников этих исследований сильно отличается. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=df %&gt;% filter(Design != &quot;School Age Cutoff&quot;)) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Policy Change&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_wrap(~Design) ## `geom_smooth()` using formula &#39;y ~ x&#39; Теперь поставим два графика друг над другом, поместив Design слева от ~ внутри facet_wrap(). Справа нужно добавить точку. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=df %&gt;% filter(Design != &quot;School Age Cutoff&quot;)) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Policy Change&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_grid(Design~.) ## `geom_smooth()` using formula &#39;y ~ x&#39; Теперь нужно изменить подписи. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2)), data=df %&gt;% filter(Design != &quot;School Age Cutoff&quot;)) + geom_point(alpha=.55, colour=&quot;#BA1825&quot;) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=F) + geom_smooth(method=&quot;lm&quot;, colour=&quot;#BA1825&quot;,fill=&quot;#BA1825&quot;,size=.5, alpha=.25) + ggtitle(&quot;Effect of education as a function of age at the outcome test&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_grid(Design~.) ## `geom_smooth()` using formula &#39;y ~ x&#39; Чтобы акцентировать графики, можно раскрасить их в разные цвета в дополнение к фасеткам. Для этого мы переносим colour = и fill = из параметров соответствующих геомов внутрь эстетик и делаем зависимыми от Design. Поскольку эти эстетики (точнее, colour =) одинаковы заданы для двух геомов (geom_point() и geom_smooth()), то мы спокойно можем вынести их в эстетики по умолчанию — в параметры aes() внутри ggplot(). При этом сразу выключим легенды для новых эстетик, потому они избыточны. ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2), colour = Design, fill = Design), data=df %&gt;% filter(Design != &quot;School Age Cutoff&quot;)) + geom_point(alpha=.55) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=FALSE, colour = FALSE, fill = FALSE) + geom_smooth(method=&quot;lm&quot;, size=.5, alpha=.25) + ggtitle(&quot;Effect of education as a function of age at the outcome test&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_grid(Design~.) ## `geom_smooth()` using formula &#39;y ~ x&#39; Слишком блеклая палитра? Не беда, можно задать палитру вручную! В ggplot2 встроены легендарные Brewer’s Color Palettes, которыми мы и воспользуемся. Функции для шкал устроены интересным образом: они состоят из трех слов, первое из которых scale_*_*(), второе — эстетика, например, scale_color_*(), а последнее слово — тип самой шкалы, в некоторых случаях - специальное название для используемой шкалы, как и в случае с scale_color_brewer(). meta_2_gg &lt;- ggplot(aes(x=Outcome_age, y=Effect_size, size=1/(SE^2), colour = Design, fill = Design), data=df %&gt;% filter(Design != &quot;School Age Cutoff&quot;)) + geom_point(alpha=.55) + geom_hline(yintercept=0, linetype=&quot;dotted&quot;) + theme_bw() + scale_x_continuous(breaks=c(20,30,40,50,60,70,80)) + xlab(&quot;Age at outcome test (years)&quot;) + ylab(&quot;Gain for 1 year of education\\n(IQ points)&quot;) + guides(size=FALSE, colour = FALSE, fill = FALSE) + geom_smooth(method=&quot;lm&quot;, size=.5, alpha=.25) + ggtitle(&quot;Effect of education as a function of age at the outcome test&quot;)+ theme(plot.title = element_text(hjust=0.5)) + facet_grid(Design~.)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ scale_fill_brewer(palette = &quot;Set1&quot;) meta_2_gg ## `geom_smooth()` using formula &#39;y ~ x&#39; 11.12 Расширения ggplot2 ggplot2 стал очень популярным пакетом и быстро обзавелся расширениями - пакетами R, которые являются надстройками над ggplot2. Эти расширения бывают самого разного рода, например, добавляющие дополнительные геомы или просто реализующие отдельные типы графиков на языке ggplot2. Я рекомендую посмотреть самостоятельно галерею расширений ggplot2: https://exts.ggplot2.tidyverse.org/gallery/ Для примера мы возьмем пакет hrbrthemes, который предоставляет дополнительные темы для ggplot2, компоненты тем и шкалы. install.packages(&quot;hrbrthemes&quot;) library(hrbrthemes) meta_2_gg + theme_ipsum_tw() ## `geom_smooth()` using formula &#39;y ~ x&#39; 11.13 Динамические визуализации в R 11.13.1 Интерфейс для JavaScript фреймворков: пакет htmlwidgets До этого мы делали только статические картинки, но в R можно делать динамические визуализации с интерактивными элементами! Делаются такие визуализации на основе JavaScript, в первую очередь, на основе фреймворка D3.js. Существует пакет для R htmlwidgets, который предоставляет интерфейс для работы с JavaScript визуализациями из R и вставлять их в RMarkdown HTML-документы и веб-приложения Shiny. htmlwidgets — это пакет, в первую очередь, для разработчиков R пакетов, которые делают на его основе очень простые и удобные в использовании R пакеты для создания динамических визуализаций и прочих динамических элементов. 11.13.2 Динамические визуализации в plotly Один из самых распространенных средств для динамических визуализаций — это пакет plotly. install.packages(&quot;plotly&quot;) library(plotly) ## ## Присоединяю пакет: &#39;plotly&#39; ## Следующий объект скрыт от &#39;package:ggplot2&#39;: ## ## last_plot ## Следующий объект скрыт от &#39;package:stats&#39;: ## ## filter ## Следующий объект скрыт от &#39;package:graphics&#39;: ## ## layout Есть два базовых способа использовать plotly в R. Первый — это просто оборачивать готовые графики ggplot2 с помощью функции ggplotly(). ggplotly(meta_2_gg) ## `geom_smooth()` using formula &#39;y ~ x&#39; Не всегда это получается так, как хотелось бы, но простота этого способа подкупляет: теперь наведение на курсора на точки открывает небольшое окошко с дополнительной информацией о точке (конечно, если вы читаете эту книгу в PDF или ePUB, то этого не увидите). Другой способ создания графиков — создание вручную с помощью plot_ly(). Такой способ частично напоминает ggplot2 использованием пайпов (обычных %&gt;%, а не +), задание эстетик здесь происходит с помощью ~. plot_ly(poli, x = ~Outcome_age, y = ~Effect_size, size = ~1/(SE^2), color = ~Effect_size, sizes = c(40, 400), text = ~paste(&quot;N: &quot;, n, &#39;&lt;br&gt;Country:&#39;, Country)) %&gt;% add_markers() ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: `line.width` does not currently support multiple values. 11.13.3 Другие пакеты для динамической визуализации Кроме plotly есть и множество других HTML-виджетов для динамической визуализации. Я рекомендую посмотреть их самостоятельно на http://gallery.htmlwidgets.org/ Выделю некоторые из них: echarts4r — один из основных конкурентов для plotly. Симпатичный, работает довольно плавно, синтаксис тоже пытается вписаться в логику tidyverse. leaflet — основной (но не единственный!) пакет для работы с картами. Leaflet — это очень популярная библиотека JavaScript, используемая во многих веб-приложениях, а пакет leaflet - это довольно понятный интерфейс к ней с широкими возможностями. networkD3 — пакет для интерактивной визуализации сетей. Подходит для небольших сетей. Квартиль — это частный пример квантиля. Другой известный квантиль — процентиль. Процентили часто используют для сравнения значения с другими значениями. Например, 63ий процентиль означает, что данное значение больше 63% значений в выборке.↩︎ идентична по своему смыслу функции dplyr::count, которая считает частоты по выбранной колонке тиббла @ref(tidy_count)↩︎ Декартова система координат названа в честь великого математика и философа Рене Декарта, на латинском — Renatus Cartesius, отсюда и название cartesian coordinate system.↩︎ Кстати, именно функция subset() вдохновила Уикхема на создание filter().↩︎ "],["rmd.html", "12 R Markdown 12.1 Что такое R Markdown 12.2 Начало работы в R Markdown 12.3 Структура R Markdown документа 12.4 Настройки чанка 12.5 Синтаксис Markdown (без R) 12.6 Дополнительные возможности R Markdown", " 12 R Markdown 12.1 Что такое R Markdown После подсчета описательных статитистик, создания графиков и, в особенности, интерактивных визуализаций, возникает вопрос о том, как представить полученные результаты. R Markdown представляет такую возможность. С помощью R Markdown в документе можно совмещать код, результаты его исполнения и написанный текст. Кроме того, можно вставлять картинки, ссылки, видео и многое другое. В чем-то R Markdown напоминает Jupyter Notebook знакомый всем питоноводом, но это сходство, скорее, функциональное (и то, и то позволяет превращать сухой текст скрипта в красивый документ), их устройство значительно различается. R Markdown представляет собой текстовый документ специального формата .Rmd, который можно скомпилировать в самые различные документы: Документы в форматах Word, ODT, RTF, PDF (с использованием LaTeX), HTML, в том числе: Онлайн-книги (bookdown) Научные статьи (papaja) Презентации в виде HTML (ioslides, Slidy, revealjs, rmdshower, Beamer) Веб-сайты (blogdown) Дашборды (flexdashboard) Формат вывода легко настроить и поменять по ходу работы, что позволяет гибко изменять формат документа на выходе. 12.2 Начало работы в R Markdown Для работы с R Markdown у RStudio есть специальные инструменты, которые позволяют не только удобно писать и компилировать R Markdown документы, но и превращают R Markdown в удобную среду для работы с R вместо обычных R-скриптов. Чтобы начать работать с R Markdown, нужно создать новый .Rmd файл с помощью File - New File - R Markdown... Перед вами появится меню выбора формата R Markdown документа, названия и имени автора. Меню выбора формата R Markdown документа Выбирайте что угодно, все это можно потом изменить вручную. Если пакет rmarkdown у вас еще не установлен, то он будет автоматически установлен. Кроме того, если вы выбрали в качестве формата PDF (презентацию или документ), то вам понадобится еще установить LaTeX на компьютер. Это тоже можно сделать с помощью специального пакета: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # install TinyTeX .Rmd файл, который вы создадите таким образом, будет создан из шаблона, который демонстрирует основной функционал R Markdown. В отличие от работы с R скриптом, перед вами будет немного другой набор кнопок. Самая важная из новых кнопок — это кнопка Knit (с клубком и спицей рядом), нажав на которую, начнется “вязание” (knitting) финального документа, то есть его компиляция. Если компиляция завершится успешно, то перед вами появится скомпилированный документ в том формате, который вы выбрали. 12.3 Структура R Markdown документа R Markdown документ состоит из трех базовых элементов: YAML-шапки32 Текста с использованием разметки Markdown Чанков (chunks) с кодом Разберем их по порядку. YAML-шапка находится в самом верху документа и отделена тремя дефисами (---) сверху и снизу. В нем содержится, во-первых, мета-информация о документе, которая будет отображена на титульном листе/слайде, во-вторых, информация о формате документа, который будет “связан.” Пример YAML-шапки: --- title: &quot;Классное название для документа&quot; author: &quot;Поздняков Иван&quot; date: &quot;15 11 2020&quot; output: html_document --- Текст с использование синтаксиса Markdown идет сразу после YAML-шапки и составляет основную часть .Rmd документа. Markdown (не путать с R Markdown!) — это популярный и очень удобный язык разметки. Markdown используется повсюду: в ReadMe страницах на GitHub, как способ ведения записей во многих программах для заметок и даже в Telegram! Например, вот так можно задавать полужирный шрифт и курсив: Вот так мы делаем **полужирный**, а вот так мы делаем *курсив.* В результате мы получим следующую строчку: Вот так мы делаем полужирный, а вот так мы делаем курсив. Далее мы разберем подробнее синтаксис Markdown. Чанки с кодом содержат в себе код на языке R или другом языке программирования, которые будут исполнены, а результат которых будет отображен прямо под чанком с кодом. Чанк с кодом отделяется ``` с обоих сторон и содержит {r}. Это означает, что внутри находится код на R, который должен быть выполнен: ```{r} 2+2 ``` В итоговом документе чанк будет выглядеть так: 2+2 ## [1] 4 12.4 Настройки чанка У чанка с кодом есть набор настроек. Самый важные из них такие: echo: будет ли показан сам код message и warning: будут ли показаны сообщения и предупреждения, всплывающие во время исполнения кода eval: будет ли испольняться код внутри чанка 12.4.1 Настройка нескольких чанков Все эти настройки можно настроить как для отдельных чанков, так и для все чанков сразу: knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) Этот чанк нужно вставлять в начале .Rmd документа, тогда выбранные настройки повлияют на все последующие чанки. 12.4.2 Чанки с Python и другими языками программирования Можно вставлять чанки с кодом на других языках программирования! Для этого вместо {r} нужно написать {python}. x = &#39;hello, python !&#39; print (x.split(&quot; &quot;)) ## [&#39;hello,&#39;, &#39;python&#39;, &#39;!&#39;] Вот полный список поддерживаемых языков: names(knitr::knit_engines$get()) ## [1] &quot;awk&quot; &quot;bash&quot; &quot;coffee&quot; &quot;gawk&quot; &quot;groovy&quot; ## [6] &quot;haskell&quot; &quot;lein&quot; &quot;mysql&quot; &quot;node&quot; &quot;octave&quot; ## [11] &quot;perl&quot; &quot;psql&quot; &quot;Rscript&quot; &quot;ruby&quot; &quot;sas&quot; ## [16] &quot;scala&quot; &quot;sed&quot; &quot;sh&quot; &quot;stata&quot; &quot;zsh&quot; ## [21] &quot;highlight&quot; &quot;Rcpp&quot; &quot;tikz&quot; &quot;dot&quot; &quot;c&quot; ## [26] &quot;cc&quot; &quot;fortran&quot; &quot;fortran95&quot; &quot;asy&quot; &quot;cat&quot; ## [31] &quot;asis&quot; &quot;stan&quot; &quot;block&quot; &quot;block2&quot; &quot;js&quot; ## [36] &quot;css&quot; &quot;sql&quot; &quot;go&quot; &quot;python&quot; &quot;julia&quot; ## [41] &quot;sass&quot; &quot;scss&quot; &quot;theorem&quot; &quot;lemma&quot; &quot;corollary&quot; ## [46] &quot;proposition&quot; &quot;conjecture&quot; &quot;definition&quot; &quot;example&quot; &quot;exercise&quot; ## [51] &quot;proof&quot; &quot;remark&quot; &quot;solution&quot; 12.4.3 Код вне чанков (inline code) Иногда хочется вставить результат расчетов прямо в текст. Для этого нужно поставить символ ` с обоих краев команды и написать r перед самой командой. В этом случае результат выполнения этой команды будет в тексте вместо этой конструкции. Число пи равно ` r pi `: Число пи равно 3.1415927 12.5 Синтаксис Markdown (без R) В RStudio есть подсказка по синтаксису Markdown, для ее вызова нужно нажать Help - Markdown Quick Reference 12.5.1 Выделение текста Выделение текста происходит с помощью обособления текста специальными символами: *Курсив* _Тоже курсив_ **Полужирный** __Тоже полужирный__ ~~перечеркнутый~~ индекс^надстрочный^ индекс~подстрочный~ Курсив Тоже курсив Полужирный Тоже полужирный перечеркнутый индекснадстрочный индексподстрочный 12.5.2 Заголовки разных уровней С помощью решенточек (#) выделяются заголовки разных уровней. # Самый верхний заголовок ## Заголовок второго уровня ### Мне заголовок #### И моему сыну тоже ##### И моему! ###### Все, дальше опускаться некуда 12.5.3 Списки Списки можно создавать по-разному, в зависимости от того, является ли список пронумерованным: * Первый вариант списка выглядит так: + Можно и с подсписком + Почему бы и нет? 1. Кому нужен порядок 2. Тот списки номерует Первый вариант списка выглядит так: Можно и с подсписком Почему бы и нет? Кому нужен порядок Тот списки номерует 12.5.4 Цитаты Цитаты выделяются с помощью знака &gt; в начале строки. &gt; Я устал &gt; Который год во мне живет нарвал Я устал Который год во мне живет нарвал 12.5.5 Таблицы Табличные данные имеют особое значение в R, в R Markdown им тоже уделяется особое внимание. Функция knitr::kable() превращает табличные данные (матрицы, датафреймы) в текст, отформатированный как Markdown-таблицы. Таким образом, вот такая таблица: knitr::kable(heroes[1:3, 1:4]) Превращается вот в такую, отформатированную с помощью символов -, | и т.п.: | X1|name |Gender |Eye color | |--:|:----------|:------|:---------| | 0|A-Bomb |Male |yellow | | 1|Abe Sapien |Male |blue | | 2|Abin Sur |Male |blue | А эта таблица, в свою очередь, превращается в такую в финальном документе: X1 name Gender Eye color 0 A-Bomb Male yellow 1 Abe Sapien Male blue 2 Abin Sur Male blue Если вам нужно самостоятельно отформатировать таблицу в Markdown, то для этого есть специальный ресурс. Пакет knitr является ключевым для R Markdown, поэтому он устанавливается вместе с rmarkdown. А вот для дополнительной настройки вывода таблиц рекомендуется пакет kableExtra. 12.6 Дополнительные возможности R Markdown 12.6.1 Динамические таблицы Один из самых интересных HTML-виджетов (??) — пакет DT для создания интерактивных таблиц прямо внутри HTML-документа. library(tidyverse) heroes &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) DT::datatable(heroes) 12.6.2 Графики в R Markdown Все создаваемые графики будут появляться под чанком с кодом. height_weight_gg &lt;- heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point(aes(colour = Gender), alpha = 0.5) + coord_fixed() + facet_wrap(~Publisher)+ theme_minimal() height_weight_gg Это так же относится и к динамическим визуализациям с помощью HTML-виджетов (??), например, plotly. library(plotly) ggplotly(height_weight_gg) Конечно, чтобы эта интерактивность сохранилась, используемый формат итогового документа должен ее поддерживать. Word-документы, так же как и PDF-документы, — статичны, поэтому единственный вариант сохранить интерактивные элементы — это использование HTML-документов или HTML-презентаций. 12.6.3 HTML-код Если вы выбрали HTML форматом итогового документа, то можете использовать все его фишки, включая форматирование с помощью HTML-тегов (в дополнение к обычному Markdown). Еще вы можете вставлять куски HTML-кода, например, вставить видео с YouTube или отдельный пост из Twitter. &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/5qap5aO4i9A&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; YAML расшифровывается как “YAML Ain’t Markup Language”, ранее — как Yet Another Markup Language. Связано это с тем, что сначала этот язык позиционировался как язык разметки (например, HTML), затем — как язык сериализации, т.е. хранения данных (как JSON).↩︎ "],["infer-stats.html", "13 Статистика вывода 13.1 Введение в статистику вывода 13.2 Оценки 13.3 Распределения 13.4 Точечные оценки 13.5 Интервальные оценки 13.6 Выборочное распределение 13.7 Важность нормального распределения 13.8 Центральная предельная теорема 13.9 Строим доверительный интервал 13.10 Тестирование значимости нулевой гипотезы (#nhst)", " 13 Статистика вывода 13.1 Введение в статистику вывода Статистика вывода (inferential statistics) - это основной раздел статистики, связанный уже не с описанием и суммированием имеющихся данных (описательная статистика), а с попытками сделать вывод о генеральной совокупности (population) на основе имеющихся данных по выборке. Короче говоря, статистика вывода - это о том, как выйти за пределы наших данных. Это именно то, зачем мы проводим исследования - мы не можем собрать информацию обо всей генеральной совокупности, но по тому, что имеем (т.е. по нашей выборке), можем попытаться как-то оценить параметры генеральной совокупности. Итак, еще раз: генеральной совокупности - параметры (Population - Parameters), а у выборки - статистики (Sample - Statistics). Параметры обычно обозначаются греческими буквами: \\(\\mu\\), \\(\\sigma\\) (или большими латинскими: \\(M\\), \\(S\\) ), а статистики - соответствующими латинскими: \\(m\\), \\(s\\). 13.2 Оценки Самая основа статистики вывода - это оценки. Как мы уже знаем, у распределений есть определенные параметры, которые описывают данное распределение. Возьмем для примера нормальное распределение. \\[P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\] Например, для того, чтобы описать нормальное распределение нам нужно всего два параметра - его среднее \\(\\mu\\) и стандартное отклонение \\(\\sigma\\). Если же \\(\\mu = 0\\), а \\(\\sigma = 1\\), то такое нормальное распределение называется стандартным. Наша цель - как-нибудь оценить эти параметры, потому что обычно мы их не знаем. Какой средний рост в популяции? Какое у него стандартное отклонение? Для этого мы используем разного рода оценки: точечные и интервальные. В качестве оценок параметров часто используются статистики по выборке. Вот здесь легко запутаться, поэтому покажу картинку: Давайте познакомимся поближе с распределениями. 13.3 Распределения Распределения разделяются на дискретные и непрерывные. По большей части нас будут интересовать именно непрерывные. Хотя для полного их понимания нужен матан (да, именно тот, который calculus), непрерывные распределения довольно интуитивно понятны. Для каждого имеющегося в R распределения есть свой набор из четырех функций. Скоро мы разберем их все. Возьмем, например, нормальное распределение, про которое мы уже немного говорили (функции rnorm(), dnorm(), pnorm(), qnorm()). То, что Вы видите на картинке - это так называемая функция плотности вероятности (probability density function). Это аналог probability mass function для дискретных величин - распределение вероятности того, что случайная величина имеет данное значение. Довольно очевидно, да? Тогда почему аналогичная штука для непрерывных распределений называется, черт возьми, по-другому? Дело в том, что вероятность получения конкретного значения для непрерывных величин равна нулю. Это немного необычный момент, но тем не менее: если распределение непрерывно, то получить точно 0,000000000000 с бесконечным количеством нулей после запятой невозможно, даже если среднее распределения равно нулю. Тем не менее, мы можем посчитать вероятность того, что случайное значение окажется в определенном промежутке. Чтобы посчитать эту вероятность, надо посчитать площадь соответствующего участка. Соответсвенно, площадь под всей функцией плотности вероятности равна 1. Конечно, функцию плотности можно получить в R - это функции d*(), где * означает соответствующее распределение. Например, функция dnorm(). Функция dnorm() имеет следующие основные параметры: x = - вектор принимаемых значений, среднее и стандартное отклонение (mean = и sd =). В качестве параметров по умолчанию используются 0 для среднего и 1 для стандартного отклонения, то есть стандартное нормальное распределение. Давайте посмотрим, как работает эта функция, визуализировав результат ее выполнения на вектор от -3 до 3 с небольшим шагом. vec &lt;- seq(-3,3, 0.1) plot(vec,dnorm(vec)) Вот мы и получили то, что называем стандартным нормальным распределением, точнее, функцию плотности веротности стандартного нормального распределения. Давайте теперь возьмем среднее 100 и стандартное отклонение 15 — нормы для шкалы IQ: iq &lt;- seq(50,150, 0.1) plot(iq, dnorm(iq, mean = 100, sd = 15)) Шкала IQ — это вообще очень удобная шкала. Поскольку в измерениях интеллекта нет никаких объективных метрик (сантиметров, градусов, килограммов), с которыми его можно было бы сравнить, то единственная возможность дать какую-то оценку величины интеллекта у человека — сравнить его с интеллектом другого человека, например, по количеству решенных заданий за определенное время. Поэтому шкала IQ так сконструирована, чтобы среднее значение количество решенных заданий обозначалось за 100, а стандартное отклонение за 15. Поэтому по баллу IQ (если используется хороший тест, разумеется) можно понять процент людей, которых респондент опережает по интеллекту, по крайней мере, сравнивая с выборкой, на которой был стандартизирован тест. Примерно так же устроены и другие психологические шкалы. Следующая функция это функция накопленной плотности распределения (cumulative distribution function; cdf).33 Это функция очень важная, потому что именно на ней основано тестирование уровня значимости нулевой гипотезы, которым мы будем заниматься в дальнейшем. Она означает вероятность того, что полученное случайное значение из распределения будет меньше искомого или равно ему. Для этого используется функция pnorm(). plot(iq, pnorm(iq, mean = 100, sd = 15)) Какова вероятность того, что полученное случайное значение IQ будет меньше или равно 100? pnorm(100, mean = 100, sd = 15) ## [1] 0.5 А 130? pnorm(130, mean = 100, sd = 15) ## [1] 0.9772499 Следующая функция — это квантильная функция (quantile function), или обратная функция накопленной плотности распределения (inverse cumulative distribution function): prob &lt;- seq(0,1, 0.01) plot(prob, qnorm(prob, mean = 100, sd = 15)) Обратная функция означает, что если мы применим сначала одну, а потом другую, то (если не берем особо крайних значений) на выходе получим исходные числа.34 qnorm(pnorm(-4:4)) ## [1] -4 -3 -2 -1 0 1 2 3 4 Ну и последняя важная функция - это rnorm() - просто генерирует выборку значений из данного распределения заданной длины n =: set.seed(42) samp &lt;- rnorm(100, mean = 100, sd = 15) samp ## [1] 120.56438 91.52953 105.44693 109.49294 106.06402 98.40813 122.67283 ## [8] 98.58011 130.27636 99.05929 119.57304 134.29968 79.16709 95.81817 ## [15] 98.00018 109.53926 95.73621 60.15317 63.39300 119.80170 95.40042 ## [22] 73.28037 97.42124 118.22012 128.42790 93.54296 96.14096 73.55255 ## [29] 106.90146 90.40008 106.83175 110.57256 115.52655 90.86610 107.57433 ## [36] 74.24487 88.23311 87.23639 63.78689 100.54184 103.08998 94.58414 ## [43] 111.37245 89.09943 79.47578 106.49227 87.82910 121.66152 93.52831 ## [50] 109.83472 104.82888 88.24242 123.63591 109.64349 101.34641 104.14826 ## [57] 110.18933 101.34749 55.10365 104.27324 94.49148 102.77846 108.72736 ## [64] 120.99605 89.09062 119.53814 105.03772 115.57759 113.81093 110.81317 ## [71] 84.35322 98.64720 109.35277 85.69715 91.85757 108.71495 111.52268 ## [78] 106.95651 86.71336 83.50329 122.69061 103.86882 101.32660 98.18655 ## [85] 82.08507 109.17995 96.74290 97.25865 114.00019 112.32660 120.88175 ## [92] 92.85739 109.75523 120.86666 83.33817 87.08811 83.02392 78.11179 ## [99] 101.19974 109.79807 set.seed() - это функция, которая позволяет получить нам воспроизводимые результаты при использовании генератора случайных чисел. Короче говоря, если все мы поставим set.seed(42),35 то одна и та же строчка выдаст нам один и тот же результат на разных компьютерах. Как это вообще возможно, это же случайные числа? Дело в том, что… нет. Они “псевдо-случайные.” На самом деле, используются определенные алгоритмы, чтобы генерировать числа, которые выглядят как случайные. Например, можно брать цифры после запятой в числе пи после, например, 42го знака. Реальные алгоритмы создания псевдо-случайных чисел, конечно, гораздо сложнее, но суть примерно такая. Насколько подобные числа действительно получаются случайными - отдельный сложный математический вопрос. Но для нас этого вполне достаточно. hist(samp, breaks = 10) 13.4 Точечные оценки Представим, что только что сгенерированные данные с помощью rnorm() - это и есть наша выборка. Перед нами стоит задача оценить IQ популяции по этой выборке. Ну что, давайте попробуем оценить среднее. Здесь все просто и очень очевидно - самой лучшей оценкой среднего в генеральной совокупности будет среднее по выборке. mean(samp) ## [1] 100.4877 Конечно, не совсем точно, но лучше оценки не придумаешь. Что есть, то есть. Чем больше выборка, тем ближе мы будем к популяционному среднему. А что с оценкой стандартного отклонения? Давайте воспользуемся нашей предыдущей формулой. \\[s= \\sqrt\\frac{\\sum\\limits_{i=1}^{n} (x_{i} - \\overline{x})^2} {n}\\] sqrt(sum((samp - mean(samp))^2)/length(samp)) ## [1] 15.54206 В данном случае мы несколько промахнулись вверх, но обычно оценка по этой формуле дает небольшое смещение (bias) в меньшую сторону. Само по себе это кажется странным, и это нормально. Но этому есть всякие серьезные математические доказательства. Кроме того, есть множество всяких демонстраций, например, от Академии Хана. Одно из простых объяснений такое: поскольку мы оцениваем стандартное отклонение на основе оценки среднего, среднеквадратичные расстояния счиаются не от реального среднего в генеральной совокупности, а от среднего по выборке, которое немного смещено в ту или иную сторону. Представьте, что так получилось, что в нашей выборке оно оказалось сильно смещено и выборочное среднее получилось около 90 (такое бывает). Это значит, что получилось много довольно низких значений около этого 90, а высчитываться среднеквадратичные разницы будут не от среднего 100 (которое мы не знаем), а от этого 90. Поэтому стандартное отклонение получится сильно меньше, чем должно было получиться. Короче говоря, чтобы получить несмещенную (unbiased) оценку стандартного отклонения или дисперсии, то нам нужно делить не на \\(n\\), а на \\(n-1\\). \\[s= \\sqrt\\frac{\\sum\\limits_{i=1}^{n} (x_{i} - \\overline{x})^2} {n-1}\\] Заметьте, что именно так делает дефолтная функция sd(): sqrt(sum((samp - mean(samp))^2)/(length(samp) - 1)) ## [1] 15.62035 sd(samp) ## [1] 15.62035 Это называется поправкой Бесселя, и она настолько распространена, что именно скорректированное стандартное отклонение обычно называют стандартным отклонением. Поэтому в дальнейшем будет использоваться именно эта формула или же просто функция sd(). Таким образом, у оценки есть два критерия качества - ее точность и ее несмещенность. Тем не менее, точечные оценки - это еще не все. Мы еще хотим оценить, в каком интервале находится тот или иной параметр генеральной совокупности. 13.5 Интервальные оценки Самая распространенная интервальная оценка называется доверительным интервалом (confidence interval). Цель доверительного интервала - “покрыть” параметр генеральной совокупности с определенной степенью уверенности. Например, 95% доверительный интервал или просто \\(CI95\\%\\) означает, что примерно в 95% случаев подсчитанный на выборках интервал будет ловить значение параметра в популяции. Например, построив \\(CI95\\%\\) по нашей сгенерированной выборке, мы хотим чтобы примерно в 95% случаев выборок, построенных таким способом, этот интервал ловил истинное среднее (в данном случае — 100). Для того, чтобы научиться строить такие интервалы, нам нужно разобраться с выборочным распределение (sampling distribution). 13.6 Выборочное распределение Представьте себе, что мы бы выбрали не одну, а сразу много выборок, а потом у каждой выборки посчитали бы среднее. Мы бы получили новый вектор данных - средние выборок из одного распределения. Давайте это сделаем. v &lt;- rep(100, 1000) samplemeans &lt;- sapply(v, function(x) mean(rnorm(x, mean = 100, sd = 15))) Каждая выборка состоит из сотни “испытуемых,” всего таких выборок 1000. По каждой мы посчитали среднее. Как распределены эти средние? hist(samplemeans, breaks = 30) Вот это распределение и есть выборочное распределение средних. Точнее, было бы, если бы мы взяли не 1000 выборок, а бесконечное количество выборок. Так что у нас всего лишь апроксимация. При этом мы могли взять другие статистики, не средние выборок, а, например, медианы или стандартные отклонения выборок. Тогда это были бы выборочные распределения медиан и выборочные распределения стандартных отклонений. Но именно выборочное распределение средних обладает уникальными математическими свойствами, про которые мы скоро узнаем. Среднее выборочного распределения средних будет близко к популяционному среднему: mean(samplemeans) ## [1] 99.93772 А вот чему будет равно стандартное отклонение средних? sd(samplemeans) ## [1] 1.501392 Очень похоже на стандартное отклонение IQ, но в 10 раз меньше. В действительности, стандартное отклонение выборочного распределения средних равно стандартному отклонению в генеральной совокупности, деленному на корень из размера выборки. \\[\\sigma_{\\overline{x}}= \\frac{\\sigma} {\\sqrt{n}}\\] Как раз это мы и получили: размер нашей выборки - 100, а корень из 100 равен 10. Стандартное отклонение выборочного распределения средних (ох, йо) называется еще стандартной ошибкой или standard error of the mean (s.e.m.). Именно стандартную ошибку обычно используют на графиках в качестве error bars. Теперь вы знаете как ее посчитать! sem &lt;- 15/sqrt(length(samp)) sem ## [1] 1.5 Ну а если мы не знаем стандартного отклонения в генеральной совокупности (что обычно и бывает в жизни), то можем оценить стандартную ошибку среднего с помощью оценки стандартного отклонения, посчитанного на выборке: \\[s_{\\overline{x}}= \\frac{s} {\\sqrt{n}}\\] sd(samp)/sqrt(length(samp)) ## [1] 1.562035 Это означает, что стандартная ошибка тем меньше, чем больше выборка. И это довольно логично: чем больше у нас размер выборок, тем меньше будет расброс их средних. 13.7 Важность нормального распределения В математике все числа равны, но некоторые все-таки заметно равнее других. Например, есть 0 и 1 - и очевидно, что это очень важные числа. Даже гугл выдает больше страниц по числам 0 и 1, чем по другим числам, например, 8 и 23218974. Не только целые числа могут быть важными. Вот, например, число пи и число Эйлера: они очень часто встречаются в самых разных, подчас неожиданных местах. Например, в тех же формулах распределений. На эти числа завязано очень интересные и важные свойтсва, поэтому такая зацикленность на них неудивительна. Например, число пи связывает радиус круг с длиной его окружности и площадью, поэтому когда перед нами что-то круглое, то и пи появится с большой вероятностью. В статистике тоже есть распределения, которые “главнее” других распределений. Есть, например, равномерное распределение. Например, равномерно распределены исходы одного броска кубика. Но есть и распределение, которое смело можно называть королем всех распределений - это уже знакомое нам нормальное распределение. Нормальное распределение - это что-то в духе единицы из мира распределений. Это не просто одно из распределений, а это фундаментально важная штуковина, которая обладает почти магической силой. И сила эта зовется “Центральная Пределельная Теорема.” 13.8 Центральная предельная теорема Давайте теперь сделаем много выборок не из нормального, а из логнормального распределения. Что-то похожее представляет собой распределение времени реакции на многие задачи (особенно связанные с выбором и когда нужно подумать перед нажатием кнопки). Одна выборка будет распределена примерно так: hist(rlnorm(10000), breaks = 100) Распределение сильно ассиметрично, но его форма примерно понятна. А как будут распределены средние многих выборок, взятых из логнормального распределения? Хочется сказать, что так же, но нет! many_means &lt;- replicate(1000, mean(rlnorm(10000))) hist(many_means, breaks = 100) Удивительно, но средние по выборкам из логнормального распределения будут выглядеть почти нормально! Более того, для других распределений это тоже будет верно: согласно центральной предельной теореме (ЦПТ, central limit theorem), какой бы ни была форма распределения в генеральной совокупности, выборочное распределение средних будет стремиться к нормальному. При этом чем больше размер выборки, тем ближе выборочное распределение средних будет к нормальному. Это очень важная фишка, на которой основаны многие статистические тесты. Не верите? Попробуйте сами! Можете поиграться с разными распределениями с помощью кода (можете посмотреть другие распределения в хэлпе: ?Distributions ). Можете поиграться с интерактивной Shiny-демонстрацией магии ЦПТ. Я очень и очень рекомендую поиграться с ней. Попробуйте разные значения, посмотрите что будет. Как только наиграетесь, то сразу станет понятно, почему именно нормальное распределение занимает такое важное место в статистике. Если выходить за рамки выборочного распределения средних, то центральная предельная теорема говорит нам о том, что сумма слабо зависящих случайных величин, имеющих примерно одинаковое влияние, имеет распределение близкое к нормальному. Например, рост является следствием большого количества генетических и средовых факторов, поэтому он распределен примерно нормально. Однако это нормальное распределение портят такие факторы как пол — он вносит очень сильный вклад в рост, поэтому распределение становится немного “двугорбым.” 13.9 Строим доверительный интервал Теперь мы знаем достаточно, чтобы построить доверительный интервал своими руками на основе стандартной ошибки. Мы построим самый стандартный вариант - 95% доверительный интервал. Давайте еще раз посмотрим на нормальное распределение. Мы хотим поймать симметрично 95% от площади под кривой. Для этого нам нужно отбросить по 2.5% с обоих сторон. Эти 2.5% соответствуют примерно двум стандартным отклонениям от среднего. Если быть точнее, то 1.96. Если быть еще точнее: qnorm(0.975) ## [1] 1.959964 Почему 0.975? Потому что мы смотрим квантильную функцию по верхней границе: отсекаем правые 0.025: qnorm(1 - (1 - 0.95)/2) ## [1] 1.959964 Давайте сохраним это число. Назовем его zcr: zcr &lt;- qnorm(1 - (1 - 0.95)/2) Это количество стандартных отклонений от среднего в нормальном распределении, которое включает в себя ровно 95% площади нормального распределения. Теперь давайте посчитаем стандартную ошибку. Здесь мы знаем стандартное отклонение в генеральной совокупности (это 15), его поделим на корень из размера выборки: sem &lt;- 15/sqrt(length(samp)) Чтобы посчитать нижнюю и верхнюю границы доверительного интервала, нам нужно вычесть и прибавить соответственно нужное количество стандартных ошибок: mean(samp) - sem*zcr #нижняя граница ## [1] 97.54778 mean(samp) + sem*zcr #верхняя граница ## [1] 103.4277 Давайте теперь нарисуем сотню доверительных интервалов с помощью ggplot2! Цветом обозначим интервалы, которые не поймали истинное значение параметра в центральной совокупности. library(tidyverse) sample_size &lt;- 100 set.seed(42) ci_simulations &lt;- tibble( m = replicate(sample_size, mean(rnorm(sample_size, mean = 100, sd = 15))), se = 15/sqrt(sample_size), lower = m - se*zcr, higher = m + se*zcr, parameter_inside = lower&lt;100 &amp; higher&gt;100 ) many_ci_gg &lt;- ggplot(data = ci_simulations, aes(x = 1:sample_size,y = m)) + geom_pointrange(aes(ymin = lower,ymax = higher,colour = parameter_inside))+ geom_hline(yintercept = 100)+ coord_flip() + theme_minimal() many_ci_gg Примерно 5% не ловят 100 в 95% доверительный интервал! Примерно это и означает доверительный интервал: где-то в 95% он ловит параметр в генеральной совокупности, а в 5% - нет. Понятие доверительного интервала вызывает кучу недопонимания и ошибок. Очень многие его интерпретируют, например, как интервал, включающий в себя 95% значений популяции, но это неправильно. Еще я советую посмотреть вот эту визуализацию: Доверительные интервалы сыпятся как из мешка 13.10 Тестирование значимости нулевой гипотезы (#nhst) Тестирование значимости нулевой гипотезы (null hypothesis significance testing) — это основной подход в статистике вывода. Вы про него точно слышали, потому что де-факто он является стандартом в психологии, биологии, медицине и многих других науках. Мы сейчас детально его проведем на примере одного из самых простых статистических тестов - z-тестов. Однако та же самая логика стоит и за остальными статистическими тестами. 1. Формулирование нулевой и альтернативной гипотезы. Сначала мы задаем две гипотезы о параметрах распределения. Одна из них называется нулевой: она обычно включает положение о том, что различий или связи нет или что это различие/связь равно определенному числу. Если мы хотим применить тестирование значимости к нашей “выборке,” то нулевую гипотезу можно будет сформулировать так: \\[H_0: \\mu = 100\\]. Альтернативная или ненулевая гипотеза либо говорит о том, что среднее в генеральной совокупности на самом деле не равно какому-то конкретному числу (в нашем случае — 100) или что две выборки взяты из групп с различным средним и т.п. \\[H_1: \\mu \\ne 100\\] Тестирование нулевой гипотезы предполагает подсчет какой-то статистики, а потом вычисление того, какова вероятность получить такой или более радикальный результат при условии, что верна нулевая гипотеза. Заметьте, мы формулируем гипотезу не про статистики в выборке, а про параметры в генеральной совокупности, поэтому пользуемся греческими (или большими латинскими) буквами. Вся дальнейшая логика расчетов будет строиться именно на нулевой гипотезе: мы будем пытаться понять, насколько реалистичны наши результаты при верности нулевой гипотезы, которую мы заранее обозначили 2. Подсчет тестовой статистики по выборке Следующий этап тестирования значимости нулевой гипотезы — подсчет тестовой статистики. Тестовые статистики по своей сути не отличаются от описательных статистик, с которыми мы уже познакомились, но имеют другую функцию. Как и в случае описательных статистик, мы пытаемся выразить информацию о выборке в виде одного числа, но делаем это для того, чтобы сравнить это значение с другими возможными значениями, которые мы могли бы получить, если бы наша нулевая гипотеза была верна. Если мы знаем стандартное отклонение в генеральной совокупности, то можем посчитать z-статистику по формуле: \\[z = \\frac{\\overline{x} - \\mu} {\\sigma / \\sqrt{N}} \\] m &lt;- mean(samp) sem &lt;- 15/sqrt(length(samp)) z &lt;- (m - 100)/sem z ## [1] 0.3251482 z-статистика — это выборочное среднее, из которого вычтено среднее в генеральной совокупносности согласно нашей гипотезе. Получившуюся разницу мы делим на стандартную ошибку. 3. Расчет p-value И вот мы подобрались к самому важному этапу — расчет p-value. Очень важно понять, как именно он расчитывается, потому что на этом основывается сама идея тестирования значимости нулевой гипотезы! Для этого нам нужно вернуться к идее выборочного распределения, только теперь уже не среднего, а z-статистики. Впрочем, выглядеть оно будет абсолютно так же - нормально! Благодаря ЦПТ, если распределение в генеральной совокупности не слишком далеко от нормального, а выборка достаточно большая, то выборочное распределение z-статистик при верности нулевой гипотезы будет (примерно) нормальным со средним 0 и стандартным отклонением 1. Теперь нам нужно соотнести нашу z-статистику с теоретическим выборочным распределением z-статистик. Какова вероятность получить z-статистику 0.3251482? Это вопрос с подвохом: как мы выяснили ранее, для непрерывных распределений вероятность получить отдельное число равна 0. Но мы можем посчитать, какова вероятность получить такую же или большую z-статистику! Эту вероятность можно посчитать с помощью pnorm(): pnorm(z) ## [1] 0.6274655 pnorm() считает от минус бесконечности до заданного числа, а нам нужно наоборот — от заданного числа до плюс бесконечности. Этого можно добиться вычетанием из 1:36 1 - pnorm(z) ## [1] 0.3725345 Обычно это число еще и умножают на 2, потому что мы заранее не знаем, в какую сторону будет отклоняться среднее по нашей выборке. Вернемся к шагу 1: мы сформулировали ненулевую гипотезу таким образом, что среднее генеральной совокупности не равно 100: \\[H_1: \\mu \\ne 100\\] Это означает, что \\(H_1\\) включает в себя как случаи, когда среднее отклоняется в большую сторону, так и случаи когда среднее отклоняется в меньшую сторону. p &lt;- (1 - pnorm(z))*2 p ## [1] 0.7450689 Вот эта число и есть p-value — вероятность получения такого же и более экстремального значения тестовой статистики при условии, что нулевая гипотеза верна. 4. Принятие решения о гипотезах Отлично, мы посчитали p-value. В данном конкретном случае он оказался равен 0.7450689. Это много или мало? Фактически это означает, что если нулевая гипотеза верна, то в большинстве случаев мы будем получать z-статистики больше нашей. Короче говоря, это вполне реалистичный случай, если нулевая гипотеза верна. Значит ли это, что нулевая гипотеза верна? Нет, не значит. При тестировании значимости нулевой гипотезы мы в принципе ничего не можем сказать про верность альтернативной гипотезы. Например, возможно, настоящее среднее в генеральной совокупности, из которой мы взяли выборку, очень мало отличается от 100. Поэтому если p-value достаточно большой, мы не можем сделать выводов про верность нулевой и альтернативной гипотезы. Мы можем лишь сказать, что у нас нет оснований отклонить нулевую гипотезу. Если же p-value очень маленький, то здесь у нас появляется больше однозначности. Например, если p-value равен .02, мы можем сказать, что ситуация малореалистичная: такие и более сильные отклонения от среднего мы можем получить только раз в 50 случаев, если \\(H_0\\) верна. Поэтому мы отклоняем \\(H_0\\) и принимаем \\(H_1\\). Насколько маленьким должен быть p-value, чтобы отклонить нулевую гипотезу? Критическое значение p-value, при котором отклоняют нулевую гипотезу, называется уровнем \\(\\alpha\\). Это максимальный уровень ошибки, который мы допускаем в исследовании. Так получилось исторически, что стандартный уровень \\(\\alpha\\) равен .05. Нужно помнить, что .05 — это просто общепринятая условность, за этим числом не стоит никакого сакрального знания. Просто так получилось. Очевидно, что такой статистический подход к принятию решений будет периодически приводить к нас ошибкам. Если \\(H_0\\) на самом деле верна, а мы ее отвергли и приняли \\(H_1\\), то это ошибка первого рода (type I error). Вероятность этой ошибки и есть наше критическое значение \\(\\alpha\\). Однако есть вероятность ошибиться и в другую сторону, т.е. ошибочно не отклонить \\(H_0\\) — это ошибка второго рода (type II error), эта вероятность обозначается буквой \\(\\beta\\). Принятое решение Реальность \\(H_0\\) верна \\(H_1\\) верна Не отклоняем \\(H_0\\) Верный пропуск Ошибка 2 рода (type II error) Отклоняем \\(H_0\\) Ошибка 1 рода (type I error) Верное попадание Иногда эта функция называется интегральной функцией вероятности, а иногда просто функцией вероятности. Последний вариант может запутать, потому что часто под функцией вероятности подразумевают функцию плотности вероятности.↩︎ Если мы возьмем достаточно большие значения, то этот трюк не сработает, что свзязано с тем, что числа дробные числа в компьютерах хранятся с некоторой погрешностью.↩︎ А почему именно 42? Ну, можно брать любое число, которое Вам нравится. А 42 - это ответ на главный вопрос жизни, вселенной и всего такого↩︎ или же можно поставить параметру lower.tail = функции pnorm() значение FALSE.↩︎ "],["ttest.html", "14 t-тест 14.1 Одновыборочный t-тест 14.2 Двухвыборочный t-тест 14.3 Допущения t-теста 14.4 Непараметрические аналоги t-теста", " 14 t-тест 14.1 Одновыборочный t-тест Мы научились делать z-тест. Однако на практике он не используется, потому что предполагает, что мы откуда-то знаем стандартное отклонение в генеральной совокупности. На практике это обычно не так, поэтому мы оцениваем стандартное отклонение в генеральной совокупности на основе стандартного отклонения по выборке. Это приводит к тому, что тестовая статистика уже не распределена нормально, а распределена согласно t-распределению. Ну и статистика уже называется t-статистикой. \\[t = \\frac{\\overline{x} - \\mu} {s_x / \\sqrt{N}} \\] Иногда это t-распределение называют t-распределением Стьюдента, а соответствующий статистический тест - критерий Стьюдента. Дело в том, что его открыл сотрудник пивоварни Гиннесс Уильям Госсет. Сотрудникам Гиннесса было запрещено публиковать научные работы под своим именем, поэтому он написал свою знаменитую работу про t-распределение под псевдонимом “Ученик” (Student). Форма этого распределения очень похожа на форму нормального распределения, но имеет более тяжелые “хвосты” распределения. При этом эта форма зависит от размера выборки: чем больше выборка, тем ближе распределение к нормальному. Этот параметр распределения называется степенями свободы (degrees of freedom) и вычисляется как \\(N - 1\\), где \\(N\\) - это размер выборки. t_normal_pdf_gg &lt;- tibble(x = seq(-3, 3, .01), t_3 = dt(x, df = 3), t_10 = dt(x, df = 10), t_100 = dt(x, df = 100), normal = dnorm(x)) %&gt;% pivot_longer(cols = -x, values_to = &#39;pdf&#39;, names_to = &#39;distribution&#39;) %&gt;% ggplot(aes(x = x, y = pdf, colour = distribution))+ geom_line()+ theme_light() t_normal_pdf_gg Как видите, чем больше выборка (и количество степеней свободы соответственно), тем ближе t-распределение к стандартному нормальному распределению. При 100 степенях свободы они уже почти не различимы! Поэтому на больших выборках разница между t-тестом и z-тестом будет минимальна, тогда как на маленьких выборках разница может быть значительной. Давайте посчитаем t-статистику на тех же симулированных данных: set.seed(42) samp &lt;- rnorm(100, 100, 15) m &lt;- mean(samp) sem &lt;- sd(samp)/sqrt(length(samp)) t &lt;- (m - 100)/sem t ## [1] 0.3122351 Давайте для сравнения еще раз посчитаем z-статистику: (m - 100) / (15/sqrt(100)) ## [1] 0.3251482 Как видите, расчет довольно схожий, разница только в том, откуда мы берем стандартное отклонение. Для z-статистики у нас был заранее известный параметр генеральной совокупности (что обычно не так), для t-статистики мы оценивали стандартное отклонение по выборке. Давайте теперь посчитаем p-value. Мы будем пользоваться не функцией pnorm(), а функцией pt(), а в качестве параметра распределения указать количество степеней свобод в df = pt(t, df = length(samp) - 1) ## [1] 0.6222407 Функция pt() считает от минус бесконечности до \\(t\\), а нам нужно от \\(t\\) до плюс бесконечности, потому что \\(t\\) больше 0: 1 - pt(t, df = length(samp) - 1) ## [1] 0.3777593 И не забываем умножать на 2, если мы хотим сделать двусторонний тест. (1 - pt(t, df = length(samp) - 1))*2 ## [1] 0.7555186 В отличие от z-теста, t-тест есть в базовом R. t.test(samp, mu = 100) ## ## One Sample t-test ## ## data: samp ## t = 0.31224, df = 99, p-value = 0.7555 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 97.38831 103.58714 ## sample estimates: ## mean of x ## 100.4877 Да, конечно, мы могли сразу запустить эту функцию и получить результаты. Обычно именно так вы и будете делать. Зато теперь вы знаете, что стоит за всеми числами в результате выполнения функции t.test(). Здесь можно увидеть выборочное среднее как оценку среднего в генеральной совокупности, 95% доверительный интервал для оценки среднего в генеральной совокупности, t-статистику, степени свобод и p-value. 14.2 Двухвыборочный t-тест Одна из наиболее часто встречающихся задач при анализе данных - это сравнение средних двух выборок. Для этого нам тоже понадобится t-тест, но теперь \\(H_0\\) нужно сформулировать по-другому: что две генеральные совокупности (из которых взяты соответствующие выборки) имеют одинаковое среднее. \\[H_0: \\mu_1 = \\mu_2\\] Ну а альтернативная гипотеза, что эти две выборки взяты из распределений с разным средним в генеральной совокупности. \\[H_1: \\mu_1 \\ne \\mu_2\\] Есть две разновидности двухвыборочного t-теста: зависимый t-тест и независимый t-тест. Различие между зависимыми и независимыми тестами принципиальное, мы с ним еще будем сталкиваться. Зависимые тесты предполагают, что каждому значению в одной выборке мы можем поставить соответствующее значение из другой выборки. Обычно это повторные измерения какого-либо признака в разные моменты времени. В независимых тестах нет возможности сопоставить одно значение с другим. Мы уже не можем напрямую соотнести значения в двух выборках друг с другом, более того, размер двух выборок может быть разным! Использование зависимых и независимых тестов связано с использованием внутрииндивидуального и межиндивидуального экспериментальных дизайнов в планировании научных экспериментов. Даже если вы не планируете в дальнейшем заниматься проведением экспериментов, понимание различий между двумя видами дизайнов поможет вам понять разницу между зависимыми и независимыми тестами. Например, мы хотим исследовать влияние кофеина на скорость реакции. Можно поступить по-разному: Набрать выборку, каждому испытуемому дать либо кофеин (например, в виде раствора небольшого количества кофеина в воде), либо обычную воду. Что именно получит испытуемый получит определяется случайным образом. Испытуемый не должен знать, что ему дают (слепое тестирование), а в идеале этого должен не знать даже экспериментатор, который дает напиток и измеряет показатели (двойное слепое тестирование). Посчитать скорость выполнения выбранной задачи, отправить домой. Это межинидивидуальный экспериментальный дизайн, для анализа результатов которого нам понадобится независимый t-тест. Набрать выборку, каждому испытуемому дать и обычную воду, и воду с кофеином, записывать скорость решения задач после употребления простой воды и после употребления воды с кофеином, соответственно. В данном случае будет случайным образом варьироваться порядок предъявления: одни испытуемые сначала получат обычную воду, а потом воду с кофеином, другие испытуемые — наоборот. Для такого эксперимента понадобится меньше участников, но оно будет дольше для каждого участника. Более того, в этом случае мы учтем межиндивидуальные различия участников: одни участники в среднем решают задачи быстрее других. Это внутриинидивидуальный экспериментальный дизайн, для анализа результатов которого нам понадобится зависимый t-тест. Внутрииндивидуальный план Межиндивидуальный план Зависимый t-тест Независимый t-тест Итак, с тем, когда использовать зависимый, а когда независимый t-тест, более-менее разобрались, давайте опробуем их! 14.2.1 Двухвыборочный зависимый t-тест Двухвыборочный зависимый t-тест — это то же самое, что и одновыборочный t-тест, только для разницы между связанными значениями. Поскольку наша нулевая гипотеза звучит, что средние должны быть равны, \\[H_0: \\mu_1 = \\mu_2\\] то при верности нулевой гипотезы \\[\\mu_1 - \\mu_2 = 0\\]. Тогда вместо \\(x\\) подставим \\(d\\) — разницу (вектор разниц) между парами значений. Получаем вот что: \\[t = \\frac{\\overline{x} - \\mu} {s_x / \\sqrt{N}} = \\frac{\\overline{d} - (\\mu_1 - \\mu_2)} {s_d / \\sqrt{N}} = \\frac{\\overline{d} - 0} {s_d / \\sqrt{N}} = \\frac{\\overline{d}} {s_d / \\sqrt{N}}\\] Мы будем использовать данные с курса по статистике Университета Шеффилда про эффективность диет. Мы вытащим оттуда данные по диете номер 1 и посмотрим, действительно ли она помогает сбросить вес. diet &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/stcp-Rdataset-Diet.csv&quot;) diet1 &lt;- diet %&gt;% filter(Diet == 1) Провести двухвыборочный t-тест можно в R двумя базовыми способами. Первый вариант - это дать два вектора значений. Это удобно в случае широкого формата данных. t.test(diet1$pre.weight, diet1$weight6weeks, paired = TRUE) ## ## Paired t-test ## ## data: diet1$pre.weight and diet1$weight6weeks ## t = 7.2168, df = 23, p-value = 2.397e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.354069 4.245931 ## sample estimates: ## mean of the differences ## 3.3 Второй вариант - используя формулы. Это удобно при длинном формате данных: diet1_long &lt;- diet1 %&gt;% pivot_longer( cols = c(pre.weight, weight6weeks), names_to = &quot;when&quot;, values_to = &quot;weight&quot; ) t.test(weight ~ when, data = diet1_long, paired = TRUE) ## ## Paired t-test ## ## data: weight by when ## t = 7.2168, df = 23, p-value = 2.397e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.354069 4.245931 ## sample estimates: ## mean of the differences ## 3.3 t.test(diet1_long$weight ~ diet1_long$when, paired = TRUE) ## ## Paired t-test ## ## data: diet1_long$weight by diet1_long$when ## t = 7.2168, df = 23, p-value = 2.397e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.354069 4.245931 ## sample estimates: ## mean of the differences ## 3.3 В обоих вариантах мы использовали paired = TRUE , чтобы обозначить использование именно зависимого (т.е. парного) t-теста. Внутрииндивидуальный план Межиндивидуальный план Зависимый t-тест Независимый t-тест t.test(..., paired = TRUE) t.test(..., paired = FALSE) 14.2.2 Двухвыборочный независимый t-тест В случае независимого t-теста формула отличается. Однако гипотеза остается такой же и логика примерна та же. У двух выборок могут различаться стандартные отклонения, поэтому для подсчет стандартной ошибки разницы средних для независимых выборок нужно сначала посчитать объединенное стандартное отклонение (pooled standard deviation): \\[s^2_{pool} = \\frac {(n_1-1)s^2_1 + (n_2-1)s^2_2} {(n_1 - 1) + (n_2 -1)}\\] Тогда стандартная ошибка разницы средних считается следующим образом: \\[se_{m_1 - m_2} = \\sqrt {(s^2_{pool}) (\\frac {1} {n_1} + \\frac {1}{n_2} )}\\] Выглядит сложно, но по своей сути это что-то вроде усредненного стандартного отклонения (с учетом размеров выборок). Ну а t-статистика затем считается просто: \\[t = \\frac {(m_1 - m_2) - (\\mu_1 - \\mu_2)} {se_{m_1 - m_2}} = \\frac {(m_1 - m_2) - 0} {se_{m_1 - m_2}} = \\frac {m_1 - m_2} {se_{m_1 - m_2}}\\] Давайте теперь опробуем независимый t-тест для сравнения веса испытуемых двух групп после диеты. Мы снова воспользуемся функцией t.test(), но теперь уже поставим paired = FALSE: diet12 &lt;- diet %&gt;% filter(Diet %in% 1:2) t.test(weight6weeks ~ Diet, data = diet12, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: weight6weeks by Diet ## t = 0.5711, df = 48.724, p-value = 0.5706 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.753268 6.732897 ## sample estimates: ## mean in group 1 mean in group 2 ## 69.57500 68.08519 Если присмотритесь, то увидите, что со степенями свободы что-то странное. Они дробные! Дело в том, что мы провели не совсем “настоящий” t-тест, а его очень близкую альтернативу под названием тест Уэлча (Welch test), который иногда называют поправкой Уэлча к t-тесту. Эта поправка позволяет тесту лучше справляться с выборками с разной дисперсией. Даже если у нас нет проблем с разной дисперсией, то от поправки Уэлча хуже не будет, поэтому это вариант по умолчанию в R. Но если его хочется отключить, то нужно поставить var.equal = TRUE. t.test(weight6weeks ~ Diet, data = diet12, paired = FALSE, var.equal = TRUE) ## ## Two Sample t-test ## ## data: weight6weeks by Diet ## t = 0.5645, df = 49, p-value = 0.575 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.813769 6.793399 ## sample estimates: ## mean in group 1 mean in group 2 ## 69.57500 68.08519 14.3 Допущения t-теста Чтобы подсчет p-value был корректным, некоторые допущения должны быть выполнены. Нормальное распределение. Это самое известное допущение, но чуть ли не наименее важное. Часто утверждается, что выборки должны быть взяты из нормального распределения. Это не совсем так: да, верно, это достаточное условие, но не необходимое. В первую очередь, важно именно выборочное распределение (средних разниц или разниц средних), которое достигается за счет центральной предельной теоремы (13.8) особенно при большой выборке (Lumley et al. 2002). Отсюда и все правила в духе “для t-теста распределение должно быть нормальным, но если n &gt; 30, то это необязательно.” Откуда именно 30? Да ниоткуда, просто. Как и со всеми точными числами в статистике. Для проверки на нормальность существуют различные статистические тесты. Самый известный из них — тест Шапиро-Уилка. Его можно провести в R при помощи функции shapiro.test(). shapiro.test(samp) ## ## Shapiro-Wilk normality test ## ## data: samp ## W = 0.98122, p-value = 0.1654 В нашем случае p-value больше 0.05, что логично: мы взяли эту выборку именно из нормального распределения. Если p-value меньше уровня \\(\\alpha\\), который у нас стандартно 0.05, то мы можем отвергнуть нулевую гипотезу о том, что выборка взята из нормального распределения. Если это так, то нам нужно, по идее, отказаться от t-теста и использовать непараметрический тест, который не имеет требований к распределению исследуемой переменной. Однако проведения теста на нормальность для проверки допущения о нормальности — вещь довольно бессмысленная. Дело в том, что тест Шапиро-Уилка — это такой же статистический тест, как и все прочие: чем больше выборка, тем с большей вероятностью он “поймает” отклонения от нормальности, чем меньше выборка, тем с меньшей вероятностью он обнаружит даже серьезные отклонения от нормальности. А нам-то нужно наоборот! При большой выборке отклонения от нормальности нам не особо страшны, а при маленькой тест все равно ничего не обнаружит. Более того, идеально нормальных распределений в природе вообще почти не существует! А это значит, что при достаточно большой выборке тест Шапиро-Уилка практически всегда будет находить отклонения от нормальности. Все это делает его малоинформативным при тестировании допущения о нормальности. Это же верно и для других тестов на нормальность. Другой способ проверять допущение о нормальности — это проверять визуально с помощью гистограммы или Q-Q plot (см. @ref(lm_a)) Для двувыборочного независимого t-теста выборки должны быть взяты из распределения с одинаковыми дисперсиями. Однако это не так критично с применением поправки Уэлча. Независимость значений (или пар) в выборке. Типичным примером нарушения независимости является случай, когда t-тест применяется на неусредненных (например, по испытуемому) значениях. Еще один пример нарушения независимости — использование одного наблюдения несколько раз или использование одного и того же испытуемого несколько раз. Определить независимость можно следующим мысленным экспериментом: могу ли я хоть как-нибудь предсказать следующее значение в выборке? Например, в случае с несколькими значениями от одного испытуемого я могу ориентироваться на его предыдущие результаты и предсказать последующие результаты лучше, чем на основе простого среднего по всем остальным значениям. Это значит, что допущение о независимости нарушено. 14.4 Непараметрические аналоги t-теста Если выборка не очень большая и взята из сильно ассиметричного распределения или выборка представляет собой порядковые данные, то можно воспользоваться непараметрическими альтернативами для t-теста. Непараметрические тесты не имеют допущений о распределении, что делает их более универсальными. Большинство подобных тестов подразумевает превращение данных в ранги, т.е. внутри этих тестов происходит преобразование в ранговую шкалу. Такое преобразование может снизить статистическую мощность теста и привести к повышению вероятности ошибки второго рода. 14.4.1 Тест Уилкоксона Непараметрический аналог двустороннего зависимого t-теста называется тестом Уилкоксона. Функция для него называется wilcox.test(), и она имеет такой же синтаксис, как и t.test(). wilcox.test(weight ~ when, data = diet1_long, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: weight by when ## V = 299, p-value = 2.203e-05 ## alternative hypothesis: true location shift is not equal to 0 Не пугайтесь, когда видите сообщение “Есть совпадающие значения: не могу высчитать точное p-значение”. Это означает, что в ваших данных есть повторяющиеся значения, поэтому расчет p-value в данном случае — это некоторая апроксимация, но в большинстве случаев это не играет серьезной роли. 14.4.2 Тест Манна-Уитни Непараметрическим аналогом двустороннего независимого t-теста является тест Манна-Уитни. Для него тоже используется функция wilcox.test(), только в данном случае с параметром paired = FALSE. wilcox.test(weight ~ when, data = diet1_long, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight by when ## W = 357.5, p-value = 0.1546 ## alternative hypothesis: true location shift is not equal to 0 Внутрииндивидуальный план Межиндивидуальный план Зависимый t-тест: Независимый t-тест: t.test(..., paired = TRUE) t.test(..., paired = FALSE) Тест Уилкоксона: Тест Манна-Уитни: wilcox.test(..., paired = TRUE) wilcox.test(..., paired = FALSE) "],["cov-cor.html", "15 Ковариация и корреляция 15.1 Ковариация 15.2 Корреляция 15.3 Корреляционная матрица 15.4 Хитмэп корреляций", " 15 Ковариация и корреляция Возьмем новый набор данных, на этот раз про американских студентов, вес их рюкзаков и проблемы со спиной. Этот набор данных хранится в пакете Stat2Data — пакет с большим количеством разнообразных данных. install.packages(&quot;Stat2Data&quot;) С помощью функции data() загрузим набор данных Backpack: library(tidyverse) library(Stat2Data) data(Backpack) Давайте посмотрим, что внутри этой переменной: skimr::skim(Backpack) Таблица 15.1: Data summary Name Backpack Number of rows 100 Number of columns 9 _______________________ Column type frequency: factor 3 numeric 6 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Major 0 1 FALSE 41 Bio: 9, Bus: 8, LS: 7, ME: 6 Sex 0 1 FALSE 2 Fem: 55, Mal: 45 Status 0 1 FALSE 2 U: 97, G: 3 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist BackpackWeight 0 1 11.66 5.77 2.00 8.00 11.00 14.25 35.00 ▅▇▂▁▁ BodyWeight 0 1 153.05 29.40 105.00 130.00 147.50 170.00 270.00 ▆▇▂▁▁ Ratio 0 1 0.08 0.04 0.02 0.05 0.07 0.10 0.18 ▆▇▆▃▁ BackProblems 0 1 0.32 0.47 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▃ Year 0 1 3.20 1.39 0.00 2.00 3.00 4.00 6.00 ▃▆▇▆▆ Units 0 1 14.27 2.81 0.00 13.00 15.00 16.00 19.00 ▁▁▁▇▆ С помощью ?Backpack можно получить подробное описание колонок этого датасета. Например, можно заметить, что масса как самих студентов, так и их рюкзаков выражена в фунтах. Давайте создадим новые переменные backpack_kg и body_kg, в которых будет записан вес (рюкзаков и самих студентов соответственно) в понятным для нас килограммах. Новый набор данных сохраним под названием back. back &lt;- Backpack %&gt;% mutate(backpack_kg = 0.45359237 * BackpackWeight, body_kg = 0.45359237 * BodyWeight) До этого мы говорили о различиях между выборками. Теперь мы будем говорить о связи между переменными. 15.1 Ковариация Самая простая мера связи между двумя переменными — это ковариация. Если ковариация положительная, то чем больше одна переменная, тем больше другая переменная. При отрицательной ковариации все наоборот: чем больше одна переменная, тем меньше другая. Формула ковариации: \\[\\sigma_{xy} = cov(x, y) = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{n}\\] Оценка ковариации по выборке: \\[\\hat{\\sigma}_{xy} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\\] Ковариация переменной самой с собой — дисперсия. В R есть функция cov() для подсчета ковариации. На самом деле, функция var() делает то же самое. Обе эти функции считают сразу матрицу ковариаций для всех сочетаний колонок на входе: back %&gt;% select(body_kg, backpack_kg) %&gt;% cov() ## body_kg backpack_kg ## body_kg 177.807700 6.601954 ## backpack_kg 6.601954 6.838333 back %&gt;% select(body_kg, backpack_kg) %&gt;% var() ## body_kg backpack_kg ## body_kg 177.807700 6.601954 ## backpack_kg 6.601954 6.838333 Ну а по углам этой матрицы — дисперсии! 15.2 Корреляция Корреляцией обычно называют любую связь между двумя переменными, это просто синоним слова “ассоциация.” Если вдруг слово “корреляция” вам еще непривычно, то попробуйте мысленно заменять “корреляцию” на “ассоциацию,” а “коррелирует” на “связано.” Коэффициент корреляции — это уже конкретная математическая формула, которая позволяет посчитать эту связь и принимает значения от -1 до 1.37 Если коэффициент корреляции положительный, то чем больше значения в одной переменной, тем больше значения в другой переменной. Если коэффициент корреляции отрицательный, то чем больше значения в одной переменной, тем меньше значения в другой переменной. Если коэффициент корреляции равен 0, то изменения одной переменной не связано с изменениями в другой переменной. 15.2.1 Коэффициент корреляции Пирсона Самый известный коэффициент корреляции - коэффициент корреляции Пирсона: \\[\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i = 1}^n(x_i - \\overline{x})^2}\\sqrt{\\sum_{i = 1}^n(y_i - \\overline{y})^2}} = \\frac{1}{n}\\sum_{i = 1}^n z_{x,i} z_{y, i}\\] Оценка коэффициента корреляции Пирсона по выборке: \\[r_{xy} = \\frac{\\hat{\\sigma}_{xy}}{\\hat{\\sigma}_x \\hat{\\sigma}_y} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i = 1}^n(x_i - \\overline{x})^2}\\sqrt{\\sum_{i = 1}^n(y_i - \\overline{y})^2}} = \\frac{1}{n - 1}\\sum_{i = 1}^n z_{x,i} z_{y, i}\\] Коэффициент корреляции Пирсона можно понимать по-разному. С одной стороны, это просто ковариация, нормированная на стандартное отклонение обоих переменных. С другой стороны, можно понимать это как среднее произведение z-оценок. Корреляцию в R можно посчитать с помощью функции cor(): back %&gt;% select(body_kg, backpack_kg) %&gt;% cor() ## body_kg backpack_kg ## body_kg 1.0000000 0.1893312 ## backpack_kg 0.1893312 1.0000000 Для тестирования уровня значимости нулевой гипотезы для корреляции есть функция cor.test(). В случае с коэффициентами корреляции, нулевая гипотеза формулируется как отсутствие корреляции (т.е. она равна нулю) в генеральной совокупности. cor.test(back$backpack_kg, back$body_kg) ## ## Pearson&#39;s product-moment correlation ## ## data: back$backpack_kg and back$body_kg ## t = 1.9088, df = 98, p-value = 0.05921 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.007360697 0.371918344 ## sample estimates: ## cor ## 0.1893312 Результат выполнения этой функции очень похож на то, что мы получали при проведении t-теста. 15.2.2 Непараметрические коэффициенты корреляции У коэффициента корреляции Пирсона, как и у t-теста, есть свои непараметрические братья: коэффициент корреляции Спирмена и коэффициент корреляции Кэнделла. Из них чаще используется коэффициент корреляции Спирмена. Посчитать его можно с помощью той же функции cor.test(), задав соответствующее значение параметра method =: cor.test(back$backpack_kg, back$body_kg, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: back$backpack_kg and back$body_kg ## S = 131520, p-value = 0.03527 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.2108001 cor.test(back$backpack_kg, back$body_kg, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: back$backpack_kg and back$body_kg ## z = 2.083, p-value = 0.03725 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.1478736 Заметьте, в данном случае два метода хотя и привели к схожим размерам корреляции, но в одном случае p-value оказался больше 0.05, а в другом случае - меньше 0.05. Выбирать тест a posteriori на основе того, какие результаты вам нравятся больше, — плохая практика (@ref(bad_practice)). Не надо так делать. 15.3 Корреляционная матрица Возможно, вы нашли что-то более интересное для проверки гипотезы о корреляции. Например, вы еще хотите проверить гипотезу о связи количества учебных кредитов и массе рюкзака: логично предположить, что чем больше студент набрал себе курсов, тем тяжелее его рюкзак (из-за большего количества учебников). Или что студенты к старшим курсам худеют и становятся меньше. Или что те, кто набрал себе много курсов, меньше питаются и от того меньше весят. В общем, хотелось бы прокоррелировать все интересующие нас переменные со всеми. Это можно сделать с помощью функции cor(): back %&gt;% select(body_kg, backpack_kg, Units, Year) %&gt;% cor() ## body_kg backpack_kg Units Year ## body_kg 1.00000000 0.18933115 -0.23524088 -0.09301727 ## backpack_kg 0.18933115 1.00000000 0.09438453 0.05762194 ## Units -0.23524088 0.09438453 1.00000000 -0.02946373 ## Year -0.09301727 0.05762194 -0.02946373 1.00000000 Но функция cor()не позволяет посчитать p-value для этих корреляций! Функция cor.test() позволяет получить p-value, но только для одной пары переменных. На помощь приходит пакет psych с функцией corr.test(): back %&gt;% select(body_kg, backpack_kg, Units, Year) %&gt;% psych::corr.test() ## Call:psych::corr.test(x = .) ## Correlation matrix ## body_kg backpack_kg Units Year ## body_kg 1.00 0.19 -0.24 -0.09 ## backpack_kg 0.19 1.00 0.09 0.06 ## Units -0.24 0.09 1.00 -0.03 ## Year -0.09 0.06 -0.03 1.00 ## Sample Size ## [1] 100 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## body_kg backpack_kg Units Year ## body_kg 0.00 0.30 0.11 1 ## backpack_kg 0.06 0.00 1.00 1 ## Units 0.02 0.35 0.00 1 ## Year 0.36 0.57 0.77 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Тем не менее, если у вас много гипотез для тестирования, то у вас появляется проблема: вероятность выпадения статистически значимых результатов сильно повышается. Даже если эти переменные никак не связаны друг с другом. Эта проблема называется проблемой множественных сравнений (multiple comparisons problem).38 Если мы проверяем сразу несколько гипотез, то у нас возрастает групповая вероятность ошибки первого рода (Family-wise error rate) — вероятность ошибки первого рода для хотя бы одной из множества гипотез. Например, если вы коррелируете 10 переменных друг с другом, то вы проверяете 45 гипотез о связи. Пять процентов из этих гипотез, т.е. в среднем 2-3 гипотезы у вас будут статистически значимыми даже если никаких эффектов на самом деле нет! Поэтому если вы проверяете сразу много гипотез, то необходимо применять поправки на множественные сравнения (multiple testing correction). Эти поправки позволяют контролировать групповую вероятность ошибки первого рода на желаемом уровне. Самая простая и популярная поправка на множественные сравнения — поправка Бонферрони (Bonferroni correction). Она считается очень просто: мы просто умножаем p-value на количество проверяемых гипотез! back %&gt;% select(body_kg, backpack_kg, Units, Year) %&gt;% psych::corr.test(adjust = &quot;bonferroni&quot;) ## Call:psych::corr.test(x = ., adjust = &quot;bonferroni&quot;) ## Correlation matrix ## body_kg backpack_kg Units Year ## body_kg 1.00 0.19 -0.24 -0.09 ## backpack_kg 0.19 1.00 0.09 0.06 ## Units -0.24 0.09 1.00 -0.03 ## Year -0.09 0.06 -0.03 1.00 ## Sample Size ## [1] 100 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## body_kg backpack_kg Units Year ## body_kg 0.00 0.36 0.11 1 ## backpack_kg 0.06 0.00 1.00 1 ## Units 0.02 0.35 0.00 1 ## Year 0.36 0.57 0.77 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Это очень “дубовая” и излишне консервативная поправка. Да, она гарантирует контроль групповую вероятности ошибки первого рода, но при этом сильно повышает вероятность ошибки второго рода — вероятность пропустить эффект, если он на самом деле существует. Поэтому по умолчанию в R используется более либеральная поправка на множественные сравнения под названием поправка Холма или поправка Холма-Бонферрони (Holm-Bonferroni correction), которая, тем не менее, тоже гарантирует контроль групповой вероятности ошибки первого рода. Альтернативный подход к решению проблемы множественных сравнений — это контроль средней доли ложных отклонений (False Discovery Rate; FDR) на на уровне не выше уровня \\(\\alpha\\). Это более либеральный подход: в данном случае мы контролируем, что ложно-положительных результатов у нас не больше, например, 5%. Такой подход применяется в областях, где происходит масштабное множественное тестирование. Попытка контролировать групповую вероятность ошибки первого уровня не выше уровня \\(\\alpha\\) привела бы к чрезвычайно низкой вероятности обнаружить хоть какие-нибудь эффекты (т.е. к низкой статистической мощности). Самая известная поправка для контроля средней доли ложных отклонений — это поправка Бенджамини — Хохберга (Benjamini-Hochberg correction). back %&gt;% select(body_kg, backpack_kg, Units, Year) %&gt;% psych::corr.test(adjust = &quot;BH&quot;) ## Call:psych::corr.test(x = ., adjust = &quot;BH&quot;) ## Correlation matrix ## body_kg backpack_kg Units Year ## body_kg 1.00 0.19 -0.24 -0.09 ## backpack_kg 0.19 1.00 0.09 0.06 ## Units -0.24 0.09 1.00 -0.03 ## Year -0.09 0.06 -0.03 1.00 ## Sample Size ## [1] 100 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## body_kg backpack_kg Units Year ## body_kg 0.00 0.18 0.11 0.54 ## backpack_kg 0.06 0.00 0.54 0.68 ## Units 0.02 0.35 0.00 0.77 ## Year 0.36 0.57 0.77 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Все перечсиленные поправки (и еще несколько других) доступны не только в функции corr.test(), но и в базовом R с помощью функции p.adjust(). Эта функция принимает вектор из p-value и возвращает результат применения поправок. p_vec &lt;- seq(0.0001, 0.06, length.out = 10) p_vec ## [1] 0.000100000 0.006755556 0.013411111 0.020066667 0.026722222 0.033377778 ## [7] 0.040033333 0.046688889 0.053344444 0.060000000 p.adjust(p_vec) #по умолчанию используется поправка Холма-Бонферрони ## [1] 0.0010000 0.0608000 0.1072889 0.1404667 0.1603333 0.1668889 0.1668889 ## [8] 0.1668889 0.1668889 0.1668889 p.adjust(p_vec, method = &quot;bonferroni&quot;) ## [1] 0.00100000 0.06755556 0.13411111 0.20066667 0.26722222 0.33377778 ## [7] 0.40033333 0.46688889 0.53344444 0.60000000 p.adjust(p_vec, method = &quot;BH&quot;) ## [1] 0.00100000 0.03377778 0.04470370 0.05016667 0.05344444 0.05562963 ## [7] 0.05719048 0.05836111 0.05927160 0.06000000 15.4 Хитмэп корреляций Впрочем, это не так уж и важно: на практике часто опускают слово “коэффициент” и называют корреляцией как и просто связь, так и ее способ измерения.↩︎ “Проблема множественных сравнений” - это устоявшийся термин, который используется и в случае множественных корреляций, и в случае множественных сравнений средних и в любых других случаях с тестированием нескольких гипотез одновременно.↩︎ "],["lm.html", "16 Линейная регрессия 16.1 Функция lm() 16.2 Интерпретация вывода линейной регрессии 16.3 Допущения линейной регрессии 16.4 Влияние выбросов на линейную модель 16.5 Множественная линейная регрессия", " 16 Линейная регрессия Вы уже умеете считать коэффициент корреляции Пирсона: cor.test(back$backpack_kg, back$body_kg) ## ## Pearson&#39;s product-moment correlation ## ## data: back$backpack_kg and back$body_kg ## t = 1.9088, df = 98, p-value = 0.05921 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.007360697 0.371918344 ## sample estimates: ## cor ## 0.1893312 Простая линейная регрессия - это примерно то же самое. В синтаксисе линейной регрессии уже не обойтись без формул, это такой специальный тип данных в R: class(y ~ x) ## [1] &quot;formula&quot; Если видите эту волнистую линию - тильду (~), это значит, что перед вами формула. Мы уже сталкивались с формулами ранее, они иногда используются для задания отношений между переменными, например, для определения фасеток на графике (@ref(gg_base)). Давайте исследуем зависимость размера рюкзака от массы тела. В простой линейной регрессии, в отличие от корреляции, есть направленность: одна переменная является как бы независимой переменной или предиктором, другая - как бы объясняемой переменной (outcome). В формуле предикторы находятся справа от тильды, а объясняемая переменная - слева. Я специально написал “как бы”: если одна переменная предиктор, а другая объясняется этим предиктором, то кажется, что они должны быть обязательно связаны причинно-следственной связью. Это не так: обозначения условны, более того, вы можете поменять переменные местами и ничего не изменится! Короче говоря, линейная регрессия не дает никакой магической каузальной силы исследуемым переменным. 16.1 Функция lm() Давайте посчитаем линейную регрессию функцией lm(). model &lt;- lm(backpack_kg ~ body_kg, data = back) model ## ## Call: ## lm(formula = backpack_kg ~ body_kg, data = back) ## ## Coefficients: ## (Intercept) body_kg ## 2.71125 0.03713 print(model) или просто model выводит коэфициенты линейной регрессии - это коэффициенты прямой, которая лучше всего подогнанна к данным. Как измеряется качество этой подгонки? В расстоянии точек исходных точек до прямой. По идее, расстояние до прямой нужно было бы считать просто по модулю. И так делают, хоть и очень редко. Обычно в линейной регрессии используются квадратичные расстояния точек до прямой для оценки расстояния (метод наименьших квадратов - ordinary least squares). Это дает кучу клевых математических свойств, например, возможность легко аналитически найти коэффициенты прямой линейной регрессии. Давайте теперь нарисуем регрессионную прямую поверх диаграммы рассеяния: ggplot(data = back,aes(x = body_kg, y = backpack_kg))+ geom_point(alpha = 0.3)+ geom_abline(slope = model$coefficients[2], intercept = model$coefficients[1]) Функция predict() позволяет скормить модели новые данные и получить предсказания для новых значений предикторов. Попробуем поиграть с этим немного. Допустим, предскажем вес рюкзака для студента весом в 100 кг: predict(model, newdata = data.frame(body_kg = 100)) ## 1 ## 6.424229 Мы можем даже попробовать какие-нибудь экстремальные значения для предикторов. Например, сколько будет весить рюкзак студента весом 1000 кг? predict(model, newdata = data.frame(body_kg = 1000)) ## 1 ## 39.841 Очевидно, что в этом не очень много смысла: студент весом 1000 кг не сможет ходить на занятия, поэтому и про вес рюкзака как-то не имеет смысл спрашивать. Это проблема экстрополяции: линейная регрессия позволяет более-менее достоверно предсказывать значения внутри диапазона значений, на которых была построена модель. Еще один “странный” пример - студент весом 0 кг. predict(model, newdata = data.frame(body_kg = 0)) ## 1 ## 2.711255 Здесь бессмысленность происходящего еще очевиднее. Конечно, вес студента не может быть равен нулю, иначе это не студент вовсе. Однако это позволяет понять, что такое intercept модели - это значение зависимой переменой в случае, если предиктор равен нулю. А коэффициент предиктора означает, насколько килограммов увеличивается вес рюкзака при увеличении веса студента на 1 кг: на 0.0371297. Не очень много! 16.2 Интерпретация вывода линейной регрессии Гораздо более подробные результаты мы получим, если применим уже известную нам generic функцию summary() на нашу модель. summary(model) ## ## Call: ## lm(formula = backpack_kg ~ body_kg, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4853 -1.7629 -0.4681 1.2893 9.8803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.71125 1.37483 1.972 0.0514 . ## body_kg 0.03713 0.01945 1.909 0.0592 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.581 on 98 degrees of freedom ## Multiple R-squared: 0.03585, Adjusted R-squared: 0.02601 ## F-statistic: 3.644 on 1 and 98 DF, p-value: 0.05921 Теперь мы понимаем, что это за коэффициенты. Однако это всего лишь их оценка. Это значит, что мы допускаем, что в реальности есть некие настоящие коэффициенты линейной регрессии, а каждый раз собирая новые данные, они будут посчитаны как немного разные. Короче говоря, эти коэффициенты - те же статистики, со своим выборочным распределением и стандартными ошибками. На основе чего и высчитывается p-value для каждого коэффициента - вероятность получить такой и более отклоняющийся от нуля коэффициент при верности нулевой гипотезы - независимости зависимой переменной от предиктора. Кроме p-value, у линейной регрессии есть \\(R^2\\) - доля объясненной дисперсии. Как ее посчитать? Для начала давайте сохраним как отдельные колонки ошибки (необъясненную часть модели) и предсказанные значения (они означают объясненную часть модели). Можно убедиться, что сумма предсказанных значений и ошибок будет равна зависимой переменной. head(model$residuals) ## 1 2 3 4 5 6 ## -0.7341441 -2.3666602 -0.1963429 -2.6001743 -2.1140337 -4.4853169 back$residuals &lt;- residuals(model) back$fitted &lt;- fitted(model) back %&gt;% transmute(backpack_kg - (fitted + residuals)) Соответственно, вся сумма объясненной дисперсии разделяется на объясненую и необъясненную. Полная дисперсия (total sum of squares = TSS) может быть посчитана как сумма квадратов разниц со средним. Необъясненная дисперсия - это сумма квадратов ошибок - residual sum of squares (RSS). rss &lt;- sum(back$residuals^2) rss ## [1] 652.7272 tss &lt;- sum((back$backpack_kg - mean(back$backpack_kg))^2) tss ## [1] 676.995 1- rss/tss ## [1] 0.03584628 Это очень мало, мы объяснили всего 3.5846285% дисперсии. Собственно, и p-value больше, чем 0.05. summary(model) ## ## Call: ## lm(formula = backpack_kg ~ body_kg, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4853 -1.7629 -0.4681 1.2893 9.8803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.71125 1.37483 1.972 0.0514 . ## body_kg 0.03713 0.01945 1.909 0.0592 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.581 on 98 degrees of freedom ## Multiple R-squared: 0.03585, Adjusted R-squared: 0.02601 ## F-statistic: 3.644 on 1 and 98 DF, p-value: 0.05921 При этом p-value тот же, что и при коэффициента корреляции Пирсона. Это не случайно: \\(R^2\\) - это квадрат коэффициента корреляции Пирсона, если речь идет только об одном предикторе. Давайте это проверим: cor.test(back$body_kg, back$backpack_kg)$estimate^2 ## cor ## 0.03584628 16.3 Допущения линейной регрессии Как и в случае с другими параметрическими методами, линейная регрессия имеет определенные допущения относительно используемых данных. Если они не соблюдаются, то все наши расчеты уровня значимости могут некорректными. Очень важно ставить вопрос о том, насколько результаты будут некорректными. Как сильно нарушения допущений будет влиять на модель? Ответ на этот вопрос может быть контринтуитивен. Например, достаточно большие отклонения от нормальности нам обычно не страшны при условии того, что выборка достаточно большая. Допущения линейной регрессии связаны с ошибками: они должны быть нормально распределены, а разброс ошибок должен не уменьшаться и не увеличиваться в зависимости от предсказанных значений. Это то, что называется гомоскедастичностью или гомогенностью (когда все хорошо) и гетероскедастичностью или гетерогенностью (когда все плохо). Если мы применим функцию plot(), то получим 4 скаттерплота: Зависимость ошибок от предсказанных значений. На что здесь смотреть? На симметричность относительно нижней и верхней части графика, на то, что разброс примерно одинаковый слева и справа. Q-Q plot. Здесь все довольно просто: если ошибки являются выборкой из нормального распределения, то они выстраиваются в прямую линию. Если это мало похоже на прямую линию, то имеет место отклонение от нормальности. Scale-Location plot. Этот график очень похож на график 1, только по оси у используются квадратные корни модуля ошибки. Еще один способ исследовать гетеро(гомо)скедастичность и находить выбросы. Residuals-Leverage plot. Здесь по оси х - расстояние Кука, а по оси у - стандартизированный размер выбросов. Расстояние Кука показывает high-leverage points - точки, которые имеют экстремальные предсказанные значения, то есть очень большие или очень маленькие значения по предикторам. Для линейной регрессии такие значения имеют большее значение, чем экстремальные точки по предсказываемой переменной. Особенно сильное влияние имеют точки, которые имеют экстремальные значения и по предикторам, и по предсказываемой переменной. Одна такая точка может поменять направление регрессионной прямой! Расстояние Кука отражает уровень leverage, а стандартизированные ошибки отражают экстремальные значения по у (вернее, экстремальные отклонения от предсказанных значений). В этом графике нужно смотреть на точки с правой стороны графика, особенно если они находятся высоко или низко по оси у. plot(model) 16.4 Влияние выбросов на линейную модель Давайте теперь попробуем посмотреть, как изменится модель, если выкинуть high leverage points (экстремальные значения по предиктору - body) и что будет, если выкинуть экстремальные значения по у. Обычная линия - регрессионная прямая для модели со всеми точками, штрихованная линия - регрессионная прямая для модели без экстремальных значений по предиктору, пунктирная линия - регрессионная прямая для модели без экстремальных значений по предсказываемой переменной. is_outlier &lt;- function(x, n = 2, centr = mean, vary = sd) { (x &gt; centr(x) + n * vary(x)) | (x &lt; centr(x) - n * vary(x)) } back &lt;- back %&gt;% mutate(body_outlier = is_outlier(body_kg), backpack_outlier = is_outlier(backpack_kg)) model_without_outliers_by_x &lt;- back %&gt;% filter(!body_outlier) %&gt;% lm(formula = backpack_kg ~ body_kg) summary(model_without_outliers_by_x) ## ## Call: ## lm(formula = backpack_kg ~ body_kg, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6526 -1.7471 -0.3773 1.1699 9.0854 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.48577 1.65749 0.293 0.77011 ## body_kg 0.07128 0.02413 2.953 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.548 on 94 degrees of freedom ## Multiple R-squared: 0.08491, Adjusted R-squared: 0.07517 ## F-statistic: 8.722 on 1 and 94 DF, p-value: 0.003971 model_without_outliers_by_y &lt;- back %&gt;% filter(!backpack_outlier) %&gt;% lm(formula = backpack_kg ~ body_kg) summary(model_without_outliers_by_y) ## ## Call: ## lm(formula = backpack_kg ~ body_kg, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7843 -1.4343 -0.1363 1.4296 4.9122 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.60560 1.13144 3.187 0.00196 ** ## body_kg 0.01915 0.01610 1.190 0.23716 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.087 on 93 degrees of freedom ## Multiple R-squared: 0.01499, Adjusted R-squared: 0.004402 ## F-statistic: 1.416 on 1 and 93 DF, p-value: 0.2372 ggplot(data = back, aes(x = body_kg, y = backpack_kg, shape = body_outlier, colour = backpack_outlier))+ geom_point(alpha = 0.3)+ geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2])+ geom_abline(intercept= model_without_outliers_by_x$coefficients[1], slope = model_without_outliers_by_x$coefficients[2], linetype = &quot;dashed&quot;)+ geom_abline(intercept= model_without_outliers_by_y$coefficients[1], slope = model_without_outliers_by_y$coefficients[2], linetype = &quot;dotted&quot;)+ theme_minimal() Таким образом, именно экстремальные значения по предиктору, а не по объяснияемой переменной имеют особенно сильное значение на регрессионную модель. 16.5 Множественная линейная регрессия В множественной линейной регрессионной регрессии у нас появляется несколько предикторов. Какая модель лучше: где есть много предикторов или где мало предикторов? С одной стороны, чем больше предикторов, тем лучше: каждый новый предиктор может объяснить чуть больше необъясненной дисперсиии. С другой стороны, если эта прибавка маленькая (а она всегда будет не меньше нуля), то, возможно, новый предиктор просто объясняет “случайный шум.” В действительности, если у нас будет достаточно много предикторов, то мы сможем объяснить любые данные! Парадоксальным образом такая модель будет давать очень хорошие результаты на той выборке, по которой мы считаем коэффициенты, но делать очень плохие предсказания на новой выборке - это то, что в машинном обучении называют переобучением (overfitting). Идеальная модель будет включать минимум предикторов, которые лучше всего объясненяют исследуемую переменную. Это что-то вроде бритвы Оккама в статистике. Поэтому часто используются показатели качества модели, которые “наказывают” модель за большое количество предикторов. Например, adjusted R2: \\[R_{adj} = 1 - (1 - R^2) \\frac{n -1}{n - p - 1}\\] Здесь n - это количество наблюдений, p - количество параметров. Итак, добавим новый предиктор - Units. Это количество кредитов, которые студенты взяли в четверти39. Можно предположить, что чем больше у студента набрано кредитов, тем более тяжелый у нее/него рюкзак. Давайте добавим это как второй предиктор. Для этого нужно просто записать второй предиктор в формуле через плюс. model_mult &lt;- lm(backpack_kg ~ body_kg + Units, data = back) summary(model_mult) ## ## Call: ## lm(formula = backpack_kg ~ body_kg + Units, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6221 -1.8347 -0.5023 1.2519 10.0623 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.28481 2.16170 0.132 0.8955 ## body_kg 0.04391 0.01990 2.207 0.0297 * ## Units 0.13703 0.09456 1.449 0.1505 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.566 on 97 degrees of freedom ## Multiple R-squared: 0.05628, Adjusted R-squared: 0.03682 ## F-statistic: 2.892 on 2 and 97 DF, p-value: 0.06025 Множественная линейная регрессия имеет еще одно допущение: отсутствие мультиколлинеарности. Это значит, что предикторы не должны коррелировать друг с другом. Для измерения мультколлинеарности существует variance inflation factor (VIF-фактор). Считается он просто: для предиктора \\(i\\) считается линейная регрессия, где все остальные предикторы предсказывают предиктор \\(i\\). Сам VIF-фактор считается на основе полученного R2 регрессии: \\[VIF_i = \\frac{1}{1 - R_i^2}\\] Если Ri2 большой, то и VIFi выходит большим. Это означает, что предиктор сам по себе хорошо объясняется другими предикторами. Какой VIF считать большим? Здесь нет единого мнения, но если он выше 3 и особенно если он выше 10, то с этим нужно что-то делать. car::vif(model_mult) ## body_kg Units ## 1.05858 1.05858 В нашем случае это не так. Но если бы VIF был большим для какого-либо предиктора, то можно было бы либо попробовать его выкинуть или же использовать анализ главных компонент (см. ??), о котором пойдет речь в один из следующих дней. ANOVA от ANalysis Of VAriance, по-русски часто читается как “АНОВА.”↩︎ "],["anova.html", "17 Дисперсионный анализ (ANOVA) 17.1 Тестирование значимости нулевой гипотезы в ANOVA. 17.2 Post-hoc тесты 17.3 ANOVA и т-тест как частные случаи линейной регрессии 17.4 Dummy coding 17.5 Допущения ANOVA 17.6 Многофакторный дисперсионный анализ (Factorial ANOVA) 17.7 Дисперсионный анализ с повторными измерениями (Repeated-measures ANOVA) 17.8 Смешанный дисперсионный анализ (Mixed between-within subjects ANOVA) 17.9 Непараметрические аналоги ANOVA 17.10 Заключение", " 17 Дисперсионный анализ (ANOVA) Дисперсионный анализ или ANOVA40 - один из самых распространенных методов статистического анализа в психологии, биологии, медицине и многих других дисциплинах. Дисперсионный анализ очень хорошо подходит для анализа данных, полученных в эксперименте - методе организации исследования, при котором исследователь напрямую управляет уровнями независимой переменной. Терминологическая связь между дисперсионным анализом и планированием эксперимента настолько тесная, что многие термины пересекаются, поэтому нужно быть осторожными. Как и в случае с линейной регрессией, если мы что-то называем “независимой переменной” (или “фактором”), это не порождает никакой каузальной связи. Еще одна важная вещь, которую нужно понимать про дисперсионный анализ, это то, что у этого метода очень запутывающее название: из названия кажется, что этот статистический метод для сравнения дисперсий. Нет, это не так (хотя такие статистические тесты тоже есть, и они нам сегодня пригодятся - см. 17.5). Нет, это просто сравнение средних в случае, если есть больше, чем 2 группы для сравнения. У дисперсионного анализа очень много разновидностей, для которых придумали множество названий. “Обычная” ANOVA называется One-Way ANOVA, она же межгрупповая ANOVA, это аналог независимого т-теста для нескольких групп. Давайте начнем сразу с проведения теста. Мы будем использовать данные с курса по статистике Университета Шеффилда про эффективность диет, на которых мы разбирались с t-тестом (@ref(dep_ttest)). library(tidyverse) diet &lt;- read_csv(&quot;data/stcp-Rdataset-Diet.csv&quot;) Сделаем небольшой препроцессинг данных. Создадим дополнительные “факторные” переменные, создадим переменную, в которой будет разница массы “до” и “после,” удалим NA. diet &lt;- diet %&gt;% mutate(weight.loss = weight6weeks - pre.weight, Dietf = factor(Diet, labels = LETTERS[1:3]), Person = factor(Person)) %&gt;% drop_na() 17.0.1 Функция aov() Попробуем сразу провести дисперсионных анализ с помощью функции aov(): aov_model &lt;- aov(weight.loss ~ Dietf, diet) aov_model ## Call: ## aov(formula = weight.loss ~ Dietf, data = diet) ## ## Terms: ## Dietf Residuals ## Sum of Squares 60.5270 410.4018 ## Deg. of Freedom 2 73 ## ## Residual standard error: 2.371064 ## Estimated effects may be unbalanced summary(aov_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dietf 2 60.5 30.264 5.383 0.0066 ** ## Residuals 73 410.4 5.622 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Мы получили что-то похожее на результат применения функции lm(). Правда, лаконичнее, но с новыми столбцами Sum Sq, Mean Sq и новой статистикой F вместо t. Что будет, если с теми же данными с той же формулой запустить lm() вместо aov()? summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 lm() превратил Dietf в две переменные, но F и p-value у двух моделей одинаковые! Кроме того, функция aov() является, по сути, просто “оберткой” над lm(): This provides a wrapper to lm for fitting linear models to balanced or unbalanced experimental designs. 17.1 Тестирование значимости нулевой гипотезы в ANOVA. Как и в случае с другими статистическими тестами, мы можем выделить 4 этапа в тестировании значимости нулевой гипотезы в ANOVA: Формулирование нулевой и альтернативной гипотезы. Нулевая гипотеза говорит, что между средними в генеральной совокупности нет различий: \\[H_0:\\mu_1 = \\mu_2 = ... = \\mu_n\\] Можно было бы предположить, что ненулевая гипотеза звучит как “все средние не равны,” но вообще-то это не так. Альтернативная гипотеза в дисперсионном анализе звучит так: \\[H_1: \\text{Не все средние равны}\\] Подсчет статистики. Как мы уже видели раньше, в дисперсионном анализе используется новая для нас статистика F. Впрочем, мы ее видели, когда смотрели на аутпут функции lm(), когда делали линейную регрессию. Чтобы считать F (если вдруг мы хотим сделать это вручную), нужно построить талбицу ANOVA (ANOVA table). Таблица ANOVA Степени свободы Суммы квадратов Средние квадраты F-статистика Межгрупповые \\(df_{b}\\) \\(SS_{b}\\) \\(MS_{b} =\\frac{SS_{b}}{df_{b}}\\) \\(F=\\frac{MS_{b}}{MS_{w}}\\) Внутригрупповые \\(df_{w}\\) \\(SS_{w}\\) \\(MS_{w} =\\frac{SS_{w}}{df_{w}}\\) Общие \\(df_{t}\\) \\(SS_{t}= SS_{b} + SS_{w}\\) Именно эту таблицу мы видели, когда использовали функцию aov(): summary(aov_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dietf 2 60.5 30.264 5.383 0.0066 ** ## Residuals 73 410.4 5.622 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Вот как это все считается: Таблица ANOVA Степени свободы Суммы квадратов Средние квадраты F-статистика Между \\(df_{b}=J-1\\) \\(SS_{b}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (\\overline{x_j}-\\overline{x})^2\\) \\(MS_{b} =\\frac{SS_{b}}{df_{b}}\\) \\(F=\\frac{MS_{b}}{MS_{w}}\\) Внутри \\(df_{w}=N-J\\) \\(SS_{w}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (x_{ij}-\\overline{x_j})^2\\) \\(MS_{w} =\\frac{SS_{w}}{df_{w}}\\) Общие \\(df_{t}=N-1\\) \\(SS_{t}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (x_{ij}-\\overline{x})^2\\) \\(J\\) означает количество групп, \\(N\\) - общее количество наблюдений во всех группах, \\(n_j\\) означает количество наблюдений в группе j, а \\(x_{ij}\\) - наблюдение под номером \\(i\\) в группе \\(j\\). Вариабельность обозначается \\(SS\\) и означает “сумму квадратов” (sum of squares) - это то же, что и дисперсия, только мы не делим вме в конце на количество наблюдений (или количество наблюдений минус один): \\[SS = \\sum\\limits_{i=1}^{n_j} (x_{i}-\\overline{x})^2\\] Здесь много формул, но суть довольно простая: мы разделяем вариабельность зависимой переменной на внутригрупповую и межгрупповую, считаем их соотношение, которое и будет F. В среднем, F будет равен 1 при верности нулевой гипотезы. Это означает, что и межгрупповая вариабельность, и внутригрупповая вариабельность - это просто шум. Но если же межгрупповая вариабельность - это не просто шум, то это соотношение будет сильно больше единицы. Подсчет p-value. В t-тесте мы смотрели, как статистика распределена при условии верности нулевой гипотезы. То есть что будет, если нулевая гипотеза верна, мы будем повторять эксперимент с точно таким же дизайном (и размером выборок) бесконечное количество раз и считать F. betweendf &lt;- 2 withindf &lt;- 73 f &lt;- summary(aov_model)[[1]]$F[1] v &lt;- seq(0.1,10, 0.01) fdist &lt;- data.frame(fvalues = v, pdf = df(v, betweendf, withindf)) library(ggplot2) label &lt;- paste0(&quot;F(&quot;, betweendf, &quot;, &quot;, withindf, &quot;) = &quot;, round(f, 3)) ggplot(fdist, aes(x = fvalues, y = pdf))+ geom_line()+ geom_vline(xintercept = f)+ annotate(&quot;text&quot;, x = f+1, y = 0.2, label = label)+ scale_y_continuous(expand=c(0,0)) + theme_minimal()+ theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.y = element_blank()) Рисунок 17.1: F-распределение при верности нулевой гипотезы (см. детали в тексте) Заметьте, распределение F несимметричное41. Это значит, что мы всегда считаем считаем площадь от F до плюс бесконечности (без умножения на 2, как мы это делали в t-тесте): 1 - pf(f, betweendf, withindf) ## [1] 0.006595853 Это и есть наш p-value! Сравнение p-value с уровнем \\(\\alpha\\). Самый простой этап: если наш p-value меньше, чем \\(\\alpha\\) (который обычно равен 0.05), то мы отвергаем нулевую гипотезу. Если нет - не отвергаем. В нашем случае это 0.0065959, что, очевидно, меньше, чем 0.05. Отвергаем нулевую гипотезу (о том, что нет различий), принимаем ненулевую (о том, что различия есть). Все! 17.2 Post-hoc тесты Тем не менее, дисперсионного анализа недостаточно, чтобы решить, какие именно группы между собой различаются. Для этого нужно проводить post-hoc тесты (апостериорные тесты). Post-hoc переводится с латыни как “после этого.” Post-hoc тесты или просто “пост-хоки” проводятся, если в результате ANOVA была отвергнута нулевая гипотеза. Собственно, пост-хоки никак не связаны с дисперсионным анализом на уровне расчетов - это абсолютно независимые тесты, но исторически так сложилось, что они известны именно как дополнительный этап ANOVA. Самый простой вариант пост-хок теста - это попарные т-тесты42 с поправками на множественные сравнения: pairwise.t.test(diet$weight.loss, diet$Dietf) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: diet$weight.loss and diet$Dietf ## ## A B ## B 0.962 - ## C 0.017 0.017 ## ## P value adjustment method: holm Второй подход связан с использованием специализированных тестов, таких как тест Тьюки (Tukey Honest Significant Differences = Tukey HSD). Для этого в R есть функция TukeyHSD(), которую нужно применять на объект aov: TukeyHSD(aov_model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight.loss ~ Dietf, data = diet) ## ## $Dietf ## diff lwr upr p adj ## B-A 0.032000 -1.589085 1.6530850 0.9987711 ## C-A -1.848148 -3.439554 -0.2567422 0.0188047 ## C-B -1.880148 -3.454614 -0.3056826 0.0152020 17.3 ANOVA и т-тест как частные случаи линейной регрессии Как мы уже видели, если применить lm() или aov() на одних и тех же данных с одной и той же формулой, то результат будет очень похожим. Но есть одно но: lm() создает из одного фактора две переменных-предиктора: summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 Дело в том, что мы не можем просто так загнать номинативную переменную в качестве предиктора в линейную регрессию. Мы можем это легко сделать, если у нас всего два уровня в номинативном предикторе. Тогда один из уровней можно обозначить за 0, другой - за 1. Такие переменные иногда называются “бинарными.” Тогда это легко использовать в линейной регрессии: summary(lm(weight.loss ~ gender, diet)) ## ## Call: ## lm(formula = weight.loss ~ gender, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1848 -1.7264 0.2041 1.6846 5.9930 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.8930 0.3846 -10.123 1.3e-15 *** ## gender -0.1221 0.5836 -0.209 0.835 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.522 on 74 degrees of freedom ## Multiple R-squared: 0.0005914, Adjusted R-squared: -0.01291 ## F-statistic: 0.04379 on 1 and 74 DF, p-value: 0.8348 Можно ли так делать? Вполне! Допущения линейной регрессии касаются остатков, а не переменных самих по себе. Разве что это немного избыточно: линейная регрессия с бинарным предиктором - это фактически независимый t-тест: t.test(weight.loss ~ gender, diet, var.equal = TRUE) ## ## Two Sample t-test ## ## data: weight.loss by gender ## t = 0.20925, df = 74, p-value = 0.8348 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.040810 1.285067 ## sample estimates: ## mean in group 0 mean in group 1 ## -3.893023 -4.015152 Как видите, p-value совпадают! А t статистика в квадрате - это F (при двух группах): t.test(weight.loss ~ gender, diet, var.equal = TRUE)$statistic^2 ## t ## 0.04378592 Более того, те же самые результаты можно получить и с помощью коэффициента корреляции Пирсона: cor.test(diet$gender, diet$weight.loss) ## ## Pearson&#39;s product-moment correlation ## ## data: diet$gender and diet$weight.loss ## t = -0.20925, df = 74, p-value = 0.8348 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2484113 0.2022466 ## sample estimates: ## cor ## -0.02431772 Теперь должно быть понятно, почему все эти функции делают вроде бы разные статистические тесты, но выдают такой похожий результат - это фактически один и тот же метод! Все эти методы (и некоторые из тех, что будем рассматривать далее) можно рассматривать как разновидности множественной линейной регрессии.43 17.4 Dummy coding Тем не менее, вопрос остается открытым: как превратить номинативную переменную в количественную и загнать ее в регрессию? Для этого можно использовать “фиктивное кодирование” (dummy coding): diet &lt;- diet %&gt;% mutate(isA = as.numeric(Dietf == &quot;A&quot;), isB = as.numeric(Dietf == &quot;B&quot;), isC = as.numeric(Dietf == &quot;C&quot;)) diet %&gt;% group_by(Dietf) %&gt;% slice(1:2) %&gt;% select(Dietf, isA:isC) Заметьте, что такое кодирование избыточно. Если мы знаем, что диет 3, а данная диета - это не диета В и не диета С, то это диета А. Значит, одна из созданных нами колонок - “лишняя”: diet$isA &lt;- NULL Используем новую колонки для линейной регрессии и сравним результаты: summary(lm(weight.loss ~ isB + isC, diet)) ## ## Call: ## lm(formula = weight.loss ~ isB + isC, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## isB 0.0320 0.6776 0.047 0.96246 ## isC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 То же самое! 17.5 Допущения ANOVA Нормальность распределения ошибок: hist(residuals(aov_model)) Как мы видим, распределение не сильно далеко от нормального - этого вполне достаточно. ANOVA - это метод достаточно устойчивый к отклонениям от нормальности. Гомогенность дисперсий. То есть их равенство. Можно посмотреть на распределение остатков: diet$residuals &lt;- residuals(aov_model) ggplot(diet, aes(x = Dietf, y = residuals))+ geom_jitter(width = 0.1, alpha = 0.5) Все выглядит неплохо: нет какой-то одной группы, у которой разброс сильно больше или меньше. Есть и более формальные способы проверить равенство дисперсий. Например, с помощью теста Ливиня (Levene’s test). Для того, чтобы его провести, мы воспользуемся новым пакетом ez (читать как “easy”). Этот пакет сильно упрощает проведение дисперсионного анализа, особенно для более сложных дизайнов. install.packages(&quot;ez&quot;) Синтаксис довольно простой: нужно указать, данные, зависимую переменную, переменную с ID, факторы. Необходимо прописать фактор в between = или within =. В данном случае - в between =. library(ez) ez_model &lt;- ezANOVA(data = diet, dv= weight.loss, wid = Person, between = Dietf, detailed = T, return_aov = T) ez_model ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 Dietf 2 73 60.52701 410.4018 5.383104 0.006595853 * 0.1285269 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 73 2.040419 160.8859 0.4629076 0.6312856 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## Dietf Residuals ## Sum of Squares 60.5270 410.4018 ## Deg. of Freedom 2 73 ## ## Residual standard error: 2.371064 ## Estimated effects may be unbalanced Если при проведении теста Ливиня мы получаем p &lt; .05, то мы отбрасываем нулевую гипотезу о равенстве дисперсий. В данном случае мы не можем ее отбросить и поэтому принимаем44 Полученный объект (если поставить return_aov = T) содержит еще и объект aov() - на случай, если у Вас есть функции, которые работают с этим классом: TukeyHSD(ez_model$aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = formula(aov_formula), data = data) ## ## $Dietf ## diff lwr upr p adj ## B-A 0.032000 -1.589085 1.6530850 0.9987711 ## C-A -1.848148 -3.439554 -0.2567422 0.0188047 ## C-B -1.880148 -3.454614 -0.3056826 0.0152020 Примерно одинаковое количество испытуемых в разных группах. Здесь у нас все в порядке: diet %&gt;% count(Dietf) Небольшие различия в размерах групп - это ОК, тем более, что на практике такое очень часто случается: кого-то пришлось выкинуть из анализа, для какой-то строчки были потеряны данные и т.д. Однако больших различий в размерах групп стоит избегать. Самое плохое, когда группы различаются значительно по размеру (более чем в 2 раза) и вариабельность внутри групп отличается значительно (более чем в 2 раза). 17.6 Многофакторный дисперсионный анализ (Factorial ANOVA) На практике можно встретить One-Way ANOVA (однофакторную ANOVA) довольно редко. Обычно в исследованиях встречается многофакторный дисперсионный анализ, в котором проверяется влияние сразу нескольких факторов. В научных статьях это обозначается примерно так: “3х2 ANOVA.” Это означает, что был проведен двухфакторный дисперсионный анализ, причем в одном факторе было три уровня, во втором - два. В нашем случае это будут факторы “Диета” и “Пол.” Это означает, что у нас две гипотезы: о влиянии диеты на потерю веса и о влиянии пола на потерю веса. Кроме того, появляется гипотеза о взаимодействии факторов - то есть о том, что разные диеты по разному влияют на потерю веса для разных полов. Взаимодействие двух факторов хорошо видно на графике с линиями: если две линии параллельны, то взаимодействия нет. Если они не параллельны (пересекаются, сходятся, расходятся), то взаимодействие есть. diet &lt;- diet %&gt;% mutate(genderf = factor(gender, labels = c(&quot;ж&quot;, &quot;м&quot;))) #превращаем в бинарную переменную в фактор sem &lt;- function(x) sd(x)/sqrt(length(x)) #пишем функцию для стандартной ошибки pd = position_dodge(0.05) #немного раздвигаем положение точек на будущем графике diet %&gt;% group_by(Dietf, genderf) %&gt;% summarise(meanloss = mean(weight.loss), se = sem(weight.loss)) %&gt;% ggplot(aes(x = Dietf, y = meanloss, colour = genderf)) + geom_line(aes(group = genderf), position = pd) + geom_pointrange(aes(ymin = meanloss - se, ymax = meanloss + se), position = pd) + theme_minimal() Как видно по картинке, разница в эффективности диеты С по сравнению с другими видна только для женщин. ezANOVA(data = diet, dv= weight.loss, wid = Person, between = .(Dietf, gender), detailed = T, return_aov = T) ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 Dietf 2 70 60.4172197 376.329 5.61902602 0.00545568 * ## 2 gender 1 70 0.1686958 376.329 0.03137868 0.85990976 ## 3 Dietf:gender 2 70 33.9040683 376.329 3.15320438 0.04884228 * ## ges ## 1 0.138334829 ## 2 0.000448066 ## 3 0.082645860 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## Dietf gender Dietf:gender Residuals ## Sum of Squares 60.5270 0.1687 33.9041 376.3290 ## Deg. of Freedom 2 1 2 70 ## ## Residual standard error: 2.318648 ## Estimated effects may be unbalanced Итак, теперь мы проверяем три гипотезы вместо одной. Действительно, взаимодействие диеты и пола оказалось значимым, как и ожидалось. 17.7 Дисперсионный анализ с повторными измерениями (Repeated-measures ANOVA) Если обычный дисперсионный анализ - это аналог независимого т-теста для нескольких групп, то дисперсионный анализ с повторными измерениями - это аналог зависимого т-теста. В функции ezANOVA() для проведения дисперсионного анализа с повторными измерениями нужно просто поставить нужным параметром внутригрупповую переменную. Это означает, что в данном случае мы должны иметь данные в длинном формате, для чего мы воспользуемся функцией pivot_longer(): dietlong &lt;- diet %&gt;% pivot_longer(cols = c(pre.weight, weight6weeks), names_to = &quot;time&quot;, values_to = &quot;weight&quot;) dietlongC &lt;- dietlong %&gt;% filter(Dietf == &quot;C&quot;) %&gt;% droplevels() ezANOVA(dietlongC, dv = weight, wid = Person, within = time) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 time 1 26 124.6949 2.030459e-11 * 0.0986036 17.8 Смешанный дисперсионный анализ (Mixed between-within subjects ANOVA) Нам никто не мешает совмещать и внутригруппоdые, и межгрупповые факторы вместе. В ezANOVA() это делается просто с помощью прописывания разных факторов в нужных переменных: between = и within =. ezANOVA(dietlong, dv = weight, wid = Person, within = time, between = Dietf) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 Dietf 2 73 0.8280758 4.409507e-01 0.021710057 ## 3 time 1 73 210.5004045 3.346036e-23 * 0.059209996 ## 4 Dietf:time 2 73 5.3831045 6.595853e-03 * 0.003208607 Здесь нас интересует взаимодействие между факторами. Результаты, полученные для этой гипотезы, идентичны результатам по обычному дисперсионному анализу на разницу до и после - по сути это одно и то же. 17.9 Непараметрические аналоги ANOVA Как было описано выше, ANOVA довольно устойчив к разным отклонениям от нормальности и некоторой гетероскедастичности (разным дисперсиям в выборках). Но если уж у Вас данные ну совсем-совсем ненормальные, несимметричные, а от преобразований шкалы Вы по каким-то причинам отказались, то стоит упомянуть о непараметрических аналогах ANOVA. 17.9.1 Тест Краскела-Уоллеса Это тест Краскела-Уоллеса - обобщение теста Манна-Уитни на несколько выборок (т.е. аналог межгруппового ANOVA): kruskal.test(weight.loss ~ Dietf, diet) ## ## Kruskal-Wallis rank sum test ## ## data: weight.loss by Dietf ## Kruskal-Wallis chi-squared = 9.4159, df = 2, p-value = 0.009023 17.9.2 Тест Фридмана Для зависимых выборок есть тест Фридмана - непараметрический аналог дисперсионного анализа с повторными измерениями: friedman.test(weight ~ time | Person, dietlongC) ## ## Friedman rank sum test ## ## data: weight and time and Person ## Friedman chi-squared = 27, df = 1, p-value = 2.035e-07 17.10 Заключение Мы разобрали много разных вариантов дисперсионного анализа. Зачем так много? ANOVA - один из самых распространенных методов как в психологии, так и во многих других областях. Естественно, это отнюдь не все методы, используемые в той же психологии. Более того, некоторые вопросы остались за бортом. Постараюсь коротко их перечислить: многомерный ANOVA (Multivariate ANalysis Of Variance; MANOVA) - расширение ANOVA для ситуации нескольких зависимых переменных. Это довольно редкая разновидность ANOVA, который берет во внимание ковариации между зависимыми переменными. тестирование сферичности для дисперсионного анализа с повторноми измерениями с помощью теста сферичности Моучли (Mauchly’s sphericity test). Этот тест проверяет использует матрицу ковариаций разниц каждого условия с каждым: дисперсии разниц между условиями должны быть примерно одинаковыми. Если нулевая гипотеза о сферичности может быть отброшена (p-value &lt; \\(\\alpha\\)), то нужно проводить специальные поправки, обычно это поправки Гринхауса-Гейсера (Greenhouse-Geisser corrections). Мы этого не делали, потому что в ситуации RM-ANOVA с всего двумя условиями эта сферичность никогда не нарушается: у нас всего одна дисперсия разниц между условиями, которую просто-напросто не с чем сравнивать. Тест сферичности Моучли вместе с поправками Гринхауса-Гейсера проводятся автоматически для RM-ANOVA с тремя или более группами при использовании функции ezANOVA(), так что особо париться над этим не стоит. Правда, нужно помнить, что как и все подобные статистические тесты допущений, они обладают проблемами, связанными с тем, что это статистические тесты: на маленьких выборках они не заметят даже серьезных отклонений от сферичности, на больших - даже маленькие отклонения, а \\(p-value &lt; 0.05\\), по сути, не может интерпретироваться как верность нулевой гипотезы. Тем не менее, это довольно стандартная процедура. Как правильно репортить результаты дисперсионного анализа. Здесь все, конечно, зависит от стиля, используемого конкретным журналом. В психологии и близких к ней дисциплинам фактическим lingua franca является стиль Американской Психологической Ассоциации (APA). И тут у меня есть для Вас хорошие новости: есть куча пакетов в R, которые позволяют репортить результаты статистических тестов в APA-стиле! Спасибо дотошным авторам руководства APA по офромлению статей, что этот стиль настолько точно прописывает, как нужно описывать результаты исследований, что это можно запрограммировать. Я лично пользуюсь пакетом apa, он весьма удобен: install.packages(&quot;apa&quot;) library(apa) anova_apa(ez_model, format = &quot;rmarkdown&quot;) ## Dietf: *F*(2, 73) = 5.38, *p* = .007, $\\eta^2_p$ = .13 В тексте это будет выглядеть это будет вот так: Dietf: F(2, 73) = 5.38, p = .007, \\(\\eta^2_p\\) = .13 Еще есть пакеты apaStyle и papaja, которые могут даже сразу делать весь документ в APA-формате! Если же Вы описываете результаты самостоятельно вручную, то нужно помнить: ни в коем случае не описывайте только p-value. Обязательно прописывайте значение \\(F\\) и степени свободы, желательно с размером эффекта. Для post-hoc теста часто репортятся только p-value (зачастую только для статистически значимых сравнений), но обязательно нужно прописывать какие именно post-hoc тесты проводились, какой показатель размера эффекта использовался (если использовался), применялись ли тест сферичности Моучли вместе с поправками Гринхауса-Гейсера для дисперсионного анализа с повторными измерениями. Модели со смешанными эффектами (mixed-effects models) / иерархическая регрессия (hierarchical regression) / многоуровневое моделирование (multilevel modelling). очень популярный нынче метод, которому повезло иметь много названий - в зависимости от области, в которой он используется. В экспериментальной психологии обычно он называется “модели со смешанными эффектами” и позволяет включать в линейную регрессию не только фиксированные эффекты (fixed effects), но и случайные эффекты (random effects). Для экспериментальной психологии это интересно тем, что в таких моделях можно не усреднять показатели по испытуемым, а учитывать влияние группирующей переменной “испытуемый” как случайный эффект. Подобные модели используются в самых разных областях. Для их использования в R есть два известных пакета: nlme и lme4. ANOVA от ANalysis Of VAriance, по-русски часто читается как “АНОВА.”↩︎ Форма F-распределения будет сильно зависеть от числа степеней свободы. Но оно всегда определено от 0 до плюс бесконечности: в числителе и знаменателе всегда неотрицательные числа.↩︎ По умолчанию функция pairwise.t.test() использует объединенное стандартное отклонение для всех условий при расчетах. Это дает немного другие результаты, чем просто попарные t-тесты, хотя разница обычно незначительная. Чтобы отключить такое поведение функции pairwise.t.test(), можно поставить FALSE для параметра pool.sd()↩︎ Обобщением множественной линейной регрессии (вернее, одним из) можно считать общую линейную модель (general linear model). Общая линейная модель может предсказывать не одну, а сразу несколько объясняемых переменных в отличие от множественной линейной регрессии. Следующим этапом обобщения служит обобщенная линейная модель (generalized linear model). Фишка последней в том, что можно использовать не только модели с нормально распределенными остатками, но и, например, логистическую и пуассоновскую регрессию.↩︎ Вообще-то эта логика не совсем корректна. Тест Ливиня - это такой же статистический тест, как и остальные. Поэтому считать, что допущения соблюдаются на основании того, что p-value больше допустимого уровня \\(\\alpha\\), - это неправильно. Но для проверки допущений такая не очень корректная практика считается допустимой.↩︎ "],["glm-general.html", "18 Общая линейная модель и ее расширения 18.1 Общая линейная модель 18.2 Обобщенная линейная модель 18.3 Модель со смешанными эффектами", " 18 Общая линейная модель и ее расширения 18.1 Общая линейная модель Обобщением множественной линейной регрессии можно считать общую линейную модель (general linear model). Общая линейная модель может предсказывать не одну, а сразу несколько объясняемых переменных в отличие от множественной линейной регрессии. \\[Y = XB\\] где \\(Y\\) — матрица объясняемых переменных, \\(X\\) — матрица предикторов, \\(B\\) — матрица параметров. Почти все пройденные нами методы можно рассматривать как частный случай общей линейной модели: t-тесты, коэффициент корреляции Пирсона, линейная регрессия, ANOVA. 18.2 Обобщенная линейная модель Обобщенная линейная модель (generalized linear model) была придумана как обобщение линейной регрессии и ее сородичей: логистической регрессии и пуассоновской регрессии. Общая линейная модель задается формулой \\[Y = XB\\] Обобщенная оборачивает предиктор \\(XB\\) связывающей функцией (link function), которая различается для разных типов регрессионных моделей. Давайте попробуем построить модель, в которой объясняемой переменной будет то, является ли супергерой хорошим или плохим. heroes$good &lt;- heroes$Alignment == &quot;good&quot; Обычная линейная модель нам не подходит, если распределение наших ошибок далеко от нормального. А это значит, что мы не можем использовать общую линейную модель с бинарной объясняемой переменной. Эту проблему решает логистическая регрессия, которая является частным случаем обобщенной линейной модели. Для этого нам понадобится функция glm(), а не lm() как раньше. Ее синтаксис очень похож, но нам теперь нужно задать еще один важный параметр family = для выбора связывающей функции (в данном случае это логит-функция, которая является связующей функцией по умолчанию для биномиального семейства функций в glm()). heroes_good_glm &lt;- glm(good ~ Weight + Gender, heroes, family = binomial()) summary(heroes_good_glm) ## ## Call: ## glm(formula = good ~ Weight + Gender, family = binomial(), data = heroes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8674 -1.3103 0.6334 0.9155 2.4007 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.763917 0.235410 7.493 6.73e-14 *** ## Weight -0.004253 0.001124 -3.783 0.000155 *** ## GenderMale -0.760310 0.245851 -3.093 0.001984 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 605.22 on 477 degrees of freedom ## Residual deviance: 570.88 on 475 degrees of freedom ## (256 observations deleted due to missingness) ## AIC: 576.88 ## ## Number of Fisher Scoring iterations: 4 Результат очень похож по своей структуре на glm(), однако вместо \\(R^2\\) перед нами AIC. AIC расшифровывается как информационный критерий Акаике (Akaike information criterion) — это критерий использующийся для выбора из нескольких моделей. Чем он меньше, тем лучше модель. Как и Adjusted R2, AIC “наказывает” за большое количество параметров в модели. Поскольку AIC — это относительный показатель качества модели, нам нужно сравнить его с AIC другой, более общей модели. Можно сравнить с моделью без веса супергероев. heroes_good_glm_noweight &lt;- glm(good ~ Gender, heroes, family = binomial()) summary(heroes_good_glm_noweight) ## ## Call: ## glm(formula = good ~ Gender, family = binomial(), data = heroes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8082 -1.4164 0.6586 0.9559 0.9559 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.4178 0.1785 7.944 1.95e-15 *** ## GenderMale -0.8716 0.2012 -4.332 1.48e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 873.81 on 698 degrees of freedom ## Residual deviance: 853.24 on 697 degrees of freedom ## (35 observations deleted due to missingness) ## AIC: 857.24 ## ## Number of Fisher Scoring iterations: 4 AIC стал больше, следовательно, мы выберем модель с весом супергероев. 18.3 Модель со смешанными эффектами Модели со смешанными эффектами (mixed-effects models) — это то же самое, что и иерархическая регрессия (hierarchical regression) или многоуровневое моделирование (multilevel modelling). Этому методу повезло иметь много названий - в зависимости от области, в которой он используется. Модели со смешанными эффектами позволяет включать в линейную регрессию не только фиксированные эффекты (fixed effects), но и случайные эффекты (random effects). Для экспериментальных дисциплин это интересно тем, что в таких моделях можно не усреднять показатели по испытуемым или образцам, а учитывать влияние соотвествующей группирующей переменной как случайный эффект. В отличие от обычного фактора как в линейной регрессии или дисперсионном анализе (здесь он называется фиксированным), случайный эффект не интересует нас сам по себе, а его значения считаются случайной переменной. Смешанные модели используются в самых разных областях. Они позволяют решить проблему зависимости наблюдений без усреднения значений по испытуемым или группам, что повышает статистическую мощность. Для работы со смешанными моделями в R есть два известных пакета: nlme и более современный lme4. install.packages(&quot;lme4&quot;) library(lme4) Для примера возьмем данные исследования влияния депривации сна на время реакции. data(&quot;sleepstudy&quot;) Данные представлены в длинном формате: каждая строчка — это усредненное время реакции для одного испытуемого в соответствующий день эксперимента. sleepstudy %&gt;% ggplot(aes(x = Days, y = Reaction)) + geom_line() + geom_point() + scale_x_continuous(breaks = 0:9) + facet_wrap(~Subject) + theme_minimal() Можно заметить, что, в среднем, время реакции у испытуемых повышается от первого к последнему дню. С помощью смешанных моделей мы можем проверить, различается ли скорость возрастания времени реакции от дня к дню у разных испытуемых. Для этого мы сравниваем две модели, одна из которых является “вложенной” в другую, то есть усложненной версией более общей модели. В данном случае, более общая модель предполагает, что время реакции увеличивается у всех испытуемых одинаково, а испытуемые различаются только средним временем реакции. sleep_lme0 &lt;- lmer(Reaction ~ Days + (1 | Subject), sleepstudy) sleep_lme1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) Визуализируем предсказания двух моделей: sleepstudy$predicted_by_sleep_lme0 &lt;- predict(sleep_lme0) sleepstudy$predicted_by_sleep_lme1 &lt;- predict(sleep_lme1) sleepstudy %&gt;% rename(observed_reaction_time = Reaction) %&gt;% pivot_longer(cols = c(observed_reaction_time, predicted_by_sleep_lme0, predicted_by_sleep_lme1), names_to = &quot;model&quot;, values_to = &quot;Reaction&quot;) %&gt;% ggplot(aes(x = Days, y = Reaction)) + geom_line(aes(colour = model)) + #geom_line(aes(y = predicted_by_M1), colour = &quot;orange&quot;) + #geom_line(aes(y = predicted_by_M2), colour = &quot;purple&quot;) + geom_point(data = sleepstudy, alpha = 0.4) + scale_x_continuous(breaks = 0:9) + facet_wrap(~Subject) + theme_minimal() Зеленая линия (нулевая модель) имеет везде один и тот же наклон, а синяя (альтернативная модель) имеет разный наклон у всех испытуемых. Есть несколько способов сравнивать модели, например, уже знакомый нам AIC. Кроме того, можно сравнить две модели с помощью теста хи-квадрат, восполльзовавшись функцией anova(). anova(sleep_lme0, sleep_lme1) Модель со случайным наклоном прямой оказалась лучше, о чем нам говорят как более низкие AIC и BIC, так и тестирование с помощью хи-квадрат. "],["plan.html", "19 Планирования научного исследования 19.1 Спорные исследовательские практики 19.2 Вопроизводимые исследования 19.3 Статистическая мощность", " 19 Планирования научного исследования 19.1 Спорные исследовательские практики Если Вы откроете какой-нибудь более-менее классический учебник по психологии, то увидите там много результатов исследований, которые в дальнейшем не удалось воспроизвести. Эта проблема накрыла академическое психологическое сообщество относительно недавно и стала, пожалуй, самой обсуждаемой темой среди ученых-психологов. Действительно, о чем еще говорить, если половина фактов твоей науки попросту неверна? Об историческом смысле методологического кризиса, конечно же. Получается, у теорий нет никакого подтверждения, а в плане знаний о психологических фактах мы находимся примерно там же, где и психологи, которые закупали метрономы и тахистоскопы почти полтора века назад. Оказалось, что то, как устроена академическая наука, способствует публикации ложноположительных результатов. Если теоретическая гипотеза подтверждается, то это дает ученому больше плюшек, чем если гипотеза не подтверждается. Да и сами ученые как-то неосознанно хотят оказаться правыми. Ну а перепроверка предыдущих достижений в психологии оказалась не в почете: всем хочется быть первопроходцами и открывать новые неожиданные феномены, а не скрупулезно перепроверять чужие открытия. Все это привело психологию к тому, что в ней (да и не только в ней) закрепилось какое-то количество спорных исследовательских практик (questionable research practices). Спорные практики на то и спорные, что не все они такие уж плохие, многие ученые даже считают, что в них нет ничего плохого. В отличие, например, от фальсификации данных - это, очевидно, совсем плохо. Вот некоторые распространенные спорные исследовательские практики: Выбор зависимых переменных пост-фактум. По своей сути, это проблема скрытых множественных сравнений: если у нас много зависимых переменных, то можно сделать вид, что измерялось только то, на чем нашли эффект. Поэтому стоит задуматься, если возникает идея измерять все подряд на всякий случай - что конкретно предполагается обнаружить? Если конкретной гипотезы нет, то нужно так и написать, делая соответствующие поправки при анализе. Обсуждение неожиданных результатов как ожидаемых. Возможно, Вам это покажется смешным, но очень часто результаты оказываются статистически значимыми, но направлены в другую сторону, чем предполагалось в гипотезе. И в таких случаях исследователи часто пишут, как будто бы такие результаты и ожидались изначально! Приходится, правда, немного подкрутить теоретические построения. Остановка набора испытуемых при достижении уровня значимости (Optional stopping). Представьте себе, что Вы не обнаружили значимых результатов, но p-value болтается около 0.05. Тогда Вам захочется добрать парочку испытуемых. А потом еще. И еще немного. Проблема с таким подходом в том, что рано или поздно Вы получите статистически значимые результаты. В любом случае, даже если эффекта нет. На самом деле, эта проблема не так сильно влияет на результаты как может показаться, но это все-таки достаточно плохая практика, а ее применение увеличивает количество опубликованных ложно-положительных результатов. Неправильное округление до .05. Всякий раз, когда видите p = .05 будьте внимательны: вообще-то он не может быть равен именно .05. Либо автор не очень этого понимает, либо просто p больше, а не меньше .05. Например, .054, что потом округляется до .05 и преподносится как статистически значимые результаты. Использование односторонних тестов для сравнения средних. Эта практика похожа на предыдущую: исследователь получил p &gt; .05, но меньше, чем .1. Недобросовестный исследователь, который хочет опубликоваться проводит односторонний т-тест вместо двустороннего, т.е. фактически просто делит р на 2. Совсем недобросовестные даже не пишут, что они использовали односторонний т-тест, хотя по умолчанию все используют двусторонний. Данный список не претендует на полноту. Этих практик стоит избегать и других от этого отучать. Ну а если Вы думаете, что никто не заметит, если Вы так сделаете, то это не так. Например, последние две практики можно легко обнаружить с помощью сайта http://statcheck.io. Этот сайт делает магию: нужно кинуть ему статью, он распознает в ней статистические тесты и пересчитывает их. На самом деле, ничего сложного, а это сайт сделан с помощью R и уже знакомых нам инструментов: с помощью специальных пакетов из файлов вытаскивается текст, в тексте с помощью регулярных выражений находятся паттерны вроде “t(18) = -1.91, p = .036” - в большинстве журналов используется очень похожее форматирование статистических результатов. Зная степени свободы и т-статистику, можно пересчитать p-value. В данном случае это можно посчитать вот так: pt(-1.91, df = 18)*2 #умножаем на 2, потому что двусторонний тест ## [1] 0.07219987 Ну а дальше остается сравнить это с тем, что написали авторы. Например, бывает как в данном случае, что пересчитанный p-value в два раза больше того, что написали авторы. Это означает, скорее всего, что авторы использовали односторонний критерий. Если они об этом нигде не сказали, то это очень плохо. 19.2 Вопроизводимые исследования Очевидно, что перечисленных практик стоит избегать. Однако недостаточно просто сказать “ребята, давайте вы не будете пытаться во что бы то ни стало искать неожиданные результаты и публиковать их.” Поэтому сейчас предлагаются потенциальные решения пробоемы воспроизводимости исследований, которые постепенно набирают все большую популярность. Самое распространенное решение - это использование пререгистраций. Все просто: исследователь планирует исследование, какую выборку он хочет собрать, как будет обрабатывать данные, какие результаты он будет принимать как соответствующие гипотезе, а какие - нет. Получается что-то вроде научной статьи без результатов и их обсуждения, хотя можно и в более простом виде все представить: главное, есть доказательство, что исследование вы планировали провести именно так, а не подменяли все на ходу для красивых выводов. Другой способ добиться большей воспроизводимости результатов (и защиты от фальсификации) - это увеличение прозрачности в публикации данных и методов анализа. Существуют ученые (к счастью, такое встречается все реже), которые будут раздражаться, если их попросить дать вам данные. Мол, ну как же так, я их столько собирал, это же мои данные, а вот вы украдете у меня их и сделаете что-нибудь на них, опубликуете свою статью. Нет уж, мол, сами собирайте свои данные. Я терпел, и вы терпите. Очевидно, что наука - не забивание гвоздей, а ценность научной работы не обязательно пропорциональна количеству задействованных испытуемых. Собранные данные в некоторых случаях можно использовать в других исследованиях, а если все могут посмотреть исходные данные и проверить анализ, то это вообще круто и может защитить от ошибок. Конечно, не все готовы к таким разворотам в исследовательской практике. Но лучше быть готовым, потому что в какой-то момент может оказаться, что новые практики станут обязательными. И тут R будет весьма кстати: можно выкладывать данные c RMarkdown документом, который будет сразу собирать из этого статью и графики. Данные со скриптами можно выкладывать на GitHub - это удобно для коллективной работы над проектом. Другой вариант - выкладывать данные и скрипты для анализа на сайте osf.io. Это сайт специально сделанный как платформа для публикации данных исследований и скриптов для них. 19.3 Статистическая мощность Чтобы избежать optional stopping, нужно определять размер выборки заранее. Как это сделать? Наиболее корректный способ предполагает использование анализа статистической мощности (statistical power analysis). Для этого понадобится пакет pwr. install.packages(&quot;pwr&quot;) Этот пакет предоставляет семейство функций для расчета мощности, размера эффекта, уровня значимости или нужного размера выборки для разных статистических тестов. Статистическая мощность - вероятность обнаружить статистически значимый эффект, если он действительно есть. Размер эффекта - собственно, размер эффекта в исследовании, обычно в универсальных единицах. Например, размер различия средних обычно измеряется в стандартных отклонениях. Это позволяет сравнивать эффекты в разных исследованиях и даже эффекты в разных областях науки. Если задать 3 из 4 чисел (статистическая мощность - стандартно это .8, уровень значимости - стандартно .05, размер эффекта, размер выборки), то функция выдаст недостающее число. Я очень советую поиграться с этим самостоятельно. Например, если размер эффекта в единицах стандартных отклонений Cohen’s d = 2, то сколько нужно испытуемых, чтобы обнаружить эффект для двухвыборочного т-теста с вероятностью .8? library(pwr) pwr.t.test(d = 2, power = 0.8, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 5.089995 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Всеего 6 в каждой группе (округляем в большую сторону)! pwr.t.test(d = 2, power = 0.8, type = &quot;paired&quot;) ## ## Paired t test power calculation ## ## n = 4.220726 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number of *pairs* Еще меньше, если использовать within-subject дизайн исследования и зависимый т-тест. А если эффект маленький - всего d = .2? pwr.t.test(d = .2, power = 0.8, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 393.4057 ## d = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 394 испытуемых в каждой группе! Можно проверить такие расчеты самостоятельно с помощью симуляции данных. mean(replicate(1000, t.test(rnorm(394, 100, 15), rnorm(394, 103, 15))$p.value &lt; 0.05)) ## [1] 0.792 Действительно, если много раз делать случайные выборки из двух нормальных распределений с средними, отличающимися на 0.2 стандартных отклонения, то примерно в 80% случаев вы получите статистически значимые различия! "],["tasks.html", "20 Задания 20.1 Начало работы в R 20.2 Создание векторов 20.3 Приведение типов 20.4 Векторизация 20.5 Индексирование векторов 20.6 Работа с пропущенными значениями 20.7 Матрицы 20.8 Списки 20.9 Датафрейм 20.10 Условные конструкции 20.11 Создание функций 20.12 Проверка на адекватность 20.13 Семейство функций apply() 20.14 magrittr::%&gt;% 20.15 Выбор столбцов: dplyr::select() 20.16 Выбор строк: dplyr::slice() и dplyr::filter() 20.17 Сортировка строк: dplyr::arrange() 20.18 Уникальные значения: dplyr::distinct() 20.19 Создание колонок: dplyr::mutate() и dplyr::transmute() 20.20 Агрегация: dplyr::group_by() %&gt;% summarise() 20.21 Соединение датафреймов: *_join {#task_join} 20.22 Tidy data 20.23 Операции с несколькими колонками: across() 20.24 Описательная статистика 20.25 Построение графиков в ggplot2 20.26 Распределения 20.27 Одновыборочный t-test 20.28 Двухвыборочный зависимый t-test 20.29 Двухвыборочный независимый t-test 20.30 Непараметрические аналоги t-теста 20.31 Исследование набора данных Backpack 20.32 Ковариация 20.33 Коэффициент корреляции", " 20 Задания Задания, которые помечены звездочкой (*) можно пропускать: это задания повышенной сложности, в них требуется подумать над решением, а не просто применить выученные инструменты. 20.1 Начало работы в R Разделите 9801 на 9. ## [1] 1089 Посчитайте логарифм от 2176782336 по основанию 6. ## [1] 12 Теперь натуральный логарифм 10 и умножьте его на 5. ## [1] 11.51293 С помощью функции sin() посчитайте \\(\\sin (\\pi), \\sin \\left(\\frac{\\pi}{2}\\right), \\sin \\left(\\frac{\\pi}{6}\\right)\\). Значение \\(\\pi\\) - зашитая в R константа (pi). ## [1] 1.224647e-16 ## [1] 1 ## [1] 0.5 20.2 Создание векторов Создайте вектор из значений 2, 30 и 4000. ## [1] 2 30 4000 Создайте вектор от 1 до 20. ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Создайте вектор от 20 до 1. ## [1] 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 Функция sum() возвращает сумму элементов вектора на входе. Посчитайте сумму первых 100 натуральных чисел (т.е. всех целых чисел от 1 до 100). ## [1] 5050 Создайте вектор от 1 до 20 и снова до 1. Число 20 должно присутствовать только один раз! ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 19 18 17 16 15 ## [26] 14 13 12 11 10 9 8 7 6 5 4 3 2 1 Создайте вектор значений 5, 4, 3, 2, 2, 3, 4, 5: ## [1] 5 4 3 2 2 3 4 5 Создайте вектор 2, 4, 6, … , 18, 20. ## [1] 2 4 6 8 10 12 14 16 18 20 Создайте вектор 0.1, 0.2, 0.3, …, 0.9, 1. ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2020 год — високосный. Следующий високосный год через 4 года — это будет 2024 год. Составьте календарь всех високосных годов XXI века, начиная с 2020 года. 2100 год относится к XXI веку, а не к XXII. ## [1] 2020 2024 2028 2032 2036 2040 2044 2048 2052 2056 2060 2064 2068 2072 2076 ## [16] 2080 2084 2088 2092 2096 2100 Создайте вектор, состоящий из 20 повторений “Хэй!” ## [1] &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; ## [11] &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; Как я и говорил, многие функции, работающие с одним значением на входе, так же прекрасно работают и с целыми векторами. Попробуйте посчитать квадратный корень чисел от 1 до 10 с помощью функции sqrt() и сохраните результат в векторе roots. ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Давайте убедимся, что это действительно квадратные корни. Для этого возведите все значения вектора roots в квадрат! ## [1] 1 2 3 4 5 6 7 8 9 10 Если все верно, то того же самого можно добиться поэлементным умножением вектора roots на себя. ## [1] 1 2 3 4 5 6 7 8 9 10 *Создайте вектор из одной единицы, двух двоек, трех троек, …. , девяти девяток. ## [1] 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5 6 6 6 6 6 6 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 9 9 ## [39] 9 9 9 9 9 9 9 20.3 Приведение типов Сделайте вектор vec1, в котором соедините 3, а также значения \"Мой\" и \"вектор\". ## [1] &quot;3&quot; &quot;Мой&quot; &quot;вектор&quot; Попробуйте вычесть TRUE из 10. ## [1] 9 Соедините значение 10 и TRUE в вектор vec2. ## [1] 10 1 Соедините вектор vec2 и значение \"r\": ## [1] &quot;10&quot; &quot;1&quot; &quot;r&quot; Соедините значения 10, TRUE, \"r\" в вектор. ## [1] &quot;10&quot; &quot;TRUE&quot; &quot;r&quot; 20.4 Векторизация Создайте вектор p, состоящий из значений 4, 5, 6, 7, и вектор q, состоящий из 0, 1, 2, 3. ## [1] 4 5 6 7 ## [1] 0 1 2 3 Посчитайте поэлементную сумму векторов p и q: ## [1] 4 6 8 10 Посчитайте поэлементную разницу p и q: ## [1] 4 4 4 4 Поделите каждый элемент вектора p на соответствующий ему элемент вектора q: О, да, Вам нужно делить на 0! ## [1] Inf 5.000000 3.000000 2.333333 Возведите каждый элемент вектора p в степень соответствующего ему элемента вектора q: ## [1] 1 5 36 343 Умножьте каждое значение вектора p на 10. ## [1] 40 50 60 70 Создайте вектор квадратов чисел от 1 до 10: ## [1] 1 4 9 16 25 36 49 64 81 100 Создайте вектор 0, 2, 0, 4, … , 18, 0, 20. ## [1] 0 2 0 4 0 6 0 8 0 10 0 12 0 14 0 16 0 18 0 20 Создайте вектор 1, 0, 3, 0, 5, …, 17, 0, 19, 0. ## [1] 1 0 3 0 5 0 7 0 9 0 11 0 13 0 15 0 17 0 19 0 *Создайте вектор, в котором будут содержаться первые 20 степеней двойки. ## [1] 2 4 8 16 32 64 128 256 512 ## [10] 1024 2048 4096 8192 16384 32768 65536 131072 262144 ## [19] 524288 1048576 *Создайте вектор из чисел 1, 10, 100, 1000, 10000: ## [1] 1 10 100 1000 10000 *Посчитать сумму последовательности \\(\\frac{1}{1 \\cdot 2}+\\frac{1}{2 \\cdot 3}+\\frac{1}{3 \\cdot 4}+\\ldots+\\frac{1}{50 \\cdot 51}\\). ## [1] 0.9803922 *Посчитать сумму последовательности \\(\\frac{1}{2^{0}}+\\frac{1}{2^{1}}+\\frac{1}{2^{2}}+\\frac{1}{2^{3}}+\\ldots \\frac{1}{2^{20}}\\). ## [1] 1.999999 *Посчитать сумму последовательности \\(1+\\frac{4}{3}+\\frac{7}{9}+\\frac{10}{27}+\\frac{13}{81}+\\ldots+\\frac{28}{19683}\\). ## [1] 3.749174 *Сколько чисел из последовательности \\(1+\\frac{4}{3}+\\frac{7}{9}+\\frac{10}{27}+\\frac{13}{81}+\\ldots+\\frac{28}{19683}\\) больше чем 0.5? ## [1] 3 20.5 Индексирование векторов Создайте вектор troiki со значениями 3, 6, 9, …, 24, 27. ## [1] 3 6 9 12 15 18 21 24 27 Извлеките 2, 5 и 7 значения вектора troiki. ## [1] 6 15 21 Извлеките предпоследнее значение вектора troiki. ## [1] 24 Извлеките все значения вектора troiki кроме предпоследнего: ## [1] 3 6 9 12 15 18 21 27 Создайте вектор vec3: vec3 &lt;- c(3, 5, 2, 1, 8, 4, 9, 10, 3, 15, 1, 11) Найдите второй элемент вектора vec3. ## [1] 5 Верните второй и пятый элемент вектора vec3. ## [1] 5 8 Попробуйте извлечь сотое значение вектора vec3: ## [1] NA Верните все элементы вектора vec3 кроме второго элемента. ## [1] 3 2 1 8 4 9 10 3 15 1 11 Верните все элементы вектора vec3 кроме второго и пятого элемента. ## [1] 3 2 1 4 9 10 3 15 1 11 Найдите последний элемент вектора vec3. ## [1] 11 Верните все значения вектора vec3 кроме первого и последнего. ## [1] 5 2 1 8 4 9 10 3 15 1 Найдите все значения вектора vec3, которые больше 4. ## [1] 5 8 9 10 15 11 Найдите все значения вектора vec3, которые больше 4, но меньше 10. Если хотите сделать это в одну строчку, то вам помогут логические операторы! ## [1] 5 8 9 Найдите все значения вектора vec3, которые меньше 4 или больше 10. ## [1] 3 2 1 3 15 1 11 Возведите в квадрат каждое значение вектора vec3. ## [1] 9 25 4 1 64 16 81 100 9 225 1 121 *Возведите в квадрат каждое значение вектора на нечетной позиции и извлеките корень из каждого значения на четной позиции вектора vec3. Извлечение корня - это то же самое, что и возведение в степень 0.5. ## [1] 9.000000 2.236068 4.000000 1.000000 64.000000 2.000000 81.000000 ## [8] 3.162278 9.000000 3.872983 1.000000 3.316625 Создайте вектор 2, 4, 6, … , 18, 20 как минимум 2 новыми способами. Знаю, это задание может показаться бессмысленным, но это очень базовая операция, с помощью которой можно, например, разделить данные на две части. Чем больше способов Вы знаете, тем лучше! ## [1] 2 4 6 8 10 12 14 16 18 20 20.6 Работа с пропущенными значениями Создайте вектор vec4 со значениями 300, 15, 8, 2, 0, 1, 110: vec4 &lt;- c(300, 15, 8, 20, 0, 1, 110) vec4 ## [1] 300 15 8 20 0 1 110 Замените все значения vec4, которые больше 20 на NA. Проверьте полученный вектор vec4: ## [1] NA 15 8 20 0 1 NA Посчитайте сумму vec4 с помощью функции sum(). Ответ NA не считается! ## [1] 44 20.7 Матрицы Создайте матрицу 4х4, состоящую из единиц. Назовите ее M1. ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 1 1 1 ## [3,] 1 1 1 1 ## [4,] 1 1 1 1 Поменяйте все некрайние значения матрицы M1 (то есть значения на позициях [2,2], [2,3], [3,2] и [3,3]) на число 2. ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 2 2 1 ## [3,] 1 2 2 1 ## [4,] 1 1 1 1 Выделите второй и третий столбик из матрицы M1. ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 2 2 ## [4,] 1 1 Сравните (==) вторую колонку и вторую строчку матрицы M1. ## [1] TRUE TRUE TRUE TRUE *Создайте таблицу умножения (9х9) в виде матрицы. Сохраните ее в переменную mult_tab. ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 2 3 4 5 6 7 8 9 ## [2,] 2 4 6 8 10 12 14 16 18 ## [3,] 3 6 9 12 15 18 21 24 27 ## [4,] 4 8 12 16 20 24 28 32 36 ## [5,] 5 10 15 20 25 30 35 40 45 ## [6,] 6 12 18 24 30 36 42 48 54 ## [7,] 7 14 21 28 35 42 49 56 63 ## [8,] 8 16 24 32 40 48 56 64 72 ## [9,] 9 18 27 36 45 54 63 72 81 *Из матрицы mult_tab выделите подматрицу, включающую в себя только строчки с 6 по 8 и столбцы с 3 по 7. ## [,1] [,2] [,3] [,4] [,5] ## [1,] 18 24 30 36 42 ## [2,] 21 28 35 42 49 ## [3,] 24 32 40 48 56 *Создайте матрицу с логическими значениями, где TRUE, если в этом месте в таблице умножения (mult_tab) двузначное число и FALSE, если однозначное. Матрица - это почти вектор. К нему можно обращаться с единственным индексом. ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [7,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [8,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [9,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE *Создайте матрицу mult_tab2, в которой все значения tab меньше 10 заменены на 0. ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 10 12 14 16 18 ## [3,] 0 0 0 12 15 18 21 24 27 ## [4,] 0 0 12 16 20 24 28 32 36 ## [5,] 0 10 15 20 25 30 35 40 45 ## [6,] 0 12 18 24 30 36 42 48 54 ## [7,] 0 14 21 28 35 42 49 56 63 ## [8,] 0 16 24 32 40 48 56 64 72 ## [9,] 0 18 27 36 45 54 63 72 81 20.8 Списки Дан список list1: list1 = list(numbers = 1:5, letters = letters, logic = TRUE) list1 ## $numbers ## [1] 1 2 3 4 5 ## ## $letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $logic ## [1] TRUE Найдите первый элемент списка list1. Ответ должен быть списком длиной один. ## $numbers ## [1] 1 2 3 4 5 Теперь найдите содержание первого элемента списка list1 двумя разными способами. Ответ должен быть вектором. ## [1] 1 2 3 4 5 ## [1] 1 2 3 4 5 Теперь возьмите первый элемент содержания первого элемента списка list1. Ответ должен быть вектором. ## [1] 1 Создайте список list2, содержащий в себе два списка list1. Один из них будет иметь имя pupa, а другой — lupa. ## $pupa ## $pupa$numbers ## [1] 1 2 3 4 5 ## ## $pupa$letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $pupa$logic ## [1] TRUE ## ## ## $lupa ## $lupa$numbers ## [1] 1 2 3 4 5 ## ## $lupa$letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $lupa$logic ## [1] TRUE *Извлеките первый элемент списка list2, из него — второй полэлемент, а из него — третье значение. ## [1] &quot;c&quot; 20.9 Датафрейм Запустите команду data(mtcars) чтобы загрузить встроенный датафрейм с информацией про автомобили. Каждая строчка датафрейма - модель автомобиля, каждая колонка - отдельная характеристика. Подробнее см. ?mtcars. data(mtcars) mtcars Изучите структуру датафрейма mtcars с помощью функции str(). ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... Найдите значение третьей строчки четвертого столбца датафрейма mtcars. ## [1] 93 Извлеките первые шесть строчек и первые шесть столбцов датафрейма mtcars. Извлеките колонку wt датафрейма mtcars - массу автомобиля в тысячах фунтов. ## [1] 2.620 2.875 2.320 3.215 3.440 3.460 3.570 3.190 3.150 3.440 3.440 4.070 ## [13] 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 3.435 3.840 ## [25] 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780 Извлеките колонки из mtcars в следующем порядке: hp, mpg, cyl. Посчитайте количество автомобилей с 4 цилиндрами (cyl) в датафрейме mtcars. ## [1] 11 Посчитайте долю автомобилей с 4 цилиндрами (cyl) в датафрейме mtcars. ## [1] 0.34375 Найдите все автомобили мощностью не менее 100 лошадиных сил (hp) в датафрейме mtcars. Найдите все автомобили мощностью не менее 100 лошадиных сил (hp) и 4 цилиндрами (cyl) в датафрейме mtcars. Посчитайте максимальную массу (wt) автомобиля в выборке, воспользовавшись функцией max(): ## [1] 5.424 Посчитайте минимальную массу (wt) автомобиля в выборке, воспользовавшись функцией min(): ## [1] 1.513 Найдите строчку датафрейма mtcars с самым легким автомобилем. Извлеките строчки датафрейма mtcars с автомобилями, масса которых ниже средней массы. Масса автомобиля указана в тысячах фунтов. Создайте колонку wt_kg с массой автомобиля в килограммах. Результат округлите до целых значений с помощью функции round(). 1 фунт = 0.45359237 кг. 20.10 Условные конструкции Создайте вектор vec5: vec5 &lt;- c(5, 20, 30, 0, 2, 9) Создайте новый строковый вектор, где на месте чисел больше 10 в vec5 будет стоять “большое число,” а на месте остальных чисел — “маленькое число.” ## [1] &quot;маленькое число&quot; &quot;большое число&quot; &quot;большое число&quot; &quot;маленькое число&quot; ## [5] &quot;маленькое число&quot; &quot;маленькое число&quot; Загрузите файл heroes_information.csv в переменную heroes. heroes &lt;- read.csv(&quot;data/heroes_information.csv&quot;, stringsAsFactors = FALSE, na.strings = c(&quot;-&quot;, &quot;-99&quot;)) Создайте новою колонку hair в heroes, в которой будет значение \"Bold\" для тех супергероев, у которых в колонке Hair.color стоит \"No Hair\", и значение \"Hairy\" во всех остальных случаях. Создайте новою колонку tall в heroes, в которой будет значение \"tall\" для тех супергероев, у которых в колонке Height стоит число больше 190, значение \"short\" для тех супергероев, у которых в колонке Height стоит число меньше 170, и значение \"middle\" во всех остальных случаях. 20.11 Создание функций Создайте функцию plus_one(), которая принимает число и возвращает это же число + 1. Проверьте функцию plus_one() на числе 41. plus_one(41) ## [1] 42 Создайте функцию circle_area(), которая вычисляет площадь круга по радиусу согласно формуле \\(\\pi r^2\\). Посчитайте площадь круга с радиусом 5. ## [1] 78.53982 Создайте функцию cels2fahr(), которая будет превращать градусы по Цельсию в градусы по Фаренгейту. Проверьте на значениях -100, -40 и 0, что функция cels2fahr() работает корректно. cels2fahr(c(-100, -40, 0)) ## [1] -148 -40 32 Напишите функцию highlight(), которая принимает на входе строковый вектор, а возвращает тот же вектор, но дополненный значением \"***\" в начале и конце вектора. Лучше всего это рассмотреть на примере: highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;)) ## [1] &quot;***&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;***&quot; Теперь сделайте функцию highlight более гибкой. Добавьте в нее параметр wrapper =, который по умолчанию равен \"***\". Значение параметра wrapper = и будет вставлено в начало и конец вектора. Проверьте написанную функцию на векторе c(\"Я\", \"Бэтмен!\"). highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;)) ## [1] &quot;***&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;***&quot; highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;), wrapper = &quot;__&quot;) ## [1] &quot;__&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;__&quot; Создайте функцию na_n(), которая будет возвращать количество NA в векторе. Проверьте функцию na_n() на векторе: na_n(c(NA, 3:5, NA, 2, NA)) ## [1] 3 Напишите функцию factors(), которая будет возвращать все делители числа в виде числового вектора. Здесь может понадобиться оператор для получения остатка от деления: %%. Проверьте функцию factors() на простых и сложных числах: factors(3) ## [1] 1 3 factors(161) ## [1] 1 7 23 161 factors(1984) ## [1] 1 2 4 8 16 31 32 62 64 124 248 496 992 1984 *Напишите функцию is_prime(), которая проверяет, является ли число простым. Здесь может пригодиться функция any() - она возвращает TRUE, если в векторе есть хотя бы один TRUE. Проверьте какие года были для нас простыми, а какие нет: is_prime(2017) ## [1] TRUE is_prime(2019) ## [1] FALSE 2019/3 #2019 делится на 3 без остатка ## [1] 673 is_prime(2020) ## [1] FALSE is_prime(2021) ## [1] FALSE *Создайте функцию monotonic(), которая возвращает TRUE, если значения в векторе не убывают (то есть каждое следующее - больше или равно предыдущему) или не возврастают. Полезная функция для этого — diff() — возвращает разницу соседних значений. monotonic(1:7) ## [1] TRUE monotonic(c(1:5,5:1)) ## [1] FALSE monotonic(6:-1) ## [1] TRUE monotonic(c(1:5, rep(5, 10), 5:10)) ## [1] TRUE Бинарные операторы типа + или %in% тоже представляют собой функции. Более того, мы можем создавать свои бинарные операторы! В этом нет особой сложности — нужно все так же создавать функцию (для двух переменных), главное окружать их % и название обрамлять обратными штрихами `. Например, можно сделать свой бинарный оператор %notin%, который будет выдавать TRUE, если значения слева нет в векторе справа: `%notin%` &lt;- function(x, y) ! (x %in% y) 1:10 %notin% c(1, 4, 5) ## [1] FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE *Создайте бинарный оператор %without%, который будет возвращать все значения вектора слева без значений вектора справа. c(&quot;а&quot;, &quot;и&quot;, &quot;б&quot;, &quot;сидели&quot;, &quot;на&quot;, &quot;трубе&quot;) %without% c(&quot;а&quot;, &quot;б&quot;) ## [1] &quot;и&quot; &quot;сидели&quot; &quot;на&quot; &quot;трубе&quot; *Создайте бинарный оператор %between%, который будет возвращать TRUE, если значение в векторе слева накходится в диапазоне значений вектора справа: 1:10 %between% c(1, 4, 5) ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE 20.12 Проверка на адекватность Создайте функцию trim(), которая будет возвращать вектор без первого и последнего значения (вне зависимости от типа данных). Проверьте, что функция trim() работает корректно: trim(1:7) ## [1] 2 3 4 5 6 trim(letters) ## [1] &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; ## [20] &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; Теперь добавьте в функцию trim() параметр n = со значением по умолчанию 1. Этот параметр будет обозначать сколько значений нужно отрезать слева и справа от вектора. Проверьте полученную функцию: trim(letters) ## [1] &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; ## [20] &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; trim(letters, n = 2) ## [1] &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; ## [20] &quot;v&quot; &quot;w&quot; &quot;x&quot; Сделайте так, чтобы функция trim() работала корректно с n = 0, т.е. функция возвращала бы исходный вектор без изменений. trim(letters, n = 0) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; *Теперь добавьте проверку на адекватность входных данных: функция trim() должна выдавать ошибку, если n = меньше нуля или если n = слишком большой и отрезает все значения вектора: *Проверьте полученную функцию trim(): trim(1:6, 3) ## Error in trim(1:6, 3): n слишком большой! trim(1:6, -1) ## Error in trim(1:6, -1): n не может быть меньше нуля! 20.13 Семейство функций apply() Создайте матрицу M2: M2 &lt;- matrix(c(20:11, 11:20), nrow = 5) M2 ## [,1] [,2] [,3] [,4] ## [1,] 20 15 11 16 ## [2,] 19 14 12 17 ## [3,] 18 13 13 18 ## [4,] 17 12 14 19 ## [5,] 16 11 15 20 Посчитайте максимальное значение матрицы M2 по каждой строчке. ## [1] 20 19 18 19 20 Посчитайте максимальное значение матрицы M2 по каждому столбцу. ## [1] 20 15 15 20 Посчитайте среднее значение матрицы M2 по каждой строке. ## [1] 15.5 15.5 15.5 15.5 15.5 Посчитайте среднее значение матрицы M2 по каждому столбцу. ## [1] 18 13 13 18 Создайте список list3: list3 &lt;- list( a = 1:5, b = 0:20, c = 4:24, d = 6:3, e = 6:25 ) Найдите максимальное значение каждого вектора списка list3. ## a b c d e ## 5 20 24 6 25 Посчитайте сумму каждого вектора списка list3. ## a b c d e ## 15 210 294 18 310 Посчитайте длину каждого вектора списка list3. ## a b c d e ## 5 21 21 4 20 Напишите функцию max_item(), которая будет принимать на входе список, а возвращать - (первый) самый длинный его элемент. Для этого вам может понадобиться функция which.max(), которая возвращает индекс максимального значения (первого, если их несколько). Проверьте функцию max_item() на списке list3. max_item(list3) ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Теперь мы сделаем сложный список list4: list4 &lt;- list(1:3, 3:40, list3) Посчитайте длину каждого вектора в списке, в т.ч. для списка внутри. Результат должен быть списком с такой же структорой, как и изначальный список list4. Для этого может понадобиться функция rapply(): recursive lapply ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 38 ## ## [[3]] ## [[3]]$a ## [1] 5 ## ## [[3]]$b ## [1] 21 ## ## [[3]]$c ## [1] 21 ## ## [[3]]$d ## [1] 4 ## ## [[3]]$e ## [1] 20 *Загрузите набор данных heroes и посчитайте, сколько NA в каждом из столбцов. Для этого удобно использовать ранее написанную функцию na_n(). ## X name Gender Eye.color Race Hair.color Height ## 0 0 29 172 304 172 217 ## Publisher Skin.color Alignment Weight hair tall ## 0 662 7 239 172 217 *Используя ранее написанную функцию is_prime(), напишите функцию prime_numbers(), которая будет возвращать все простые числа до выбранного числа. prime_numbers(200) ## [1] 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 ## [20] 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 ## [39] 173 179 181 191 193 197 199 20.14 magrittr::%&gt;% Перепишите следующие выражения, используя %&gt;%: sqrt(sum(1:10)) ## [1] 7.416198 ## [1] 7.416198 abs(min(-5:5)) ## [1] 5 ## [1] 5 c(&quot;Корень из&quot;, 2, &quot;равен&quot;, sqrt(2)) ## [1] &quot;Корень из&quot; &quot;2&quot; &quot;равен&quot; &quot;1.4142135623731&quot; ## [1] &quot;Корень из&quot; &quot;2&quot; &quot;равен&quot; &quot;1.4142135623731&quot; 20.15 Выбор столбцов: dplyr::select() Для выполнения следующих заданий нам понадобятся датасеты heroes и powers, которые можно загрузить, используя следующие команды: library(tidyverse) heroes &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) powers &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/super_hero_powers.csv&quot;) Выберете первые 4 столбца в powers. Выберите все столбцы от Reflexes до Empathy в тиббле powers: Выберите все столбцы тиббла powers кроме первого (hero_names): 20.16 Выбор строк: dplyr::slice() и dplyr::filter() Выберите только те строчки, в которых содержится информация о супергероях тяжелее 500 кг. Выберите только те строчки, в которых содержится информация о женщинах-супергероях тяжелее 500 кг. Выберите только те строчки, в которых содержится информация о супергероях человеческой расы (\"Human\") женского пола. Из этих супергероев возьмите первые 5. 20.17 Сортировка строк: dplyr::arrange() Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки по возрастанию Height. Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки по убыванию Height. Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки сначала по Gender, затем по убыванию Height. 20.18 Уникальные значения: dplyr::distinct() Извлеките уникальные значения столбца Eye color из тиббла heroes. Извлеките уникальные значения столбца Hair color из тиббла heroes. 20.19 Создание колонок: dplyr::mutate() и dplyr::transmute() Создайте колонку height_m с ростом супергероев в метрах, затем выберите только колонки name и height_m. Создайте новою колонку hair в heroes, в которой будет значение \"Bold\" для тех супергероев, у которых в колонке Hair.color стоит \"No Hair\", и значение \"Hairy\" во всех остальных случаях. Затем выберите только колонки name, Hair color, hair. 20.20 Агрегация: dplyr::group_by() %&gt;% summarise() Посчитайте количество супергероев по расам и отсортируйте по убыванию. Извлеките первые 5 строк. Посчитайте средний пост по полу. 20.21 Соединение датафреймов: *_join {#task_join} Создайте тиббл web_creators, в котором будут супергерои, которые могут плести паутину, т.е. у них стоит TRUE в колонке Web Creation в тиббле powers. Найдите всех супергероев, которые присутствуют в heroes, но отсутствуют в powers. Ответом должен быть строковый вектор с именами супергероев. ## [1] &quot;Agent 13&quot; &quot;Alfred Pennyworth&quot; &quot;Arsenal&quot; ## [4] &quot;Batgirl III&quot; &quot;Batgirl V&quot; &quot;Beetle&quot; ## [7] &quot;Black Goliath&quot; &quot;Black Widow II&quot; &quot;Blaquesmith&quot; ## [10] &quot;Bolt&quot; &quot;Boomer&quot; &quot;Box&quot; ## [13] &quot;Box III&quot; &quot;Captain Mar-vell&quot; &quot;Cat II&quot; ## [16] &quot;Cecilia Reyes&quot; &quot;Clea&quot; &quot;Clock King&quot; ## [19] &quot;Colin Wagner&quot; &quot;Colossal Boy&quot; &quot;Corsair&quot; ## [22] &quot;Cypher&quot; &quot;Danny Cooper&quot; &quot;Darkside&quot; ## [25] &quot;ERG-1&quot; &quot;Fixer&quot; &quot;Franklin Storm&quot; ## [28] &quot;Giant-Man&quot; &quot;Giant-Man II&quot; &quot;Goliath&quot; ## [31] &quot;Goliath&quot; &quot;Goliath&quot; &quot;Guardian&quot; ## [34] &quot;Hawkwoman&quot; &quot;Hawkwoman II&quot; &quot;Hawkwoman III&quot; ## [37] &quot;Howard the Duck&quot; &quot;Jack Bauer&quot; &quot;Jesse Quick&quot; ## [40] &quot;Jessica Sanders&quot; &quot;Jigsaw&quot; &quot;Jyn Erso&quot; ## [43] &quot;Kid Flash II&quot; &quot;Kingpin&quot; &quot;Meteorite&quot; ## [46] &quot;Mister Zsasz&quot; &quot;Mogo&quot; &quot;Moloch&quot; ## [49] &quot;Morph&quot; &quot;Nite Owl II&quot; &quot;Omega Red&quot; ## [52] &quot;Paul Blart&quot; &quot;Penance&quot; &quot;Penance I&quot; ## [55] &quot;Plastic Lad&quot; &quot;Power Man&quot; &quot;Renata Soliz&quot; ## [58] &quot;Ronin&quot; &quot;Shrinking Violet&quot; &quot;Snake-Eyes&quot; ## [61] &quot;Spider-Carnage&quot; &quot;Spider-Woman II&quot; &quot;Stacy X&quot; ## [64] &quot;Thunderbird II&quot; &quot;Two-Face&quot; &quot;Vagabond&quot; ## [67] &quot;Vision II&quot; &quot;Vulcan&quot; &quot;Warbird&quot; ## [70] &quot;White Queen&quot; &quot;Wiz Kid&quot; &quot;Wondra&quot; ## [73] &quot;Wyatt Wingfoot&quot; &quot;Yellow Claw&quot; Найдите всех супергероев, которые присутствуют в powers, но отсутствуют в heroes. Ответом должен быть строковый вектор с именами супергероев. ## [1] &quot;3-D Man&quot; &quot;Bananaman&quot; &quot;Bizarro-Girl&quot; ## [4] &quot;Black Vulcan&quot; &quot;Blue Streak&quot; &quot;Bradley&quot; ## [7] &quot;Clayface&quot; &quot;Concrete&quot; &quot;Dementor&quot; ## [10] &quot;Doctor Poison&quot; &quot;Fire&quot; &quot;Hellgramite&quot; ## [13] &quot;Lara Croft&quot; &quot;Little Epic&quot; &quot;Lord Voldemort&quot; ## [16] &quot;Orion&quot; &quot;Peek-a-Boo&quot; &quot;Queen Hippolyta&quot; ## [19] &quot;Reactron&quot; &quot;SHDB&quot; &quot;Stretch Armstrong&quot; ## [22] &quot;TEST&quot; &quot;Tommy Clarke&quot; &quot;Tyrant&quot; 20.22 Tidy data Для начала создайте тиббл heroes_weight, скопировав код: heroes_weight &lt;- heroes %&gt;% filter(Publisher %in% c(&quot;DC Comics&quot;, &quot;Marvel Comics&quot;)) %&gt;% group_by(Gender, Publisher) %&gt;% summarise(weight_mean = mean(Weight, na.rm = TRUE)) %&gt;% drop_na() heroes_weight Функция drop_na() позволяет выбросить все строчки, в которых встречается NA. Превратите тиббл heroes_weight в широкий тиббл: Затем превратите его обратно в длинный тиббл: Сделайте powers длинным тибблом с тремя колонками: hero_names, power (названгие суперсилы) и has (наличие суперсилы у данного супергероя). Сделайте тиббл powers обратно широким, но с новой структурой: каждая строчка означает суперсилу, а каждая колонка - супергероя (за исключением первой колонки - названия суперсилы). 20.23 Операции с несколькими колонками: across() Посчитайте количество NA в каждой колонке, группируя по полу (Gender). Посчитайте количество NA в каждой колонке, которая заканчивается на \"color\", группируя по полу (Gender). Найдите (первую) самую длинную строчку для каждой колонки с character типом данных, группируя по полу (Gender). Для расчета количества значений в строке есть функция nchar(), для расчета индекса (первого) максимального значения есть функция which.max(). Создайте из тиббла heroes новый тиббл, в котором числовые значения Height и Weight заменены на следующие строковые значения: если у супергероя рост или вес выше среднего по колонке, то \"выше среднего\", если его/ее рост или вес ниже или равен среднему, то \"ниже среднего\". Создайте из тиббла heroes новый тиббл, в котором числовые значения Height и Weight заменены на следующие строковые значения: если у супергероя внутри соответствующей группы по полу рост или вес выше среднего по колонке, то \"выше среднего по X\", если его/ее рост или вес ниже или равен среднему внутри соответствующей группы по полу, то \"ниже среднего по X\" , где X — соответствующий пол (Gender). 20.24 Описательная статистика Для выполнения задания создайте вектор height из колонки Height датасета heroes, удалив в нем NA. Посчитайте среднее в векторе height. ## [1] 186.7263 Посчитайте усеченное среднее в векторе height с усечением 5% значений с обоих сторон. ## [1] 182.5846 Посчитайте медиану в векторе height. ## [1] 183 Посчитайте стандартное отклонение в векторе height. ## [1] 59.25189 Посчитайте межквартильный размах в векторе height. ## [1] 18 Посчитайте ассиметрию в векторе height. ## [1] 8.843432 Посчитайте эксцесс в векторе height. ## [1] 105.0297 Примените функции для получения множественных статистик на векторе height. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 15.2 173.0 183.0 186.7 191.0 975.0 Таблица 20.1: Data summary Name height Number of rows 517 Number of columns 1 _______________________ Column type frequency: numeric 1 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 186.73 59.25 15.2 173 183 191 975 ▇▁▁▁▁ 20.25 Построение графиков в ggplot2 Нарисуйте столбиковую диаграмму (geom_bar()), которая будет отражать количество супергероев издателей \"Marvel Comics\", \"DC Comics\" и всех остальных (отдельным столбиком) из датасета heroes. Добавьте к этой диаграме заливку цветом (fill =) в зависимости от распределения Gender внутри каждой группы. Сделайте так, чтобы каждый столбик был максимальной высоты (position = \"fill\"). Финализируйте график, задав ему описания осей (например, функция labs()), использовав процентную шкалу (scale_y_continuous(labels = scales::percent)) и задав тему theme_minimal(). Создайте диаграмму рассеяния для датасета heroes, для которой координаты по оси x будут взяты из колонки Height, а координаты по оси y — из колонки Weight. Удалите с графика все экстремальные значения, для которых Weight больше или равен 700 или Height больше или равен 400. (Подсказка: это можно делать как средствами ggplot2, так и функцией filter() из dplyr). Раскрасьте точки в зависимости от Gender, сделайте их полупрозрачными ( параметр alpha =). Сделайте так, чтобы координатная плоскость имела соотношение 1:1 шкал по оси x и y. Этого можно добиться с помощью функции coord_fixed(). Разделите график (facet_wrap()) на три: для \"DC Comics\",\"Marvel Comics\" и всех остальных. Используйте для графика тему theme_linedraw(). Постройте новый график (или возьмите старый) по датасетам heroes и/или powers и сделайте его некрасивым! Чем хуже у вас получится график, тем лучше. Желательно, чтобы этот график был по-прежнему графиком, а не произведением абстрактного искусства. Разница очень тонкая, но она есть. Вот несколько подсказок для этого задания: Для вдохновения посмотрите на вот эти графики. Для реально плохих графиков вам придется покопаться с настройками темы. Посмотрите подсказку по темам ?theme, попытайтесь что-то поменять в теме. Экспериментируйте с разными геомами и необычными их применениями. По изучайте дополнения к gpplot2. Попробуйте подготовить интересные данные для этого графика. 20.26 Распределения Выберите любое непрерывное распределение из представленных в базовом пакете stats или же в любом другом пакете. Найти все распределения пакета stats можно с помощью ?Distributions. Подберите для него какие-нибудь параметры или используйте параметры по умолчанию. Я возьму F-распределение с параметрами df1 = 4 и df = 10, но вы можете выбрать другое распределение. Визуализируйте функцию плотности вероятности для выбранного распределения. Визуализируйте функцию накопленной плотности распределения для выбранной функции. Визуализируйте квантильную функцию для выбранного распределения. Сделайте выборку из 100 случайных значений из выбранного распределения и постройте гистограмму (функция hist()) для полученной выборки. 20.27 Одновыборочный t-test Представьте, что наши супергерои из набора данных heroes — это выборка из генеральной совокупности всех написанных и ненаписанных супергероев. Проведите одновыборочный t-тест для веса супергероев и числа 100 — предположительного среднего веса в генеральной совокупности всех супергероев. Проинтерпретируйте результат. Проведите одновыборочный t-тест для роста супергероев и числа 185 — предположительного среднего роста в генеральной совокупности всех супергероев. Проинтерпретируйте результат. 20.28 Двухвыборочный зависимый t-test Для дальнейших заданий понадобится набор данных о результативности трех диет, который мы использовали во время занятия. diet &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/stcp-Rdataset-Diet.csv&quot;) Посчитайте двухвыборочный зависимый т-тест для остальных диет: для диеты 2 и диеты 3. Проинтерпретируйте полученные результаты. 20.29 Двухвыборочный независимый t-test Сделайте независимый t-тест для сравнения веса испытуемых двух групп после диеты, сравнив вторую и третью группу. Проинтерпретируйте результаты. Сделайте независимый t-тест для сравнения веса испытуемых двух групп после диеты, сравнив первую и третью группу. Проинтерпретируйте результаты. 20.30 Непараметрические аналоги t-теста Сравните вес первой и второй группы после диеты, используя тест Манна-Уитни. Сравните результаты теста Манна-Уитни с результатами t-теста? Проинтерпретируйте полученные результаты. Повторите задание для второй и третьей группы, а так же для первой и третьей группы. Сравните вес до и после для диеты 1, используя тест Уилкоксона. Сравните с результатами применения t-теста. Проинтерпретируйте полученные результаты. Сравните вес до и после для диеты 2 и диеты 3, используя тест Уилкоксона. Сравните с результатами применения t-теста. Проинтерпретируйте полученные результаты. 20.31 Исследование набора данных Backpack Для следующих тем нам понадобится набор данных Backpack из пакета Stat2Data. #install.packages(&quot;Stat2Data&quot;) library(Stat2Data) data(Backpack) back &lt;- Backpack %&gt;% mutate(backpack_kg = 0.45359237 * BackpackWeight, body_kg = 0.45359237 * BodyWeight) Как различается вес рюкзака в зависимости от пола? Кто весит больше? Если допустить, что выборка репрезентативна, то можно ли сделать вывод о различии по среднему весу рюкзаков в генеральной совокупности? Повторите пунктs 2 и 3 для веса самих студентов. Визуализируйте распределение этих двух переменных в зависимости от пола (используя ggplot2) Постройте диаграмму рассеяния с помощью ggplot2. Цветом закодируйте пол респондента. 20.32 Ковариация Посчитайте матрицу ковариаций для веса студентов и их рюкзаков в фунтах. Различаются ли результаты подсчета ковариации этих двух переменных от результатов подсчета ковариаций веса студентов и их рюкзаков в килограммах? Почему? 20.33 Коэффициент корреляции Посчитайте коэффициент корреляции Пирсона для веса студентов и их рюкзаков в фунтах. Различаются ли результаты подсчета коэффициента корреляции Пирсона (сам коэффициент, p-value) этих двух переменных от результатов подсчета корреляции Пирсона веса студентов и их рюкзаков в килограммах? Почему? Посчитайте коэффициент корреляции Пирсона для веса и роста супергероев из датасета heroes. Проинтерпретируйте результат. Теперь посчитайте коэффициент корреляции Спирмена и коэффициент корреляции Кэнделла для веса и роста супергероев из датасета heroes. Различаются ли результаты по сравнению с коэффициентом корреляции Пирсона? Почему? "],["solutions.html", "21 Решения заданий 21.1 Начало работы в R 21.2 Создание векторов 21.3 Приведение типов 21.4 Векторизация 21.5 Индексирование векторов 21.6 Работа с пропущенными значениями 21.7 Матрицы 21.8 Списки 21.9 Датафрейм 21.10 Условные конструкции 21.11 Создание функций 21.12 Проверка на адекватность 21.13 Семейство функций apply() 21.14 magrittr::%&gt;% 21.15 Выбор столбцов: dplyr::select() 21.16 Выбор строк: dplyr::slice() и dplyr::filter() 21.17 Сортировка строк: dplyr::arrange() 21.18 Уникальные значения: dplyr::distinct() 21.19 Создание колонок: dplyr::mutate() и dplyr::transmute() 21.20 Агрегация: dplyr::group_by() %&gt;% summarise() 21.21 Соединение датафреймов: *_join {#solution_join} 21.22 Tidy data 21.23 Операции с несколькими колонками: across() 21.24 Описательная статистика 21.25 Построение графиков в ggplot2 21.26 Распределения 21.27 Одновыборочный t-test 21.28 Двухвыборочный зависимый t-test 21.29 Двухвыборочный независимый t-test 21.30 Непараметрические аналоги t-теста 21.31 Исследование набора данных Backpack 21.32 Ковариация 21.33 Коэффициент корреляции", " 21 Решения заданий Задания, которые помечены звездочкой (*) можно пропускать: это задания повышенной сложности, в них требуется подумать над решением, а не просто применить выученные инструменты. 21.1 Начало работы в R Разделите 9801 на 9. 9801/9 ## [1] 1089 Посчитайте логарифм от 2176782336 по основанию 6. log(2176782336, 6) ## [1] 12 Теперь натуральный логарифм 10 и умножьте его на 5. log(10)*5 ## [1] 11.51293 С помощью функции sin() посчитайте \\(\\sin (\\pi), \\sin \\left(\\frac{\\pi}{2}\\right), \\sin \\left(\\frac{\\pi}{6}\\right)\\). Значение \\(\\pi\\) - зашитая в R константа (pi). sin(pi) ## [1] 1.224647e-16 sin(pi/2) ## [1] 1 sin(pi/6) ## [1] 0.5 21.2 Создание векторов Создайте вектор из значений 2, 30 и 4000. c(2, 30, 4000) ## [1] 2 30 4000 Создайте вектор от 1 до 20. 1:20 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Создайте вектор от 20 до 1. 20:1 ## [1] 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 Функция sum() возвращает сумму элементов вектора на входе. Посчитайте сумму первых 100 натуральных чисел (т.е. всех целых чисел от 1 до 100). sum(1:100) ## [1] 5050 Создайте вектор от 1 до 20 и снова до 1. Число 20 должно присутствовать только один раз! c(1:20, 19:1) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 19 18 17 16 15 ## [26] 14 13 12 11 10 9 8 7 6 5 4 3 2 1 Создайте вектор значений 5, 4, 3, 2, 2, 3, 4, 5: c(5:2, 2:5) ## [1] 5 4 3 2 2 3 4 5 Создайте вектор 2, 4, 6, … , 18, 20. seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 Создайте вектор 0.1, 0.2, 0.3, …, 0.9, 1. seq(0.1, 1, 0.1) ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 2020 год — високосный. Следующий високосный год через 4 года — это будет 2024 год. Составьте календарь всех високосных годов XXI века, начиная с 2020 года. 2100 год относится к XXI веку, а не к XXII. seq(2020, 2100, 4) ## [1] 2020 2024 2028 2032 2036 2040 2044 2048 2052 2056 2060 2064 2068 2072 2076 ## [16] 2080 2084 2088 2092 2096 2100 Создайте вектор, состоящий из 20 повторений “Хэй!” rep(&quot;Хэй!&quot;, 20) ## [1] &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; ## [11] &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; &quot;Хэй!&quot; Как я и говорил, многие функции, работающие с одним значением на входе, так же прекрасно работают и с целыми векторами. Попробуйте посчитать квадратный корень чисел от 1 до 10 с помощью функции sqrt() и сохраните результат в векторе roots. roots &lt;- sqrt(1:10) roots ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Давайте убедимся, что это действительно квадратные корни. Для этого возведите все значения вектора roots в квадрат! roots ^ 2 ## [1] 1 2 3 4 5 6 7 8 9 10 Если все верно, то того же самого можно добиться поэлементным умножением вектора roots на себя. roots * roots ## [1] 1 2 3 4 5 6 7 8 9 10 *Создайте вектор из одной единицы, двух двоек, трех троек, …. , девяти девяток. rep(1:9, 1:9) ## [1] 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5 6 6 6 6 6 6 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 9 9 ## [39] 9 9 9 9 9 9 9 21.3 Приведение типов Сделайте вектор vec1, в котором соедините 3, а также значения \"Мой\" и \"вектор\". vec1 &lt;- c(3, &quot;Мой&quot;, &quot;вектор&quot;) vec1 ## [1] &quot;3&quot; &quot;Мой&quot; &quot;вектор&quot; Попробуйте вычесть TRUE из 10. 10 - TRUE ## [1] 9 Соедините значение 10 и TRUE в вектор vec2. vec2 &lt;- c(10, TRUE) vec2 ## [1] 10 1 Соедините вектор vec2 и значение \"r\": c(vec2, &quot;r&quot;) ## [1] &quot;10&quot; &quot;1&quot; &quot;r&quot; Соедините значения 10, TRUE, \"r\" в вектор. c(10, TRUE, &quot;r&quot;) ## [1] &quot;10&quot; &quot;TRUE&quot; &quot;r&quot; 21.4 Векторизация Создайте вектор p, состоящий из значений 4, 5, 6, 7, и вектор q, состоящий из 0, 1, 2, 3. p &lt;- 4:7 p ## [1] 4 5 6 7 q &lt;- 0:3 q ## [1] 0 1 2 3 Посчитайте поэлементную сумму векторов p и q: p + q ## [1] 4 6 8 10 Посчитайте поэлементную разницу p и q: p - q ## [1] 4 4 4 4 Поделите каждый элемент вектора p на соответствующий ему элемент вектора q: О, да, Вам нужно делить на 0! p / q ## [1] Inf 5.000000 3.000000 2.333333 Возведите каждый элемент вектора p в степень соответствующего ему элемента вектора q: p ^ q ## [1] 1 5 36 343 Умножьте каждое значение вектора p на 10. p * 10 ## [1] 40 50 60 70 Создайте вектор квадратов чисел от 1 до 10: (1:10)^2 ## [1] 1 4 9 16 25 36 49 64 81 100 Создайте вектор 0, 2, 0, 4, … , 18, 0, 20. 1:20 * 0:1 ## [1] 0 2 0 4 0 6 0 8 0 10 0 12 0 14 0 16 0 18 0 20 Создайте вектор 1, 0, 3, 0, 5, …, 17, 0, 19, 0. 1:20 * 1:0 ## [1] 1 0 3 0 5 0 7 0 9 0 11 0 13 0 15 0 17 0 19 0 *Создайте вектор, в котором будут содержаться первые 20 степеней двойки. 2 ^ (1:20) ## [1] 2 4 8 16 32 64 128 256 512 ## [10] 1024 2048 4096 8192 16384 32768 65536 131072 262144 ## [19] 524288 1048576 *Создайте вектор из чисел 1, 10, 100, 1000, 10000: 10 ^ (0:4) ## [1] 1 10 100 1000 10000 *Посчитать сумму последовательности \\(\\frac{1}{1 \\cdot 2}+\\frac{1}{2 \\cdot 3}+\\frac{1}{3 \\cdot 4}+\\ldots+\\frac{1}{50 \\cdot 51}\\). sum(1 / (1:50 * 2:51)) ## [1] 0.9803922 *Посчитать сумму последовательности \\(\\frac{1}{2^{0}}+\\frac{1}{2^{1}}+\\frac{1}{2^{2}}+\\frac{1}{2^{3}}+\\ldots \\frac{1}{2^{20}}\\). sum(1 / 2 ^ (0:20)) ## [1] 1.999999 *Посчитать сумму последовательности \\(1+\\frac{4}{3}+\\frac{7}{9}+\\frac{10}{27}+\\frac{13}{81}+\\ldots+\\frac{28}{19683}\\). sum((3 * (1:10) - 2) / 3 ^ (0:9)) ## [1] 3.749174 *Сколько чисел из последовательности \\(1+\\frac{4}{3}+\\frac{7}{9}+\\frac{10}{27}+\\frac{13}{81}+\\ldots+\\frac{28}{19683}\\) больше чем 0.5? sum((3 * (1:10) - 2) / 3 ^ (0:9) &gt; 0.5) ## [1] 3 21.5 Индексирование векторов Создайте вектор troiki со значениями 3, 6, 9, …, 24, 27. troiki &lt;- seq(3, 27, 3) troiki ## [1] 3 6 9 12 15 18 21 24 27 Извлеките 2, 5 и 7 значения вектора troiki. troiki[c(2, 5, 7)] ## [1] 6 15 21 Извлеките предпоследнее значение вектора troiki. troiki[length(troiki) - 1] ## [1] 24 Извлеките все значения вектора troiki кроме предпоследнего: troiki[-(length(troiki) - 1)] ## [1] 3 6 9 12 15 18 21 27 Создайте вектор vec3: vec3 &lt;- c(3, 5, 2, 1, 8, 4, 9, 10, 3, 15, 1, 11) Найдите второй элемент вектора vec3. vec3[2] ## [1] 5 Верните второй и пятый элемент вектора vec3. vec3[c(2, 5)] ## [1] 5 8 Попробуйте извлечь сотое значение вектора vec3: vec3[100] ## [1] NA Верните все элементы вектора vec3 кроме второго элемента. vec3[-2] ## [1] 3 2 1 8 4 9 10 3 15 1 11 Верните все элементы вектора vec3 кроме второго и пятого элемента. vec3[c(-2, -5)] ## [1] 3 2 1 4 9 10 3 15 1 11 Найдите последний элемент вектора vec3. vec3[length(vec3)] ## [1] 11 Верните все значения вектора vec3 кроме первого и последнего. vec3[c(-1, -length(vec3))] ## [1] 5 2 1 8 4 9 10 3 15 1 Найдите все значения вектора vec3, которые больше 4. vec3[vec3 &gt; 4] ## [1] 5 8 9 10 15 11 Найдите все значения вектора vec3, которые больше 4, но меньше 10. Если хотите сделать это в одну строчку, то вам помогут логические операторы! vec3[vec3 &gt; 4 &amp; vec3 &lt; 10] ## [1] 5 8 9 Найдите все значения вектора vec3, которые меньше 4 или больше 10. vec3[vec3 &lt; 4 | vec3 &gt; 10] ## [1] 3 2 1 3 15 1 11 Возведите в квадрат каждое значение вектора vec3. vec3 ^ 2 ## [1] 9 25 4 1 64 16 81 100 9 225 1 121 *Возведите в квадрат каждое значение вектора на нечетной позиции и извлеките корень из каждого значения на четной позиции вектора vec3. Извлечение корня - это то же самое, что и возведение в степень 0.5. vec3 ^ c(2, 0.5) ## [1] 9.000000 2.236068 4.000000 1.000000 64.000000 2.000000 81.000000 ## [8] 3.162278 9.000000 3.872983 1.000000 3.316625 Создайте вектор 2, 4, 6, … , 18, 20 как минимум 2 новыми способами. Знаю, это задание может показаться бессмысленным, но это очень базовая операция, с помощью которой можно, например, разделить данные на две части. Чем больше способов Вы знаете, тем лучше! (1:20)[c(FALSE,TRUE)] ## [1] 2 4 6 8 10 12 14 16 18 20 #(1:10)*2 21.6 Работа с пропущенными значениями Создайте вектор vec4 со значениями 300, 15, 8, 2, 0, 1, 110: vec4 &lt;- c(300, 15, 8, 20, 0, 1, 110) vec4 ## [1] 300 15 8 20 0 1 110 Замените все значения vec4, которые больше 20 на NA. vec4[vec4 &gt; 20] &lt;- NA Проверьте полученный вектор vec4: vec4 ## [1] NA 15 8 20 0 1 NA Посчитайте сумму vec4 с помощью функции sum(). Ответ NA не считается! sum(vec4, na.rm = TRUE) ## [1] 44 21.7 Матрицы Создайте матрицу 4х4, состоящую из единиц. Назовите ее M1. M1 &lt;- matrix(rep(1, 16), ncol = 4) M1 ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 1 1 1 ## [3,] 1 1 1 1 ## [4,] 1 1 1 1 Поменяйте все некрайние значения матрицы M1 (то есть значения на позициях [2,2], [2,3], [3,2] и [3,3]) на число 2. M1[2:3, 2:3] &lt;- 2 M1 ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 2 2 1 ## [3,] 1 2 2 1 ## [4,] 1 1 1 1 Выделите второй и третий столбик из матрицы M1. M1[,2:3] ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 2 2 ## [4,] 1 1 Сравните (==) вторую колонку и вторую строчку матрицы M1. M1[,2] == M1[2,] ## [1] TRUE TRUE TRUE TRUE *Создайте таблицу умножения (9х9) в виде матрицы. Сохраните ее в переменную mult_tab. mult_tab &lt;- matrix(rep(1:9, rep(9,9))*(1:9), nrow = 9) mult_tab ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 2 3 4 5 6 7 8 9 ## [2,] 2 4 6 8 10 12 14 16 18 ## [3,] 3 6 9 12 15 18 21 24 27 ## [4,] 4 8 12 16 20 24 28 32 36 ## [5,] 5 10 15 20 25 30 35 40 45 ## [6,] 6 12 18 24 30 36 42 48 54 ## [7,] 7 14 21 28 35 42 49 56 63 ## [8,] 8 16 24 32 40 48 56 64 72 ## [9,] 9 18 27 36 45 54 63 72 81 #Еще #outer(1:9, 1:9, &quot;*&quot;) #1:9 %o% 1:9 *Из матрицы mult_tab выделите подматрицу, включающую в себя только строчки с 6 по 8 и столбцы с 3 по 7. mult_tab[6:8, 3:7] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 18 24 30 36 42 ## [2,] 21 28 35 42 49 ## [3,] 24 32 40 48 56 *Создайте матрицу с логическими значениями, где TRUE, если в этом месте в таблице умножения (mult_tab) двузначное число и FALSE, если однозначное. Матрица - это почти вектор. К нему можно обращаться с единственным индексом. mult_tab &gt;= 10 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE ## [4,] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [7,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [8,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [9,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE *Создайте матрицу mult_tab2, в которой все значения tab меньше 10 заменены на 0. mult_tab2 &lt;- mult_tab mult_tab2[mult_tab &lt; 10] &lt;- 0 mult_tab2 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 10 12 14 16 18 ## [3,] 0 0 0 12 15 18 21 24 27 ## [4,] 0 0 12 16 20 24 28 32 36 ## [5,] 0 10 15 20 25 30 35 40 45 ## [6,] 0 12 18 24 30 36 42 48 54 ## [7,] 0 14 21 28 35 42 49 56 63 ## [8,] 0 16 24 32 40 48 56 64 72 ## [9,] 0 18 27 36 45 54 63 72 81 21.8 Списки Дан список list1: list1 = list(numbers = 1:5, letters = letters, logic = TRUE) list1 ## $numbers ## [1] 1 2 3 4 5 ## ## $letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $logic ## [1] TRUE Найдите первый элемент списка list1. Ответ должен быть списком длиной один. list1[1] ## $numbers ## [1] 1 2 3 4 5 Теперь найдите содержание первого элемента списка list1 двумя разными способами. Ответ должен быть вектором. list1[[1]] ## [1] 1 2 3 4 5 list1$numbers ## [1] 1 2 3 4 5 Теперь возьмите первый элемент содержания первого элемента списка list1. Ответ должен быть вектором. list1[[1]][1] ## [1] 1 Создайте список list2, содержащий в себе два списка list1. Один из них будет иметь имя pupa, а другой — lupa. list2 = list(pupa = list1, lupa = list1) list2 ## $pupa ## $pupa$numbers ## [1] 1 2 3 4 5 ## ## $pupa$letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $pupa$logic ## [1] TRUE ## ## ## $lupa ## $lupa$numbers ## [1] 1 2 3 4 5 ## ## $lupa$letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $lupa$logic ## [1] TRUE *Извлеките первый элемент списка list2, из него — второй полэлемент, а из него — третье значение. list2[[1]][[2]][3] ## [1] &quot;c&quot; 21.9 Датафрейм Запустите команду data(mtcars) чтобы загрузить встроенный датафрейм с информацией про автомобили. Каждая строчка датафрейма - модель автомобиля, каждая колонка - отдельная характеристика. Подробнее см. ?mtcars. data(mtcars) mtcars Изучите структуру датафрейма mtcars с помощью функции str(). str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... Найдите значение третьей строчки четвертого столбца датафрейма mtcars. mtcars[3, 4] ## [1] 93 Извлеките первые шесть строчек и первые шесть столбцов датафрейма mtcars. mtcars[1:6, 1:6] Извлеките колонку wt датафрейма mtcars - массу автомобиля в тысячах фунтов. mtcars$wt ## [1] 2.620 2.875 2.320 3.215 3.440 3.460 3.570 3.190 3.150 3.440 3.440 4.070 ## [13] 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 3.435 3.840 ## [25] 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780 Извлеките колонки из mtcars в следующем порядке: hp, mpg, cyl. mtcars[, c(&quot;hp&quot;, &quot;mpg&quot;, &quot;cyl&quot;)] Посчитайте количество автомобилей с 4 цилиндрами (cyl) в датафрейме mtcars. sum(mtcars$cyl == 4) ## [1] 11 Посчитайте долю автомобилей с 4 цилиндрами (cyl) в датафрейме mtcars. mean(mtcars$cyl == 4) ## [1] 0.34375 Найдите все автомобили мощностью не менее 100 лошадиных сил (hp) в датафрейме mtcars. mtcars[mtcars$hp &gt;= 100, ] Найдите все автомобили мощностью не менее 100 лошадиных сил (hp) и 4 цилиндрами (cyl) в датафрейме mtcars. mtcars[mtcars$hp &gt;= 100 &amp; mtcars$cyl == 4, ] Посчитайте максимальную массу (wt) автомобиля в выборке, воспользовавшись функцией max(): max(mtcars$wt) ## [1] 5.424 Посчитайте минимальную массу (wt) автомобиля в выборке, воспользовавшись функцией min(): min(mtcars$wt) ## [1] 1.513 Найдите строчку датафрейма mtcars с самым легким автомобилем. mtcars[mtcars$wt == min(mtcars$wt), ] Извлеките строчки датафрейма mtcars с автомобилями, масса которых ниже средней массы. mtcars[mtcars$wt &lt; mean(mtcars$wt), ] Масса автомобиля указана в тысячах фунтов. Создайте колонку wt_kg с массой автомобиля в килограммах. Результат округлите до целых значений с помощью функции round(). 1 фунт = 0.45359237 кг. mtcars$wt_kg &lt;- round(mtcars$wt * 1000 * 0.45359237) mtcars 21.10 Условные конструкции Создайте вектор vec5: vec5 &lt;- c(5, 20, 30, 0, 2, 9) Создайте новый строковый вектор, где на месте чисел больше 10 в vec5 будет стоять “большое число,” а на месте остальных чисел — “маленькое число.” ifelse(vec5 &gt; 10, &quot;большое число&quot;, &quot;маленькое число&quot;) ## [1] &quot;маленькое число&quot; &quot;большое число&quot; &quot;большое число&quot; &quot;маленькое число&quot; ## [5] &quot;маленькое число&quot; &quot;маленькое число&quot; Загрузите файл heroes_information.csv в переменную heroes. heroes &lt;- read.csv(&quot;data/heroes_information.csv&quot;, stringsAsFactors = FALSE, na.strings = c(&quot;-&quot;, &quot;-99&quot;)) Создайте новою колонку hair в heroes, в которой будет значение \"Bold\" для тех супергероев, у которых в колонке Hair.color стоит \"No Hair\", и значение \"Hairy\" во всех остальных случаях. heroes$hair &lt;- ifelse(heroes$Hair.color == &quot;No Hair&quot;, &quot;Bold&quot;, &quot;Hairy&quot;) head(heroes) Создайте новою колонку tall в heroes, в которой будет значение \"tall\" для тех супергероев, у которых в колонке Height стоит число больше 190, значение \"short\" для тех супергероев, у которых в колонке Height стоит число меньше 170, и значение \"middle\" во всех остальных случаях. # heroes$tall &lt;- dplyr::case_when( # heroes$Height &gt; 190 ~ &quot;tall&quot;, # heroes$Height &lt; 170 ~ &quot;short&quot;, # TRUE ~ &quot;middle&quot; # ) heroes$tall &lt;- ifelse(heroes$Height &gt; 190, &quot;tall&quot;, ifelse(heroes$Height &lt; 170, &quot;short&quot;, &quot;middle&quot;)) 21.11 Создание функций Создайте функцию plus_one(), которая принимает число и возвращает это же число + 1. plus_one &lt;- function(x) x + 1 Проверьте функцию plus_one() на числе 41. plus_one(41) ## [1] 42 Создайте функцию circle_area(), которая вычисляет площадь круга по радиусу согласно формуле \\(\\pi r^2\\). circle_area &lt;- function(r) pi * r ^ 2 Посчитайте площадь круга с радиусом 5. circle_area(5) ## [1] 78.53982 Создайте функцию cels2fahr(), которая будет превращать градусы по Цельсию в градусы по Фаренгейту. cels2fahr &lt;- function(x) x * 9 / 5 + 32 Проверьте на значениях -100, -40 и 0, что функция cels2fahr() работает корректно. cels2fahr(c(-100, -40, 0)) ## [1] -148 -40 32 Напишите функцию highlight(), которая принимает на входе строковый вектор, а возвращает тот же вектор, но дополненный значением \"***\" в начале и конце вектора. Лучше всего это рассмотреть на примере: highlight &lt;- function(x) c(&quot;***&quot;, x, &quot;***&quot;) highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;)) ## [1] &quot;***&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;***&quot; Теперь сделайте функцию highlight более гибкой. Добавьте в нее параметр wrapper =, который по умолчанию равен \"***\". Значение параметра wrapper = и будет вставлено в начало и конец вектора. highlight &lt;- function(x, wrapper = &quot;***&quot;) c(wrapper, x, wrapper) Проверьте написанную функцию на векторе c(\"Я\", \"Бэтмен!\"). highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;)) ## [1] &quot;***&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;***&quot; highlight(c(&quot;Я&quot;, &quot;Бэтмен!&quot;), wrapper = &quot;__&quot;) ## [1] &quot;__&quot; &quot;Я&quot; &quot;Бэтмен!&quot; &quot;__&quot; Создайте функцию na_n(), которая будет возвращать количество NA в векторе. na_n &lt;- function(x) sum(is.na(x)) Проверьте функцию na_n() на векторе: na_n(c(NA, 3:5, NA, 2, NA)) ## [1] 3 Напишите функцию factors(), которая будет возвращать все делители числа в виде числового вектора. Здесь может понадобиться оператор для получения остатка от деления: %%. factors &lt;- function(x) (1:x)[x %% (1:x) == 0] Проверьте функцию factors() на простых и сложных числах: factors(3) ## [1] 1 3 factors(161) ## [1] 1 7 23 161 factors(1984) ## [1] 1 2 4 8 16 31 32 62 64 124 248 496 992 1984 *Напишите функцию is_prime(), которая проверяет, является ли число простым. Здесь может пригодиться функция any() - она возвращает TRUE, если в векторе есть хотя бы один TRUE. is_prime &lt;- function(x) !any(x%%(2:(x-1)) == 0) #is_prime &lt;- function(x) length(factors(x)) == 2 #Используя уже написанную функцию factors() Проверьте какие года были для нас простыми, а какие нет: is_prime(2017) ## [1] TRUE is_prime(2019) ## [1] FALSE 2019/3 #2019 делится на 3 без остатка ## [1] 673 is_prime(2020) ## [1] FALSE is_prime(2021) ## [1] FALSE *Создайте функцию monotonic(), которая возвращает TRUE, если значения в векторе не убывают (то есть каждое следующее - больше или равно предыдущему) или не возврастают. Полезная функция для этого — diff() — возвращает разницу соседних значений. monotonic &lt;- function(x) all(diff(x)&gt;=0) | all(diff(x)&lt;=0) monotonic(1:7) ## [1] TRUE monotonic(c(1:5,5:1)) ## [1] FALSE monotonic(6:-1) ## [1] TRUE monotonic(c(1:5, rep(5, 10), 5:10)) ## [1] TRUE Бинарные операторы типа + или %in% тоже представляют собой функции. Более того, мы можем создавать свои бинарные операторы! В этом нет особой сложности — нужно все так же создавать функцию (для двух переменных), главное окружать их % и название обрамлять обратными штрихами `. Например, можно сделать свой бинарный оператор %notin%, который будет выдавать TRUE, если значения слева нет в векторе справа: `%notin%` &lt;- function(x, y) ! (x %in% y) 1:10 %notin% c(1, 4, 5) ## [1] FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE *Создайте бинарный оператор %without%, который будет возвращать все значения вектора слева без значений вектора справа. `%without%` &lt;- function(x, y) x[!x %in% y] c(&quot;а&quot;, &quot;и&quot;, &quot;б&quot;, &quot;сидели&quot;, &quot;на&quot;, &quot;трубе&quot;) %without% c(&quot;а&quot;, &quot;б&quot;) ## [1] &quot;и&quot; &quot;сидели&quot; &quot;на&quot; &quot;трубе&quot; *Создайте бинарный оператор %between%, который будет возвращать TRUE, если значение в векторе слева накходится в диапазоне значений вектора справа: `%between%` &lt;- function(x, y) x &gt;= min(y) &amp; x &lt;= max(y) 1:10 %between% c(1, 4, 5) ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE 21.12 Проверка на адекватность Создайте функцию trim(), которая будет возвращать вектор без первого и последнего значения (вне зависимости от типа данных). trim &lt;- function(x) x[c(-1, -length(x))] Проверьте, что функция trim() работает корректно: trim(1:7) ## [1] 2 3 4 5 6 trim(letters) ## [1] &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; ## [20] &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; Теперь добавьте в функцию trim() параметр n = со значением по умолчанию 1. Этот параметр будет обозначать сколько значений нужно отрезать слева и справа от вектора. trim &lt;- function(x, n = 1) x[c(-1:-n, (-length(x)+n-1):-length(x))] Проверьте полученную функцию: trim(letters) ## [1] &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; ## [20] &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; trim(letters, n = 2) ## [1] &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; ## [20] &quot;v&quot; &quot;w&quot; &quot;x&quot; Сделайте так, чтобы функция trim() работала корректно с n = 0, т.е. функция возвращала бы исходный вектор без изменений. trim &lt;- function(x, n = 1) { if (n == 0) return(x) x[c(-1:-n, (-length(x)+n-1):-length(x))] } trim(letters, n = 0) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; *Теперь добавьте проверку на адекватность входных данных: функция trim() должна выдавать ошибку, если n = меньше нуля или если n = слишком большой и отрезает все значения вектора: trim &lt;- function(x, n = 1) { if (n &lt; 0) stop(&quot;n не может быть меньше нуля!&quot;) l &lt;- length(x) if (n &gt; ceiling(l/2) - 1) stop(&quot;n слишком большой!&quot;) if (n == 0) return(x) x[c(-1:-n, (-l+n-1):-l)] } *Проверьте полученную функцию trim(): trim(1:6, 3) ## Error in trim(1:6, 3): n слишком большой! trim(1:6, -1) ## Error in trim(1:6, -1): n не может быть меньше нуля! 21.13 Семейство функций apply() Создайте матрицу M2: M2 &lt;- matrix(c(20:11, 11:20), nrow = 5) M2 ## [,1] [,2] [,3] [,4] ## [1,] 20 15 11 16 ## [2,] 19 14 12 17 ## [3,] 18 13 13 18 ## [4,] 17 12 14 19 ## [5,] 16 11 15 20 Посчитайте максимальное значение матрицы M2 по каждой строчке. apply(M2, 1, max) ## [1] 20 19 18 19 20 Посчитайте максимальное значение матрицы M2 по каждому столбцу. apply(M2, 2, max) ## [1] 20 15 15 20 Посчитайте среднее значение матрицы M2 по каждой строке. apply(M2, 1, mean) ## [1] 15.5 15.5 15.5 15.5 15.5 Посчитайте среднее значение матрицы M2 по каждому столбцу. apply(M2, 2, mean) ## [1] 18 13 13 18 Создайте список list3: list3 &lt;- list( a = 1:5, b = 0:20, c = 4:24, d = 6:3, e = 6:25 ) Найдите максимальное значение каждого вектора списка list3. sapply(list3, max) ## a b c d e ## 5 20 24 6 25 Посчитайте сумму каждого вектора списка list3. sapply(list3, sum) ## a b c d e ## 15 210 294 18 310 Посчитайте длину каждого вектора списка list3. sapply(list3, length) ## a b c d e ## 5 21 21 4 20 Напишите функцию max_item(), которая будет принимать на входе список, а возвращать - (первый) самый длинный его элемент. Для этого вам может понадобиться функция which.max(), которая возвращает индекс максимального значения (первого, если их несколько). max_item &lt;- function (x) x[[which.max(sapply(x, length))]] Проверьте функцию max_item() на списке list3. max_item(list3) ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Теперь мы сделаем сложный список list4: list4 &lt;- list(1:3, 3:40, list3) Посчитайте длину каждого вектора в списке, в т.ч. для списка внутри. Результат должен быть списком с такой же структорой, как и изначальный список list4. Для этого может понадобиться функция rapply(): recursive lapply rapply(list4, length, how = &quot;list&quot;) ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 38 ## ## [[3]] ## [[3]]$a ## [1] 5 ## ## [[3]]$b ## [1] 21 ## ## [[3]]$c ## [1] 21 ## ## [[3]]$d ## [1] 4 ## ## [[3]]$e ## [1] 20 *Загрузите набор данных heroes и посчитайте, сколько NA в каждом из столбцов. Для этого удобно использовать ранее написанную функцию na_n(). sapply(heroes, na_n) ## X name Gender Eye.color Race Hair.color Height ## 0 0 29 172 304 172 217 ## Publisher Skin.color Alignment Weight hair tall ## 0 662 7 239 172 217 *Используя ранее написанную функцию is_prime(), напишите функцию prime_numbers(), которая будет возвращать все простые числа до выбранного числа. is_prime &lt;- function(x) !any(x %% (2:(x - 1)) == 0) prime_numbers &lt;- function(x) (2:x)[sapply(2:x, is_prime)] prime_numbers(200) ## [1] 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 ## [20] 73 79 83 89 97 101 103 107 109 113 127 131 137 139 149 151 157 163 167 ## [39] 173 179 181 191 193 197 199 21.14 magrittr::%&gt;% Перепишите следующие выражения, используя %&gt;%: sqrt(sum(1:10)) ## [1] 7.416198 1:10 %&gt;% sum() %&gt;% sqrt() ## [1] 7.416198 abs(min(-5:5)) ## [1] 5 -5:5 %&gt;% min() %&gt;% abs() ## [1] 5 c(&quot;Корень из&quot;, 2, &quot;равен&quot;, sqrt(2)) ## [1] &quot;Корень из&quot; &quot;2&quot; &quot;равен&quot; &quot;1.4142135623731&quot; 2 %&gt;% c(&quot;Корень из&quot;, ., &quot;равен&quot;, sqrt(.)) ## [1] &quot;Корень из&quot; &quot;2&quot; &quot;равен&quot; &quot;1.4142135623731&quot; 21.15 Выбор столбцов: dplyr::select() Для выполнения следующих заданий нам понадобятся датасеты heroes и powers, которые можно загрузить, используя следующие команды: library(tidyverse) heroes &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/heroes_information.csv&quot;, na = c(&quot;-&quot;, &quot;-99&quot;)) powers &lt;- read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/super_hero_powers.csv&quot;) Выберете первые 4 столбца в powers. powers %&gt;% select(1:4) Выберите все столбцы от Reflexes до Empathy в тиббле powers: powers %&gt;% select(Reflexes:Empathy) Выберите все столбцы тиббла powers кроме первого (hero_names): powers %&gt;% select(!hero_names) 21.16 Выбор строк: dplyr::slice() и dplyr::filter() Выберите только те строчки, в которых содержится информация о супергероях тяжелее 500 кг. heroes %&gt;% filter(Weight &gt; 500) Выберите только те строчки, в которых содержится информация о женщинах-супергероях тяжелее 500 кг. heroes %&gt;% filter(Weight &gt; 500 &amp; Gender == &quot;Female&quot;) Выберите только те строчки, в которых содержится информация о супергероях человеческой расы (\"Human\") женского пола. Из этих супергероев возьмите первые 5. heroes %&gt;% filter(Race == &quot;Human&quot; &amp; Gender == &quot;Female&quot;) %&gt;% slice(1:5) 21.17 Сортировка строк: dplyr::arrange() Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки по возрастанию Height. heroes %&gt;% select(name, Gender, Height) %&gt;% arrange(Height) Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки по убыванию Height. heroes %&gt;% select(name, Gender, Height) %&gt;% arrange(desc(Height)) Выберите из тиббла heroes колонки name, Gender, Height и отсортируйте строчки сначала по Gender, затем по убыванию Height. heroes %&gt;% select(name, Gender, Height) %&gt;% arrange(Gender, desc(Height)) 21.18 Уникальные значения: dplyr::distinct() Извлеките уникальные значения столбца Eye color из тиббла heroes. heroes %&gt;% distinct(`Eye color`) Извлеките уникальные значения столбца Hair color из тиббла heroes. heroes %&gt;% distinct(`Hair color`) 21.19 Создание колонок: dplyr::mutate() и dplyr::transmute() Создайте колонку height_m с ростом супергероев в метрах, затем выберите только колонки name и height_m. heroes %&gt;% mutate(height_m = Height/100) %&gt;% select(name, height_m) Создайте новою колонку hair в heroes, в которой будет значение “Bold” для тех супергероев, у которых в колонке Hair.color стоит “No Hair,” и значение “Hairy” во всех остальных случаях. Затем выберите только колонки name, Hair color, hair. heroes %&gt;% mutate(hair = ifelse(`Hair color` == &quot;No Hair&quot;, &quot;Bold&quot;, &quot;Hairy&quot;)) %&gt;% select(name, `Hair color`, hair) 21.20 Агрегация: dplyr::group_by() %&gt;% summarise() Посчитайте количество супергероев по расам и отсортируйте по убыванию. Извлеките первые 5 строк. heroes %&gt;% count(Race, sort = TRUE) %&gt;% slice(1:5) Посчитайте средний пост по полу. heroes %&gt;% group_by(Gender) %&gt;% summarise(height_mean = mean(Height, na.rm = TRUE)) 21.21 Соединение датафреймов: *_join {#solution_join} Создайте тиббл web_creators, в котором будут супергерои, которые могут плести паутину, т.е. у них стоит TRUE в колонке Web Creation в тиббле powers. powers_web &lt;- powers %&gt;% select(hero_names, `Web Creation`) web_creators &lt;- left_join(heroes, powers_web, by = c(&quot;name&quot; = &quot;hero_names&quot;)) %&gt;% filter(`Web Creation`) web_creators Найдите всех супергероев, которые присутствуют в heroes, но отсутствуют в powers. Ответом должен быть строковый вектор с именами супергероев. anti_join(heroes, powers, by = c(&quot;name&quot; = &quot;hero_names&quot;)) %&gt;% pull(name) ## [1] &quot;Agent 13&quot; &quot;Alfred Pennyworth&quot; &quot;Arsenal&quot; ## [4] &quot;Batgirl III&quot; &quot;Batgirl V&quot; &quot;Beetle&quot; ## [7] &quot;Black Goliath&quot; &quot;Black Widow II&quot; &quot;Blaquesmith&quot; ## [10] &quot;Bolt&quot; &quot;Boomer&quot; &quot;Box&quot; ## [13] &quot;Box III&quot; &quot;Captain Mar-vell&quot; &quot;Cat II&quot; ## [16] &quot;Cecilia Reyes&quot; &quot;Clea&quot; &quot;Clock King&quot; ## [19] &quot;Colin Wagner&quot; &quot;Colossal Boy&quot; &quot;Corsair&quot; ## [22] &quot;Cypher&quot; &quot;Danny Cooper&quot; &quot;Darkside&quot; ## [25] &quot;ERG-1&quot; &quot;Fixer&quot; &quot;Franklin Storm&quot; ## [28] &quot;Giant-Man&quot; &quot;Giant-Man II&quot; &quot;Goliath&quot; ## [31] &quot;Goliath&quot; &quot;Goliath&quot; &quot;Guardian&quot; ## [34] &quot;Hawkwoman&quot; &quot;Hawkwoman II&quot; &quot;Hawkwoman III&quot; ## [37] &quot;Howard the Duck&quot; &quot;Jack Bauer&quot; &quot;Jesse Quick&quot; ## [40] &quot;Jessica Sanders&quot; &quot;Jigsaw&quot; &quot;Jyn Erso&quot; ## [43] &quot;Kid Flash II&quot; &quot;Kingpin&quot; &quot;Meteorite&quot; ## [46] &quot;Mister Zsasz&quot; &quot;Mogo&quot; &quot;Moloch&quot; ## [49] &quot;Morph&quot; &quot;Nite Owl II&quot; &quot;Omega Red&quot; ## [52] &quot;Paul Blart&quot; &quot;Penance&quot; &quot;Penance I&quot; ## [55] &quot;Plastic Lad&quot; &quot;Power Man&quot; &quot;Renata Soliz&quot; ## [58] &quot;Ronin&quot; &quot;Shrinking Violet&quot; &quot;Snake-Eyes&quot; ## [61] &quot;Spider-Carnage&quot; &quot;Spider-Woman II&quot; &quot;Stacy X&quot; ## [64] &quot;Thunderbird II&quot; &quot;Two-Face&quot; &quot;Vagabond&quot; ## [67] &quot;Vision II&quot; &quot;Vulcan&quot; &quot;Warbird&quot; ## [70] &quot;White Queen&quot; &quot;Wiz Kid&quot; &quot;Wondra&quot; ## [73] &quot;Wyatt Wingfoot&quot; &quot;Yellow Claw&quot; Найдите всех супергероев, которые присутствуют в powers, но отсутствуют в heroes. Ответом должен быть строковый вектор с именами супергероев. anti_join(powers, heroes, by = c(&quot;hero_names&quot; = &quot;name&quot;)) %&gt;% pull(hero_names) ## [1] &quot;3-D Man&quot; &quot;Bananaman&quot; &quot;Bizarro-Girl&quot; ## [4] &quot;Black Vulcan&quot; &quot;Blue Streak&quot; &quot;Bradley&quot; ## [7] &quot;Clayface&quot; &quot;Concrete&quot; &quot;Dementor&quot; ## [10] &quot;Doctor Poison&quot; &quot;Fire&quot; &quot;Hellgramite&quot; ## [13] &quot;Lara Croft&quot; &quot;Little Epic&quot; &quot;Lord Voldemort&quot; ## [16] &quot;Orion&quot; &quot;Peek-a-Boo&quot; &quot;Queen Hippolyta&quot; ## [19] &quot;Reactron&quot; &quot;SHDB&quot; &quot;Stretch Armstrong&quot; ## [22] &quot;TEST&quot; &quot;Tommy Clarke&quot; &quot;Tyrant&quot; 21.22 Tidy data Для начала создайте тиббл heroes_weight, скопировав код: heroes_weight &lt;- heroes %&gt;% filter(Publisher %in% c(&quot;DC Comics&quot;, &quot;Marvel Comics&quot;)) %&gt;% group_by(Gender, Publisher) %&gt;% summarise(weight_mean = mean(Weight, na.rm = TRUE)) %&gt;% drop_na() heroes_weight Функция drop_na() позволяет выбросить все строчки, в которых встречается NA. Превратите тиббл heroes_weight в широкий тиббл: heroes_weight %&gt;% pivot_wider(names_from = &quot;Publisher&quot;, values_from = &quot;weight_mean&quot;) Затем превратите его обратно в длинный тиббл: heroes_weight %&gt;% pivot_wider(names_from = &quot;Publisher&quot;, values_from = &quot;weight_mean&quot;) %&gt;% pivot_longer(cols = !Gender, names_to = &quot;Publisher&quot;, values_to = &quot;weight_mean&quot;) Сделайте powers длинным тибблом с тремя колонками: hero_names, power (названгие суперсилы) и has (наличие суперсилы у данного супергероя). powers %&gt;% pivot_longer(cols = !hero_names, names_to = &quot;power&quot;, values_to = &quot;has&quot;) Сделайте тиббл powers обратно широким, но с новой структурой: каждая строчка означает суперсилу, а каждая колонка - супергероя (за исключением первой колонки - названия суперсилы). powers %&gt;% pivot_longer(cols = !hero_names, names_to = &quot;power&quot;, values_to = &quot;has&quot;) %&gt;% pivot_wider(names_from = hero_names, values_from = has) 21.23 Операции с несколькими колонками: across() Посчитайте количество NA в каждой колонке, группируя по полу (Gender). na_n &lt;- function(x) sum(is.na(x)) heroes %&gt;% group_by(Gender) %&gt;% summarise(across(everything(), na_n)) Посчитайте количество NA в каждой колонке, которая заканчивается на \"color\", группируя по полу (Gender). na_n &lt;- function(x) sum(is.na(x)) heroes %&gt;% group_by(Gender) %&gt;% summarise(across(ends_with(&quot;color&quot;), na_n)) Найдите (первую) самую длинную строчку для каждой колонки с character типом данных, группируя по полу (Gender). Для расчета количества значений в строке есть функция nchar(), для расчета индекса (первого) максимального значения есть функция which.max(). longest_char &lt;- function(x) x[which.max(nchar(x))] heroes %&gt;% group_by(Gender) %&gt;% summarise(across(where(is.character), longest_char)) Создайте из тиббла heroes новый тиббл, в котором числовые значения Height и Weight заменены на следующие строковые значения: если у супергероя рост или вес выше среднего по колонке, то \"выше среднего\", если его/ее рост или вес ниже или равен среднему, то \"ниже среднего\". higher_than_average &lt;- function(x) ifelse(x &gt; mean(x, na.rm = TRUE), &quot;выше среднего&quot;, &quot;ниже среднего&quot;) heroes %&gt;% mutate(across(c(Height, Weight), higher_than_average)) Создайте из тиббла heroes новый тиббл, в котором числовые значения Height и Weight заменены на следующие строковые значения: если у супергероя внутри соответствующей группы по полу рост или вес выше среднего по колонке, то \"выше среднего по X\", если его/ее рост или вес ниже или равен среднему внутри соответствующей группы по полу, то \"ниже среднего по X\" , где X — соответствующий пол (Gender). heroes %&gt;% group_by(Gender) %&gt;% mutate(across(c(Height, Weight), higher_than_average)) %&gt;% ungroup() %&gt;% mutate(across(c(Height, Weight), ~paste(., &quot;по&quot;, Gender))) 21.24 Описательная статистика Для выполнения задания создайте вектор height из колонки Height датасета heroes, удалив в нем NA. height &lt;- heroes %&gt;% drop_na(Height) %&gt;% pull(Height) Посчитайте среднее в векторе height. mean(height) ## [1] 186.7263 Посчитайте усеченное среднее в векторе height с усечением 5% значений с обоих сторон. mean(height, trim = 0.05) ## [1] 182.5846 Посчитайте медиану в векторе height. median(height) ## [1] 183 Посчитайте стандартное отклонение в векторе height. sd(height) ## [1] 59.25189 Посчитайте межквартильный размах в векторе height. IQR(height) ## [1] 18 Посчитайте ассиметрию в векторе height. psych::skew(height) ## [1] 8.843432 Посчитайте эксцесс в векторе height. psych::kurtosi(height) ## [1] 105.0297 Примените функции для получения множественных статистик на векторе height. summary(height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 15.2 173.0 183.0 186.7 191.0 975.0 psych::describe(height) skimr::skim(height) Таблица 21.1: Data summary Name height Number of rows 517 Number of columns 1 _______________________ Column type frequency: numeric 1 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 186.73 59.25 15.2 173 183 191 975 ▇▁▁▁▁ 21.25 Построение графиков в ggplot2 Нарисуйте столбиковую диаграмму (geom_bar()), которая будет отражать количество супергероев издателей \"Marvel Comics\", \"DC Comics\" и всех остальных (отдельным столбиком) из датасета heroes. heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% #mutate(Publisher = fct_lump(Publisher, 2)) %&gt;% #Еще один способ сделать то же самое, но через forcats ggplot(aes(x = Publisher)) + geom_bar() Добавьте к этой диаграме заливку цветом (fill =) в зависимости от распределения Gender внутри каждой группы. heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% #mutate(Publisher = fct_lump(Publisher, 2)) %&gt;% #Еще один способ сделать то же самое, но через forcats ggplot(aes(x = Publisher, fill = Gender)) + geom_bar() Сделайте так, чтобы каждый столбик был максимальной высоты (position = \"fill\"). heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% #mutate(Publisher = fct_lump(Publisher, 2)) %&gt;% #Еще один способ сделать то же самое, но через forcats ggplot(aes(x = Publisher, fill = Gender)) + geom_bar(position = &quot;fill&quot;) Финализируйте график, задав ему описания осей (например, функция labs()), использовав процентную шкалу (scale_y_continuous(labels = scales::percent)) и задав тему theme_minimal(). heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% #mutate(Publisher = fct_lump(Publisher, 2)) %&gt;% #Еще один способ сделать то же самое, но через forcats ggplot(aes(x = Publisher, fill = Gender)) + geom_bar(position = &quot;fill&quot;) + labs(title = &quot;Распределение супергероев по полу у разных издателей коммиксов&quot;, x = &quot;Издатель&quot;, y = &quot;Количество супергероев&quot;)+ scale_y_continuous(labels = scales::percent) + theme_minimal() Создайте диаграмму рассеяния для датасета heroes, для которой координаты по оси x будут взяты из колонки Height, а координаты по оси y — из колонки Weight. heroes %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point() Удалите с графика все экстремальные значения, для которых Weight больше или равен 700 или Height больше или равен 400. (Подсказка: это можно делать как средствами ggplot2, так и функцией filter() из dplyr). heroes %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point() Раскрасьте точки в зависимости от Gender, сделайте их полупрозрачными ( параметр alpha =). heroes %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point(aes(colour = Gender), alpha = 0.5) Сделайте так, чтобы координатная плоскость имела соотношение 1:1 шкал по оси x и y. Этого можно добиться с помощью функции coord_fixed(). heroes %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point(aes(colour = Gender), alpha = 0.5) + coord_fixed() Разделите график (facet_wrap()) на три: для \"DC Comics\",\"Marvel Comics\" и всех остальных. heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point(aes(colour = Gender), alpha = 0.5) + coord_fixed() + facet_wrap(~Publisher) Используйте для графика тему theme_linedraw(). heroes %&gt;% mutate(Publisher = ifelse(Publisher %in% c(&quot;Marvel Comics&quot;, &quot;DC Comics&quot;), Publisher, &quot;Other publishers&quot;)) %&gt;% filter(Weight &lt; 700 &amp; Height &lt; 400) %&gt;% ggplot(aes(x = Height, y = Weight)) + geom_point(aes(colour = Gender), alpha = 0.5) + coord_fixed() + facet_wrap(~Publisher)+ theme_linedraw() Постройте новый график (или возьмите старый) по датасетам heroes и/или powers и сделайте его некрасивым! Чем хуже у вас получится график, тем лучше. Желательно, чтобы этот график был по-прежнему графиком, а не произведением абстрактного искусства. Разница очень тонкая, но она есть. Вот несколько подсказок для этого задания: Для вдохновения посмотрите на вот эти графики. Для реально плохих графиков вам придется покопаться с настройками темы. Посмотрите подсказку по темам ?theme, попытайтесь что-то поменять в теме. Экспериментируйте с разными геомами и необычными их применениями. По изучайте дополнения к gpplot2. Попробуйте подготовить интересные данные для этого графика. НЕТ ПРАВИЛЬНОГО РЕШЕНИЯ, ПРОЯВИТЕ СВОЮ ФАНТАЗИЮ! 21.26 Распределения Выберите любое непрерывное распределение из представленных в базовом пакете stats или же в любом другом пакете. Найти все распределения пакета stats можно с помощью ?Distributions. Подберите для него какие-нибудь параметры или используйте параметры по умолчанию. Я возьму F-распределение с параметрами df1 = 4 и df = 10, но вы можете выбрать другое распределение. Визуализируйте функцию плотности вероятности для выбранного распределения. v &lt;- seq(0, 5, 0.01) plot(v, df(v, df1 = 4, df2 = 10)) Визуализируйте функцию накопленной плотности распределения для выбранной функции. plot(v, pf(v, df1 = 4, df2 = 10)) Визуализируйте квантильную функцию для выбранного распределения. p &lt;- seq(0, 1, .01) plot(p, qf(p, df1 = 4, df2 = 10)) Сделайте выборку из 100 случайных значений из выбранного распределения и постройте гистограмму (функция hist()) для полученной выборки. hist(rf(100, df1 = 4, df2 = 10)) 21.27 Одновыборочный t-test Представьте, что наши супергерои из набора данных heroes — это выборка из генеральной совокупности всех написанных и ненаписанных супергероев. Проведите одновыборочный t-тест для веса супергероев и числа 100 — предположительного среднего веса в генеральной совокупности всех супергероев. Проинтерпретируйте результат. t.test(heroes$Weight, mu = 100) ## ## One Sample t-test ## ## data: heroes$Weight ## t = 2.6174, df = 494, p-value = 0.009133 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 103.0549 121.4501 ## sample estimates: ## mean of x ## 112.2525 p-value меньше .05, поэтому мы можем отклонить нулевую гипотезу о том, что среднее для веса в генеральной совкупности, из которой вы взяли выборку супергероев, равно 100. Мы принимаем ненулевую гипотезу о том, что в генеральной совокупности средний вес не равен 100. Проведите одновыборочный t-тест для роста супергероев и числа 185 — предположительного среднего роста в генеральной совокупности всех супергероев. Проинтерпретируйте результат. t.test(heroes$Height, mu = 185) ## ## One Sample t-test ## ## data: heroes$Height ## t = 0.66246, df = 516, p-value = 0.508 ## alternative hypothesis: true mean is not equal to 185 ## 95 percent confidence interval: ## 181.6068 191.8458 ## sample estimates: ## mean of x ## 186.7263 p-value больше .05, поэтому мы не можем отклонить нулевую гипотезу о том, что среднее для роста в генеральной совкупности, из которой вы взяли выборку супергероев, равно 185. 21.28 Двухвыборочный зависимый t-test diet &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/stcp-Rdataset-Diet.csv&quot;) Посчитайте двухвыборочный зависимый т-тест для остальных диет: для диеты 2 и диеты 3. Проинтерпретируйте полученные результаты. diet2 &lt;- diet %&gt;% filter(Diet == 2) t.test(diet2$pre.weight, diet2$weight6weeks, paired = TRUE) ## ## Paired t-test ## ## data: diet2$pre.weight and diet2$weight6weeks ## t = 6.231, df = 26, p-value = 1.36e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.027715 4.024137 ## sample estimates: ## mean of the differences ## 3.025926 p-value меньше .05, поэтому мы можем отклонить нулевую гипотезу о том, что масса до и после диеты одинаковая. Мы принимаем альтернативную гипотезу о различии средних в генеральной совокупности. diet3 &lt;- diet %&gt;% filter(Diet == 3) t.test(diet3$pre.weight, diet3$weight6weeks, paired = TRUE) ## ## Paired t-test ## ## data: diet3$pre.weight and diet3$weight6weeks ## t = 11.167, df = 26, p-value = 2.03e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 4.200493 6.095803 ## sample estimates: ## mean of the differences ## 5.148148 p-value меньше .05, поэтому мы можем отклонить нулевую гипотезу о том, что масса до и после диеты одинаковая. Мы принимаем альтернативную гипотезу о различии средних в генеральной совокупности. 21.29 Двухвыборочный независимый t-test Сделайте независимый t-тест для сравнения веса испытуемых двух групп после диеты, сравнив вторую и третью группу. Проинтерпретируйте результаты. diet23 &lt;- diet %&gt;% filter(Diet %in% 2:3) t.test(weight6weeks ~ Diet, data = diet23, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: weight6weeks by Diet ## t = -0.15686, df = 49.774, p-value = 0.876 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.471327 4.678734 ## sample estimates: ## mean in group 2 mean in group 3 ## 68.08519 68.48148 p-value больше .05, поэтому мы не можем отклонить нулевую гипотезу о том, что масса до и после диеты одинаковая. Сделайте независимый t-тест для сравнения веса испытуемых двух групп после диеты, сравнив первую и третью группу. Проинтерпретируйте результаты. diet13 &lt;- diet %&gt;% filter(Diet %in% c(1,3)) t.test(weight6weeks ~ Diet, data = diet13, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: weight6weeks by Diet ## t = 0.46818, df = 48.072, p-value = 0.6418 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.602450 5.789487 ## sample estimates: ## mean in group 1 mean in group 3 ## 69.57500 68.48148 p-value больше .05, поэтому мы не можем отклонить нулевую гипотезу о том, что масса до и после диеты одинаковая. 21.30 Непараметрические аналоги t-теста Сравните вес первой и второй группы после диеты, используя тест Манна-Уитни. Сравните результаты теста Манна-Уитни с результатами t-теста? Проинтерпретируйте полученные результаты. diet12 &lt;- diet %&gt;% filter(Diet %in% c(1,2)) wilcox.test(weight6weeks ~ Diet, data = diet12, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight6weeks by Diet ## W = 368, p-value = 0.4117 ## alternative hypothesis: true location shift is not equal to 0 В обоих случаях p-value больше 0.05, мы не можем отклонить нулевую гипотезу об отсутствии различий. Повторите задание для второй и третьей группы, а так же для первой и третьей группы. wilcox.test(weight6weeks ~ Diet, data = diet23, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight6weeks by Diet ## W = 340.5, p-value = 0.6843 ## alternative hypothesis: true location shift is not equal to 0 wilcox.test(weight6weeks ~ Diet, data = diet13, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight6weeks by Diet ## W = 346, p-value = 0.6849 ## alternative hypothesis: true location shift is not equal to 0 В обоих случаях p-value больше 0.05, как и для соответствующих t-тестов. Мы не можем отклонить нулевую гипотезу об отстутствии различий между второй и третьей диетой, между первой и третьей диетой. Сравните вес до и после для диеты 1, используя тест Уилкоксона. Сравните с результатами применения t-теста. Проинтерпретируйте полученные результаты. diet1 &lt;- diet %&gt;% filter(Diet == 1) wilcox.test(diet1$pre.weight, diet1$weight6weeks, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: diet1$pre.weight and diet1$weight6weeks ## V = 299, p-value = 2.203e-05 ## alternative hypothesis: true location shift is not equal to 0 И t-тест, и тест Уилкоксона дают p-value ниже 0.05. Мы можем отклонить нулевую гипотезу об отсутствии различий. Сравните вес до и после для диеты 2 и диеты 3, используя тест Уилкоксона. Сравните с результатами применения t-теста. Проинтерпретируйте полученные результаты. wilcox.test(diet2$pre.weight, diet2$weight6weeks, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: diet2$pre.weight and diet2$weight6weeks ## V = 313, p-value = 5.419e-05 ## alternative hypothesis: true location shift is not equal to 0 wilcox.test(diet3$pre.weight, diet3$weight6weeks, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: diet3$pre.weight and diet3$weight6weeks ## V = 378, p-value = 5.912e-06 ## alternative hypothesis: true location shift is not equal to 0 В обоих случаях и t-тест, и тест Уилкоксона дают p-value ниже 0.05. Мы можем отклонить нулевую гипотезу об отсутствии различий. 21.31 Исследование набора данных Backpack Для следующих тем нам понадобится набор данных Backpack из пакета Stat2Data. #install.packages(&quot;Stat2Data&quot;) library(Stat2Data) data(Backpack) back &lt;- Backpack %&gt;% mutate(backpack_kg = 0.45359237 * BackpackWeight, body_kg = 0.45359237 * BodyWeight) Как различается вес рюкзака в зависимости от пола? Кто весит больше? back %&gt;% group_by(Sex) %&gt;% summarise(mean(backpack_kg)) Если допустить, что выборка репрезентативна, то можно ли сделать вывод о различии по среднему весу рюкзаков в генеральной совокупности? t.test(backpack_kg ~ Sex, data = back) ## ## Welch Two Sample t-test ## ## data: backpack_kg by Sex ## t = -1.1782, df = 86.25, p-value = 0.242 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.6892365 0.4320067 ## sample estimates: ## mean in group Female mean in group Male ## 5.006010 5.634625 p-value больше .05, поэтому мы не можем отклонить нулевую гипотезу о том, что масса до и после диеты одинаковая. Повторите пунктs 2 и 3 для веса самих студентов. back %&gt;% group_by(Sex) %&gt;% summarise(mean(body_kg)) t.test(body_kg ~ Sex, data = back) ## ## Welch Two Sample t-test ## ## data: body_kg by Sex ## t = -7.0863, df = 77.002, p-value = 5.704e-10 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -20.32511 -11.40803 ## sample estimates: ## mean in group Female mean in group Male ## 62.28236 78.14893 p-value меньше .05, поэтому мы можем отклонить нулевую гипотезу о том, что масса студентов-мужчин и студентов-женщин одинаковая. Мы принимаем альтернативную гипотезу о различии средних в генеральной совокупности. Визуализируйте распределение этих двух переменных в зависимости от пола (используя ggplot2) library(ggplot2) ggplot(back)+ geom_histogram(aes(x = body_kg, fill = Sex), bins = 15, position = &quot;identity&quot;, alpha = 0.7) Постройте диаграмму рассеяния с помощью ggplot2. Цветом закодируйте пол респондента. ggplot(back, aes(x = body_kg, y = backpack_kg))+ geom_point(aes(colour = Sex), alpha = 0.5, size = 2) 21.32 Ковариация Посчитайте матрицу ковариаций для веса студентов и их рюкзаков в фунтах. Различаются ли результаты подсчета ковариации этих двух переменных от результатов подсчета ковариаций веса студентов и их рюкзаков в килограммах? Почему? back %&gt;% select(BodyWeight, BackpackWeight) %&gt;% cov() ## BodyWeight BackpackWeight ## BodyWeight 864.20960 32.08788 ## BackpackWeight 32.08788 33.23677 Результаты различаются, потому что значение ковариации зависит от размерности исходных шкал. 21.33 Коэффициент корреляции Посчитайте коэффициент корреляции Пирсона для веса студентов и их рюкзаков в фунтах. Различаются ли результаты подсчета коэффициента корреляции Пирсона (сам коэффициент, p-value) этих двух переменных от результатов подсчета корреляции Пирсона веса студентов и их рюкзаков в килограммах? Почему? cor.test(back$BackpackWeight, back$BodyWeight) ## ## Pearson&#39;s product-moment correlation ## ## data: back$BackpackWeight and back$BodyWeight ## t = 1.9088, df = 98, p-value = 0.05921 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.007360697 0.371918344 ## sample estimates: ## cor ## 0.1893312 Результаты не различаются, потому что значение ковариации не зависит от размерности исходных шкал. Посчитайте коэффициент корреляции Пирсона для веса и роста супергероев из датасета heroes. Проинтерпретируйте результат. cor.test(heroes$Weight, heroes$Height) ## ## Pearson&#39;s product-moment correlation ## ## data: heroes$Weight and heroes$Height ## t = 4.3555, df = 488, p-value = 1.619e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1066877 0.2772717 ## sample estimates: ## cor ## 0.1934412 p-value меньше 0.05, поэтому мы можем отклонить нулевую гипотезу об отсутствии линейной связи между ростом и весом и принять альтернативную гипотезу о том, что такая связь есть. Судя по знаку и размеру корреляции, чем выше рост, тем выше вес супергероя, но эта связь достаточно слабая. Теперь посчитайте коэффициент корреляции Спирмена и коэффициент корреляции Кэнделла для веса и роста супергероев из датасета heroes. Различаются ли результаты по сравнению с коэффициентом корреляции Пирсона? Почему? cor.test(heroes$Weight, heroes$Height, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: heroes$Weight and heroes$Height ## S = 3915061, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8003344 cor.test(heroes$Weight, heroes$Height, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: heroes$Weight and heroes$Height ## z = 21.548, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.6751664 В обоих случаях p-value меньше 0.05, поэтому мы можем отклонить нулевую гипотезу об отсутствии связи между ростом и весом и принять альтернативную гипотезу о том, что такая связь есть. Сильное различие между коэффициентами корреляции указывает на нелинейность этой связи, либо же на наличие значительных выбросов в данных. "],["ссылки-на-литературу.html", "Ссылки на литературу", " Ссылки на литературу Adler, Joseph. 2010. R in a Nutshell: A Desktop Quick Reference. \" O’Reilly Media, Inc.\". Lumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002. “The Importance of the Normality Assumption in Large Public Health Data Sets.” Annual Review of Public Health 23 (1): 151–69. https://doi.org/10.1146/annurev.publhealth.23.100901.140546. Ritchie, Stuart, and Elliot Tucker-Drob. 2018. “How Much Does Education Improve Intelligence? A Meta-Analysis.” Psychological Science 29 (June): 095679761877425. https://doi.org/10.1177/0956797618774253. Wickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098. Wilkinson, Leland. 2005. The Grammar of Graphics (Statistics and Computing). Berlin, Heidelberg: Springer-Verlag. "]]
